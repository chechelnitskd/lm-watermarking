This paper focuses on supporting AI/ML Security Workers -- professionals
involved in the development and deployment of secure AI-enabled software
systems. It presents AI/ML Adversarial Techniques, Tools, and Common Knowledge
(AI/ML ATT&CK) framework to enable AI/ML Security Workers intuitively to
explore offensive and defensive tactics.

As artificial intelligence (AI) advances, human-AI collaboration has become
increasingly prevalent across both professional and everyday settings. In such
collaboration, AI can express its confidence level about its performance,
serving as a crucial indicator for humans to evaluate AI's suggestions.
However, AI may exhibit overconfidence or underconfidence--its expressed
confidence is higher or lower than its actual performance--which may lead
humans to mistakenly evaluate AI advice. Our study investigates the influences
of AI's overconfidence and underconfidence on human trust, their acceptance of
AI suggestions, and collaboration outcomes. Our study reveal that disclosing AI
confidence levels and performance feedback facilitates better recognition of AI
confidence misalignments. However, participants tend to withhold their trust as
perceiving such misalignments, leading to a rejection of AI suggestions and
subsequently poorer performance in collaborative tasks. Conversely, without
such information, participants struggle to identify misalignments, resulting in
either the neglect of correct AI advice or the following of incorrect AI
suggestions, adversely affecting collaboration. This study offers valuable
insights for enhancing human-AI collaboration by underscoring the importance of
aligning AI's expressed confidence with its actual performance and the
necessity of calibrating human trust towards AI confidence.

AI has surpassed humans across a variety of tasks such as image
classification, playing games (e.g., go, "Starcraft" and poker), and protein
structure prediction. However, at the same time, AI is also bearing serious
controversies. Many researchers argue that little substantial progress has been
made for AI in recent decades. In this paper, the author (1) explains why
controversies about AI exist; (2) discriminates two paradigms of AI research,
termed "weak AI" and "strong AI" (a.k.a. artificial general intelligence); (3)
clarifies how to judge which paradigm a research work should be classified
into; (4) discusses what is the greatest value of "weak AI" if it has no chance
to develop into "strong AI".

Artificial Intelligence (AI) Ethics is a nascent yet critical research field.
Recent developments in generative AI and foundational models necessitate a
renewed look at the problem of AI Ethics. In this study, we perform a
bibliometric analysis of AI Ethics literature for the last 20 years based on
keyword search. Our study reveals a three-phase development in AI Ethics,
namely an incubation phase, making AI human-like machines phase, and making AI
human-centric machines phase. We conjecture that the next phase of AI ethics is
likely to focus on making AI more machine-like as AI matches or surpasses
humans intellectually, a term we coin as "machine-like human".

The advent of advanced AI brings to the forefront the need for comprehensive
safety evaluation. However, divergent practices and terminologies across
different communities (i.e., AI, software engineering, and governance),
combined with the complexity of AI systems and environmental affordances (e.g.,
access to tools), call for a holistic evaluation approach. This paper proposes
a framework for comprehensive AI system evaluation comprising three components:
1) harmonised terminology to facilitate communication across disciplines
involved in AI safety evaluation; 2) a taxonomy identifying essential elements
for AI system evaluation; 3) a mapping between AI lifecycle, stakeholders, and
requisite evaluations for accountable AI supply chain. This framework catalyses
a deeper discourse on AI system evaluation beyond model-centric approaches.

The comprehension and adoption of Artificial Intelligence (AI) are beset with
practical and ethical problems. This article presents a 5-level AI Capability
Assessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to
assist practitioners in AI comprehension and adoption. These practical tools
were developed with business executives, technologists, and other
organisational stakeholders in mind. They are founded on a comprehensive
conception of AI compared to those in other AI adoption models and are also
open-source artefacts. Thus, the AI-CAM and AI-CM present an accessible
resource to help inform organisational decision-makers on the capability
requirements for (1) AI-based data analytics use cases based on machine
learning technologies; (2) Knowledge representation to engineer and represent
data, information and knowledge using semantic technologies; and (3) AI-based
solutions that seek to emulate human reasoning and decision-making. The AI-CAM
covers the core capability dimensions (business, data, technology,
organisation, AI skills, risks, and ethical considerations) required at the
five capability maturity levels to achieve optimal use of AI in organisations.

Artificial intelligence (AI) and human-machine interaction (HMI) are two
keywords that usually do not fit embedded applications. Within the steps needed
before applying AI to solve a specific task, HMI is usually missing during the
AI architecture design and the training of an AI model. The human-in-the-loop
concept is prevalent in all other steps of developing AI, from data analysis
via data selection and cleaning to performance evaluation. During AI
architecture design, HMI can immediately highlight unproductive layers of the
architecture so that lightweight network architecture for embedded applications
can be created easily. We show that by using this HMI, users can instantly
distinguish which AI architecture should be trained and evaluated first since a
high accuracy on the task could be expected. This approach reduces the
resources needed for AI development by avoiding training and evaluating AI
architectures with unproductive layers and leads to lightweight AI
architectures. These resulting lightweight AI architectures will enable HMI
while running the AI on an edge device. By enabling HMI during an AI uses
inference, we will introduce the AI-in-the-loop concept that combines AI's and
humans' strengths. In our AI-in-the-loop approach, the AI remains the working
horse and primarily solves the task. If the AI is unsure whether its inference
solves the task correctly, it asks the user to use an appropriate HMI.
Consequently, AI will become available in many applications soon since HMI will
make AI more reliable and explainable.

The organizational use of artificial intelligence (AI) has rapidly spread
across various sectors. Alongside the awareness of the benefits brought by AI,
there is a growing consensus on the necessity of tackling the risks and
potential harms, such as bias and discrimination, brought about by advanced AI
technologies. A multitude of AI ethics principles have been proposed to tackle
these risks, but the outlines of organizational processes and practices for
ensuring socially responsible AI development are in a nascent state. To address
the paucity of comprehensive governance models, we present an AI governance
framework, the hourglass model of organizational AI governance, which targets
organizations that develop and use AI systems. The framework is designed to
help organizations deploying AI systems translate ethical AI principles into
practice and align their AI systems and processes with the forthcoming European
AI Act. The hourglass framework includes governance requirements at the
environmental, organizational, and AI system levels. At the AI system level, we
connect governance requirements to AI system life cycles to ensure governance
throughout the system's life span. The governance model highlights the systemic
nature of AI governance and opens new research avenues into its practical
implementation, the mechanisms that connect different AI governance layers, and
the dynamics between the AI governance actors. The model also offers a starting
point for organizational decision-makers to consider the governance components
needed to ensure social acceptability, mitigate risks, and realize the
potential of AI.

The stochastic nature of artificial intelligence (AI) models introduces risk
to business applications that use AI models without careful consideration. This
paper offers an approach to use AI techniques to gain insights on the usage of
the AI models and control how they are deployed to a production application.
  Keywords: artificial intelligence (AI), machine learning, microservices,
business process

Designing human-centered AI-driven applications require deep understandings
of how people develop mental models of AI. Currently, we have little knowledge
of this process and limited tools to study it. This paper presents the position
that AI-based games, particularly the player-AI interaction component, offer an
ideal domain to study the process in which mental models evolve. We present a
case study to illustrate the benefits of our approach for explainable AI.

Ethics in AI has become a debated topic of public and expert discourse in
recent years. But what do people who build AI - AI practitioners - have to say
about their understanding of AI ethics and the challenges associated with
incorporating it in the AI-based systems they develop? Understanding AI
practitioners' views on AI ethics is important as they are the ones closest to
the AI systems and can bring about changes and improvements. We conducted a
survey aimed at understanding AI practitioners' awareness of AI ethics and
their challenges in incorporating ethics. Based on 100 AI practitioners'
responses, our findings indicate that majority of AI practitioners had a
reasonable familiarity with the concept of AI ethics, primarily due to
workplace rules and policies. Privacy protection and security was the ethical
principle that majority of them were aware of. Formal education/training was
considered somewhat helpful in preparing practitioners to incorporate AI
ethics. The challenges that AI practitioners faced in the development of
ethical AI-based systems included (i) general challenges, (ii)
technology-related challenges and (iii) human-related challenges. We also
identified areas needing further investigation and provided recommendations to
assist AI practitioners and companies in incorporating ethics into AI
development.

The rise in the use of AI/ML applications across industries has sparked more
discussions about the fairness of AI/ML in recent times. While prior research
on the fairness of AI/ML exists, there is a lack of empirical studies focused
on understanding the views and experiences of AI practitioners in developing a
fair AI/ML. Understanding AI practitioners' views and experiences on the
fairness of AI/ML is important because they are directly involved in its
development and deployment and their insights can offer valuable real-world
perspectives on the challenges associated with ensuring fairness in AI/ML. We
conducted semi-structured interviews with 22 AI practitioners to investigate
their understanding of what a 'fair AI/ML' is, the challenges they face in
developing a fair AI/ML, the consequences of developing an unfair AI/ML, and
the strategies they employ to ensure AI/ML fairness. We developed a framework
showcasing the relationship between AI practitioners' understanding of 'fair
AI/ML' and (i) their challenges in its development, (ii) the consequences of
developing an unfair AI/ML, and (iii) strategies used to ensure AI/ML fairness.
Additionally, we also identify areas for further investigation and offer
recommendations to aid AI practitioners and AI companies in navigating
fairness.

Instances of Artificial Intelligence (AI) systems failing to deliver
consistent, satisfactory performance are legion. We investigate why AI failures
occur. We address only a narrow subset of the broader field of AI Safety. We
focus on AI failures on account of flaws in conceptualization, design and
deployment. Other AI Safety issues like trade-offs between privacy and security
or convenience, bad actors hacking into AI systems to create mayhem or bad
actors deploying AI for purposes harmful to humanity and are out of scope of
our discussion. We find that AI systems fail on account of omission and
commission errors in the design of the AI system, as well as upon failure to
develop an appropriate interpretation of input information. Moreover, even when
there is no significant flaw in the AI software, an AI system may fail because
the hardware is incapable of robust performance across environments. Finally an
AI system is quite likely to fail in situations where, in effect, it is called
upon to deliver moral judgments -- a capability AI does not possess. We observe
certain trade-offs in measures to mitigate a subset of AI failures and provide
some recommendations.

Artificial intelligence (AI) has brought benefits, but it may also cause harm
if it is not appropriately developed. Current development is mainly driven by a
"technology-centered" approach, causing many failures. For example, the AI
Incident Database has documented over a thousand AI-related accidents. To
address these challenges, a human-centered AI (HCAI) approach has been promoted
and has received a growing level of acceptance over the last few years. HCAI
calls for combining AI with user experience (UX) design will enable the
development of AI systems (e.g., autonomous vehicles, intelligent user
interfaces, or intelligent decision-making systems) to achieve its design goals
such as usable/explainable AI, human-controlled AI, and ethical AI. While HCAI
promotion continues, it has not specifically addressed the collaboration
between AI and human-computer interaction (HCI) communities, resulting in
uncertainty about what action should be taken by both sides to apply HCAI in
developing AI systems. This Viewpoint focuses on the collaboration between the
AI and HCI communities, which leads to nine recommendations for effective
collaboration to enable HCAI in developing AI systems.

This paper examines the transformative role of Large Language Models (LLMs)
in education and their potential as learning tools, despite their inherent
risks and limitations. The authors propose seven approaches for utilizing AI in
classrooms: AI-tutor, AI-coach, AI-mentor, AI-teammate, AI-tool, AI-simulator,
and AI-student, each with distinct pedagogical benefits and risks. The aim is
to help students learn with and about AI, with practical strategies designed to
mitigate risks such as complacency about the AI's output, errors, and biases.
These strategies promote active oversight, critical assessment of AI outputs,
and complementarity of AI's capabilities with the students' unique insights. By
challenging students to remain the "human in the loop," the authors aim to
enhance learning outcomes while ensuring that AI serves as a supportive tool
rather than a replacement. The proposed framework offers a guide for educators
navigating the integration of AI-assisted learning in classrooms

Recent years have seen rapid deployment of mobile computing and Internet of
Things (IoT) networks, which can be mostly attributed to the increasing
communication and sensing capabilities of wireless systems. Big data analysis,
pervasive computing, and eventually artificial intelligence (AI) are envisaged
to be deployed on top of the IoT and create a new world featured by data-driven
AI. In this context, a novel paradigm of merging AI and wireless
communications, called Wireless AI that pushes AI frontiers to the network
edge, is widely regarded as a key enabler for future intelligent network
evolution. To this end, we present a comprehensive survey of the latest studies
in wireless AI from the data-driven perspective. Specifically, we first propose
a novel Wireless AI architecture that covers five key data-driven AI themes in
wireless networks, including Sensing AI, Network Device AI, Access AI, User
Device AI and Data-provenance AI. Then, for each data-driven AI theme, we
present an overview on the use of AI approaches to solve the emerging
data-related problems and show how AI can empower wireless network
functionalities. Particularly, compared to the other related survey papers, we
provide an in-depth discussion on the Wireless AI applications in various
data-driven domains wherein AI proves extremely useful for wireless network
design and optimization. Finally, research challenges and future visions are
also discussed to spur further research in this promising area.

In the last few years, AI continues demonstrating its positive impact on
society while sometimes with ethically questionable consequences. Building and
maintaining public trust in AI has been identified as the key to successful and
sustainable innovation. This chapter discusses the challenges related to
operationalizing ethical AI principles and presents an integrated view that
covers high-level ethical AI principles, the general notion of
trust/trustworthiness, and product/process support in the context of
responsible AI, which helps improve both trust and trustworthiness of AI for a
wider set of stakeholders.

This study explores the concept of creativity and artificial intelligence
(AI) and their recent integration. While AI has traditionally been perceived as
incapable of generating new ideas or creating art, the development of more
sophisticated AI models and the proliferation of human-computer interaction
tools have opened up new possibilities for AI in artistic creation. This study
investigates the various applications of AI in a creative context,
differentiating between the type of art, language, and algorithms used. It also
considers the philosophical implications of AI and creativity, questioning
whether consciousness can be researched in machines and AI's potential
interests and decision-making capabilities. Overall, we aim to stimulate a
reflection on AI's use and ethical implications in creative contexts.

With the recent advancements in Artificial Intelligence (AI), various
organizations and individuals are debating about the progress of AI as a
blessing or a curse for the future of the society. This paper conducts an
investigation on how the public perceives the progress of AI by utilizing the
data shared on Twitter. Specifically, this paper performs a comparative
analysis on the understanding of users belonging to two categories -- general
AI-Tweeters (AIT) and expert AI-Tweeters (EAIT) who share posts about AI on
Twitter. Our analysis revealed that users from both the categories express
distinct emotions and interests towards AI. Users from both the categories
regard AI as positive and are optimistic about the progress of AI but the
experts are more negative than the general AI-Tweeters. Expert AI-Tweeters
share relatively large percentage of tweets about their personal news compared
to technical aspects of AI. However, the effects of automation on the future
are of primary concern to AIT than to EAIT. When the expert category is
sub-categorized, the emotion analysis revealed that students and industry
professionals have more insights in their tweets about AI than academicians.

With the recent advancements in Artificial Intelligence (AI), various
organizations and individuals started debating about the progress of AI as a
blessing or a curse for the future of the society. This paper conducts an
investigation on how the public perceives the progress of AI by utilizing the
data shared on Twitter. Specifically, this paper performs a comparative
analysis on the understanding of users from two categories -- general
AI-Tweeters (AIT) and the expert AI-Tweeters (EAIT) who share posts about AI on
Twitter. Our analysis revealed that users from both the categories express
distinct emotions and interests towards AI. Users from both the categories
regard AI as positive and are optimistic about the progress of AI but the
experts are more negative than the general AI-Tweeters. Characterization of
users manifested that `London' is the popular location of users from where they
tweet about AI. Tweets posted by AIT are highly retweeted than posts made by
EAIT that reveals greater diffusion of information from AIT.

Artificial Intelligence (AI) governance regulates the exercise of authority
and control over the management of AI. It aims at leveraging AI through
effective use of data and minimization of AI-related cost and risk. While
topics such as AI governance and AI ethics are thoroughly discussed on a
theoretical, philosophical, societal and regulatory level, there is limited
work on AI governance targeted to companies and corporations. This work views
AI products as systems, where key functionality is delivered by machine
learning (ML) models leveraging (training) data. We derive a conceptual
framework by synthesizing literature on AI and related fields such as ML. Our
framework decomposes AI governance into governance of data, (ML) models and
(AI) systems along four dimensions. It relates to existing IT and data
governance frameworks and practices. It can be adopted by practitioners and
academics alike. For practitioners the synthesis of mainly research papers, but
also practitioner publications and publications of regulatory bodies provides a
valuable starting point to implement AI governance, while for academics the
paper highlights a number of areas of AI governance that deserve more
attention.

The advent of artificial intelligence (AI) and machine learning (ML) bring
human-AI interaction to the forefront of HCI research. This paper argues that
games are an ideal domain for studying and experimenting with how humans
interact with AI. Through a systematic survey of neural network games (n = 38),
we identified the dominant interaction metaphors and AI interaction patterns in
these games. In addition, we applied existing human-AI interaction guidelines
to further shed light on player-AI interaction in the context of AI-infused
systems. Our core finding is that AI as play can expand current notions of
human-AI interaction, which are predominantly productivity-based. In
particular, our work suggests that game and UX designers should consider flow
to structure the learning curve of human-AI interaction, incorporate
discovery-based learning to play around with the AI and observe the
consequences, and offer users an invitation to play to explore new forms of
human-AI interaction.

AI is transforming the existing technology landscape at a rapid phase
enabling data-informed decision making and autonomous decision making. Unlike
any other technology, because of the decision-making ability of AI, ethics and
governance became a key concern. There are many emerging AI risks for humanity,
such as autonomous weapons, automation-spurred job loss, socio-economic
inequality, bias caused by data and algorithms, privacy violations and
deepfakes. Social diversity, equity and inclusion are considered key success
factors of AI to mitigate risks, create values and drive social justice.
Sustainability became a broad and complex topic entangled with AI. Many
organizations (government, corporate, not-for-profits, charities and NGOs) have
diversified strategies driving AI for business optimization and
social-and-environmental justice. Partnerships and collaborations become
important more than ever for equity and inclusion of diversified and
distributed people, data and capabilities. Therefore, in our journey towards an
AI-enabled sustainable future, we need to address AI ethics and governance as a
priority. These AI ethics and governance should be underpinned by human ethics.

Artificial Intelligence (AI) has received an increasing amount of attention
in multiple areas. The uncertainties and risks in AI-powered systems have
created reluctance in their wild adoption. As an economic solution to
compensate for potential damages, AI liability insurance is a promising market
to enhance the integration of AI into daily life. In this work, we use an
AI-powered E-diagnosis system as an example to study AI liability insurance. We
provide a quantitative risk assessment model with evidence-based numerical
analysis. We discuss the insurability criteria for AI technologies and suggest
necessary adjustments to accommodate the features of AI products. We show that
AI liability insurance can act as a regulatory mechanism to incentivize
compliant behaviors and serve as a certificate of high-quality AI systems.
Furthermore, we suggest premium adjustment to reflect the dynamic evolution of
the inherent uncertainty in AI. Moral hazard problems are discussed and
suggestions for AI liability insurance are provided.

In AI-assisted decision-making, humans often passively review AI's suggestion
and decide whether to accept or reject it as a whole. In such a paradigm,
humans are found to rarely trigger analytical thinking and face difficulties in
communicating the nuances of conflicting opinions to the AI when disagreements
occur. To tackle this challenge, we propose Human-AI Deliberation, a novel
framework to promote human reflection and discussion on conflicting human-AI
opinions in decision-making. Based on theories in human deliberation, this
framework engages humans and AI in dimension-level opinion elicitation,
deliberative discussion, and decision updates. To empower AI with deliberative
capabilities, we designed Deliberative AI, which leverages large language
models (LLMs) as a bridge between humans and domain-specific models to enable
flexible conversational interactions and faithful information provision. An
exploratory evaluation on a graduate admissions task shows that Deliberative AI
outperforms conventional explainable AI (XAI) assistants in improving humans'
appropriate reliance and task performance. Based on a mixed-methods analysis of
participant behavior, perception, user experience, and open-ended feedback, we
draw implications for future AI-assisted decision tool design.

In this paper, we propose "Confident AI" as a means to designing Artificial
Intelligence (AI) and Machine Learning (ML) systems with both algorithm and
user confidence in model predictions and reported results. The 4 basic tenets
of Confident AI are Repeatability, Believability, Sufficiency, and
Adaptability. Each of the tenets is used to explore fundamental issues in
current AI/ML systems and together provide an overall approach to Confident AI.

As AI continues to advance, human-AI teams are inevitable. However, progress
in AI is routinely measured in isolation, without a human in the loop. It is
crucial to benchmark progress in AI, not just in isolation, but also in terms
of how it translates to helping humans perform certain tasks, i.e., the
performance of human-AI teams.
  In this work, we design a cooperative game - GuessWhich - to measure human-AI
team performance in the specific context of the AI being a visual
conversational agent. GuessWhich involves live interaction between the human
and the AI. The AI, which we call ALICE, is provided an image which is unseen
by the human. Following a brief description of the image, the human questions
ALICE about this secret image to identify it from a fixed pool of images.
  We measure performance of the human-ALICE team by the number of guesses it
takes the human to correctly identify the secret image after a fixed number of
dialog rounds with ALICE. We compare performance of the human-ALICE teams for
two versions of ALICE. Our human studies suggest a counterintuitive trend -
that while AI literature shows that one version outperforms the other when
paired with an AI questioner bot, we find that this improvement in AI-AI
performance does not translate to improved human-AI performance. This suggests
a mismatch between benchmarking of AI in isolation and in the context of
human-AI teams.

The goal of the present paper is to develop and validate a questionnaire to
assess AI literacy. In particular, the questionnaire should be deeply grounded
in the existing literature on AI literacy, should be modular (i.e., including
different facets that can be used independently of each other) to be flexibly
applicable in professional life depending on the goals and use cases, and
should meet psychological requirements and thus includes further psychological
competencies in addition to the typical facets of AIL. We derived 60 items to
represent different facets of AI Literacy according to Ng and colleagues
conceptualisation of AI literacy and additional 12 items to represent
psychological competencies such as problem solving, learning, and emotion
regulation in regard to AI. For this purpose, data were collected online from
300 German-speaking adults. The items were tested for factorial structure in
confirmatory factor analyses. The result is a measurement instrument that
measures AI literacy with the facets Use & apply AI, Understand AI, Detect AI,
and AI Ethics and the ability to Create AI as a separate construct, and AI
Self-efficacy in learning and problem solving and AI Self-management. This
study contributes to the research on AI literacy by providing a measurement
instrument relying on profound competency models. In addition, higher-order
psychological competencies are included that are particularly important in the
context of pervasive change through AI systems.

Artificial intelligence (AI) is increasingly being considered to assist human
decision-making in high-stake domains (e.g. health). However, researchers have
discussed an issue that humans can over-rely on wrong suggestions of the AI
model instead of achieving human AI complementary performance. In this work, we
utilized salient feature explanations along with what-if, counterfactual
explanations to make humans review AI suggestions more analytically to reduce
overreliance on AI and explored the effect of these explanations on trust and
reliance on AI during clinical decision-making. We conducted an experiment with
seven therapists and ten laypersons on the task of assessing post-stroke
survivors' quality of motion, and analyzed their performance, agreement level
on the task, and reliance on AI without and with two types of AI explanations.
Our results showed that the AI model with both salient features and
counterfactual explanations assisted therapists and laypersons to improve their
performance and agreement level on the task when `right' AI outputs are
presented. While both therapists and laypersons over-relied on `wrong' AI
outputs, counterfactual explanations assisted both therapists and laypersons to
reduce their over-reliance on `wrong' AI outputs by 21\% compared to salient
feature explanations. Specifically, laypersons had higher performance degrades
by 18.0 f1-score with salient feature explanations and 14.0 f1-score with
counterfactual explanations than therapists with performance degrades of 8.6
and 2.8 f1-scores respectively. Our work discusses the potential of
counterfactual explanations to better estimate the accuracy of an AI model and
reduce over-reliance on `wrong' AI outputs and implications for improving
human-AI collaborative decision-making.

Recent advancements in the field of Artificial Intelligence (AI) establish
the basis to address challenging tasks. However, with the integration of AI,
new risks arise. Therefore, to benefit from its advantages, it is essential to
adequately handle the risks associated with AI. Existing risk management
processes in related fields, such as software systems, need to sufficiently
consider the specifics of AI. A key challenge is to systematically and
transparently identify and address AI risks' root causes - also called AI
hazards. This paper introduces the AI Hazard Management (AIHM) framework, which
provides a structured process to systematically identify, assess, and treat AI
hazards. The proposed process is conducted in parallel with the development to
ensure that any AI hazard is captured at the earliest possible stage of the AI
system's life cycle. In addition, to ensure the AI system's auditability, the
proposed framework systematically documents evidence that the potential impact
of identified AI hazards could be reduced to a tolerable level. The framework
builds upon an AI hazard list from a comprehensive state-of-the-art analysis.
Also, we provide a taxonomy that supports the optimal treatment of the
identified AI hazards. Additionally, we illustrate how the AIHM framework can
increase the overall quality of a power grid AI use case by systematically
reducing the impact of identified hazards to an acceptable level.

Recent years have witnessed an increasing number of artificial intelligence
(AI) applications in transportation. As a new and emerging technology, AI's
potential to advance transportation goals and the full extent of its impacts on
the transportation sector is not yet well understood. As the transportation
community explores these topics, it is critical to understand how
transportation professionals, the driving force behind AI Transportation
applications, perceive AI's potential efficiency and equity impacts. Toward
this goal, we surveyed transportation professionals in the United States and
collected a total of 354 responses. Based on the survey responses, we conducted
both descriptive analysis and latent class cluster analysis (LCCA). The former
provides an overview of prevalent attitudes among transportation professionals,
while the latter allows the identification of distinct segments based on their
latent attitudes toward AI. We find widespread optimism regarding AI's
potential to improve many aspects of transportation (e.g., efficiency, cost
reduction, and traveler experience); however, responses are mixed regarding
AI's potential to advance equity. Moreover, many respondents are concerned that
AI ethics are not well understood in the transportation community and that AI
use in transportation could exaggerate existing inequalities. Through LCCA, we
have identified four latent segments: AI Neutral, AI Optimist, AI Pessimist,
and AI Skeptic. The latent class membership is significantly associated with
respondents' age, education level, and AI knowledge level. Overall, the study
results shed light on the extent to which the transportation community as a
whole is ready to leverage AI systems to transform current practices and inform
targeted education to improve the understanding of AI among transportation
professionals.

To benefit from AI advances, users and operators of AI systems must have
reason to trust it. Trust arises from multiple interactions, where predictable
and desirable behavior is reinforced over time. Providing the system's users
with some understanding of AI operations can support predictability, but
forcing AI to explain itself risks constraining AI capabilities to only those
reconcilable with human cognition. We argue that AI systems should be designed
with features that build trust by bringing decision-analytic perspectives and
formal tools into AI. Instead of trying to achieve explainable AI, we should
develop interpretable and actionable AI. Actionable and Interpretable AI (AI2)
will incorporate explicit quantifications and visualizations of user confidence
in AI recommendations. In doing so, it will allow examining and testing of AI
system predictions to establish a basis for trust in the systems' decision
making and ensure broad benefits from deploying and advancing its computational
capabilities.

Recent years have witnessed an astonishing explosion in the evolution of
mobile applications powered by AI technologies. The rapid growth of AI
frameworks enables the transition of AI technologies to mobile devices,
significantly prompting the adoption of AI apps (i.e., apps that integrate AI
into their functions) among smartphone devices. In this paper, we conduct the
most extensive empirical study on 56,682 published AI apps from three
perspectives: dataset characteristics, development issues, and user feedback
and privacy. To this end, we build an automated AI app identification tool, AI
Discriminator, that detects eligible AI apps from 7,259,232 mobile apps. First,
we carry out a dataset analysis, where we explore the AndroZoo large repository
to identify AI apps and their core characteristics. Subsequently, we pinpoint
key issues in AI app development (e.g., model protection). Finally, we focus on
user reviews and user privacy protection. Our paper provides several notable
findings. Some essential ones involve revealing the issue of insufficient model
protection by presenting the lack of model encryption, and demonstrating the
risk of user privacy data being leaked. We published our large-scale AI app
datasets to inspire more future research.

With the increasing prevalence of artificial intelligence (AI) in diverse
science/engineering communities, AI models emerge on an unprecedented scale
among various domains. However, given the complexity and diversity of the
software and hardware environments, reusing AI artifacts (models and datasets)
is extremely challenging, especially with AI-driven science applications.
Building an ecosystem to run and reuse AI applications/datasets at scale
efficiently becomes increasingly essential for diverse science and engineering
and high-performance computing (HPC) communities. In this paper, we innovate
over an HPC-AI ecosystem -- HPCFair, which enables the Findable, Accessible,
Interoperable, and Reproducible (FAIR) principles. HPCFair enables the
collection of AI models/datasets allowing users to download/upload AI artifacts
with authentications. Most importantly, our proposed framework provides
user-friendly APIs for users to easily run inference jobs and customize AI
artifacts to their tasks as needed. Our results show that, with HPCFair API,
users irrespective of technical expertise in AI, can easily leverage AI
artifacts to their tasks with minimal effort.

Human-AI interaction in text production increases complexity in authorship.
In two empirical studies (n1 = 30 & n2 = 96), we investigate authorship and
ownership in human-AI collaboration for personalized language generation. We
show an AI Ghostwriter Effect: Users do not consider themselves the owners and
authors of AI-generated text but refrain from publicly declaring AI authorship.
Personalization of AI-generated texts did not impact the AI Ghostwriter Effect,
and higher levels of participants' influence on texts increased their sense of
ownership. Participants were more likely to attribute ownership to supposedly
human ghostwriters than AI ghostwriters, resulting in a higher
ownership-authorship discrepancy for human ghostwriters. Rationalizations for
authorship in AI ghostwriters and human ghostwriters were similar. We discuss
how our findings relate to psychological ownership and human-AI interaction to
lay the foundations for adapting authorship frameworks and user interfaces in
AI in text-generation tasks.

This pioneering study explores students' perceptions of AI-giarism, an
emergent form of academic dishonesty involving AI and plagiarism, within the
higher education context. A survey, undertaken by 393 undergraduate and
postgraduate students from a variety of disciplines, investigated their
perceptions of diverse AI-giarism scenarios. The findings portray a complex
landscape of understanding, with clear disapproval for direct AI content
generation, yet more ambivalent attitudes towards subtler uses of AI. The study
introduces a novel instrument, as an initial conceptualization of AI-giarism,
offering a significant tool for educators and policy-makers. This scale
facilitates understanding and discussions around AI-related academic
misconduct, aiding in pedagogical design and assessment in an era of AI
integration. Moreover, it challenges traditional definitions of academic
misconduct, emphasizing the need to adapt in response to evolving AI
technology. Despite limitations, such as the rapidly changing nature of AI and
the use of convenience sampling, the study provides pivotal insights for
academia, policy-making, and the broader integration of AI technology in
education.

Gender bias is rampant in AI systems, causing bad user experience,
injustices, and mental harm to women. School curricula fail to educate AI
creators on this topic, leaving them unprepared to mitigate gender bias in AI.
In this paper, we designed hands-on tutorials to raise AI creators' awareness
of gender bias in AI and enhance their knowledge of sources of gender bias and
debiasing techniques. The tutorials were evaluated with 18 AI creators,
including AI researchers, AI industrial practitioners (i.e., developers and
product managers), and students who had learned AI. Their improved awareness
and knowledge demonstrated the effectiveness of our tutorials, which have the
potential to complement the insufficient AI gender bias education in CS/AI
courses. Based on the findings, we synthesize design implications and a rubric
to guide future research, education, and design efforts.

AI alignment considers the overall problem of ensuring an AI produces desired
outcomes, without undesirable side effects. While often considered from the
perspectives of safety and human values, AI alignment can also be considered in
the context of designing and evaluating interfaces for interactive AI systems.
This paper maps concepts from AI alignment onto a basic, three step interaction
cycle, yielding a corresponding set of alignment objectives: 1) specification
alignment: ensuring the user can efficiently and reliably communicate
objectives to the AI, 2) process alignment: providing the ability to verify and
optionally control the AI's execution process, and 3) evaluation support:
ensuring the user can verify and understand the AI's output. We also introduce
the concepts of a surrogate process, defined as a simplified, separately
derived, but controllable representation of the AI's actual process; and the
notion of a Process Gulf, which highlights how differences between human and AI
processes can lead to challenges in AI control. To illustrate the value of this
framework, we describe commercial and research systems along each of the three
alignment dimensions, and show how interfaces that provide interactive
alignment mechanisms can lead to qualitatively different and improved user
experiences.

As artificial intelligence (AI) is integrated into various services and
systems in society, many companies and organizations have proposed AI
principles, policies, and made the related commitments. Conversely, some have
proposed the need for independent audits, arguing that the voluntary principles
adopted by the developers and providers of AI services and systems
insufficiently address risk. This policy recommendation summarizes the issues
related to the auditing of AI services and systems and presents three
recommendations for promoting AI auditing that contribute to sound AI
governance. Recommendation1.Development of institutional design for AI audits.
Recommendation2.Training human resources for AI audits. Recommendation3.
Updating AI audits in accordance with technological progress.
  In this policy recommendation, AI is assumed to be that which recognizes and
predicts data with the last chapter outlining how generative AI should be
audited.

This article critically examines the recent hype around AI safety. We first
start with noting the nature of the AI safety hype as being dominated by
governments and corporations, and contrast it with other avenues within AI
research on advancing social good. We consider what 'AI safety' actually means,
and outline the dominant concepts that the digital footprint of AI safety
aligns with. We posit that AI safety has a nuanced and uneasy relationship with
transparency and other allied notions associated with societal good, indicating
that it is an insufficient notion if the goal is that of societal good in a
broad sense. We note that the AI safety debate has already influenced some
regulatory efforts in AI, perhaps in not so desirable directions. We also share
our concerns on how AI safety may normalize AI that advances structural harm
through providing exploitative and harmful AI with a veneer of safety.

This paper briefly reviews the history of meta-learning and describes its
contribution to general AI. Meta-learning improves model generalization
capacity and devises general algorithms applicable to both in-distribution and
out-of-distribution tasks potentially. General AI replaces task-specific models
with general algorithmic systems introducing higher level of automation in
solving diverse tasks using AI. We summarize main contributions of
meta-learning to the developments in general AI, including memory module,
meta-learner, coevolution, curiosity, forgetting and AI-generating algorithm.
We present connections between meta-learning and general AI and discuss how
meta-learning can be used to formulate general AI algorithms.

Artificial Intelligence (AI) is an integral part of our daily technology use
and will likely be a critical component of emerging technologies. However,
negative user preconceptions may hinder adoption of AI-based decision making.
Prior work has highlighted the potential of factors such as transparency and
explainability in improving user perceptions of AI. We further contribute to
work on improving user perceptions of AI by demonstrating that bringing the
user in the loop through mock model training can improve their perceptions of
an AI agent's capability and their comfort with the possibility of using
technology employing the AI agent.

Recent AI ethics has focused on applying abstract principles downward to
practice. This paper moves in the other direction. Ethical insights are
generated from the lived experiences of AI-designers working on tangible human
problems, and then cycled upward to influence theoretical debates surrounding
these questions: 1) Should AI as trustworthy be sought through explainability,
or accurate performance? 2) Should AI be considered trustworthy at all, or is
reliability a preferable aim? 3) Should AI ethics be oriented toward
establishing protections for users, or toward catalyzing innovation? Specific
answers are less significant than the larger demonstration that AI ethics is
currently unbalanced toward theoretical principles, and will benefit from
increased exposure to grounded practices and dilemmas.

Not too long ago, AI security used to mean the research and practice of how
AI can empower cybersecurity, that is, AI for security. Ever since Ian
Goodfellow and his team popularized adversarial attacks on machine learning,
security for AI became an important concern and also part of AI security. It is
imperative to understand the threats to machine learning products and avoid
common pitfalls in AI product development. This article is addressed to
developers, designers, managers and researchers of AI software products.

This article explores the transformative impact of artificial intelligence
(AI) on scientific research. It highlights ten ways in which AI is
revolutionizing the work of scientists, including powerful referencing tools,
improved understanding of research problems, enhanced research question
generation, optimized research design, stub data generation, data
transformation, advanced data analysis, and AI-assisted reporting. While AI
offers numerous benefits, challenges such as bias, privacy concerns, and the
need for human-AI collaboration must be considered. The article emphasizes that
AI can augment human creativity in science but not replace it.

Can governments build AI? In this paper, we describe an ongoing effort to
develop ``public AI'' -- publicly accessible AI models funded, provisioned, and
governed by governments or other public bodies. Public AI presents both an
alternative and a complement to standard regulatory approaches to AI, but it
also suggests new technical and policy challenges. We present a roadmap for how
the ML research community can help shape this initiative and support its
implementation, and how public AI can complement other responsible AI
initiatives.

Trusted AI literature to date has focused on the trust needs of users who
knowingly interact with discrete AIs. Conspicuously absent from the literature
is a rigorous treatment of public trust in AI. We argue that public distrust of
AI originates from the under-development of a regulatory ecosystem that would
guarantee the trustworthiness of the AIs that pervade society. Drawing from
structuration theory and literature on institutional trust, we offer a model of
public trust in AI that differs starkly from models driving Trusted AI efforts.
This model provides a theoretical scaffolding for Trusted AI research which
underscores the need to develop nothing less than a comprehensive and visibly
functioning regulatory ecosystem. We elaborate the pivotal role of externally
auditable AI documentation within this model and the work to be done to ensure
it is effective, and outline a number of actions that would promote public
trust in AI. We discuss how existing efforts to develop AI documentation within
organizations -- both to inform potential adopters of AI components and support
the deliberations of risk and ethics review boards -- is necessary but
insufficient assurance of the trustworthiness of AI. We argue that being
accountable to the public in ways that earn their trust, through elaborating
rules for AI and developing resources for enforcing these rules, is what will
ultimately make AI trustworthy enough to be woven into the fabric of our
society.

Analyzing usability test videos is arduous. Although recent research showed
the promise of AI in assisting with such tasks, it remains largely unknown how
AI should be designed to facilitate effective collaboration between user
experience (UX) evaluators and AI. Inspired by the concepts of agency and work
context in human and AI collaboration literature, we studied two corresponding
design factors for AI-assisted UX evaluation: explanations and synchronization.
Explanations allow AI to further inform humans how it identifies UX problems
from a usability test session; synchronization refers to the two ways humans
and AI collaborate: synchronously and asynchronously. We iteratively designed a
tool, AI Assistant, with four versions of UIs corresponding to the two levels
of explanations (with/without) and synchronization (sync/async). By adopting a
hybrid wizard-of-oz approach to simulating an AI with reasonable performance,
we conducted a mixed-method study with 24 UX evaluators identifying UX problems
from usability test videos using AI Assistant. Our quantitative and qualitative
results show that AI with explanations, regardless of being presented
synchronously or asynchronously, provided better support for UX evaluators'
analysis and was perceived more positively; when without explanations,
synchronous AI better improved UX evaluators' performance and engagement
compared to the asynchronous AI. Lastly, we present the design implications for
AI-assisted UX evaluation and facilitating more effective human-AI
collaboration.

Structured access is an emerging paradigm for the safe deployment of
artificial intelligence (AI). Instead of openly disseminating AI systems,
developers facilitate controlled, arm's length interactions with their AI
systems. The aim is to prevent dangerous AI capabilities from being widely
accessible, whilst preserving access to AI capabilities that can be used
safely. The developer must both restrict how the AI system can be used, and
prevent the user from circumventing these restrictions through modification or
reverse engineering of the AI system. Structured access is most effective when
implemented through cloud-based AI services, rather than disseminating AI
software that runs locally on users' hardware. Cloud-based interfaces provide
the AI developer greater scope for controlling how the AI system is used, and
for protecting against unauthorized modifications to the system's design. This
chapter expands the discussion of "publication norms" in the AI community,
which to date has focused on the question of how the informational content of
AI research projects should be disseminated (e.g., code and models). Although
this is an important question, there are limits to what can be achieved through
the control of information flows. Structured access views AI software not only
as information that can be shared but also as a tool with which users can have
arm's length interactions. There are early examples of structured access being
practiced by AI developers, but there is much room for further development,
both in the functionality of cloud-based interfaces and in the wider
institutional framework.

Problem statement: Standardisation of AI fairness rules and benchmarks is
challenging because AI fairness and other ethical requirements depend on
multiple factors such as context, use case, type of the AI system, and so on.
In this paper, we elaborate that the AI system is prone to biases at every
stage of its lifecycle, from inception to its usage, and that all stages
require due attention for mitigating AI bias. We need a standardised approach
to handle AI fairness at every stage. Gap analysis: While AI fairness is a hot
research topic, a holistic strategy for AI fairness is generally missing. Most
researchers focus only on a few facets of AI model-building. Peer review shows
excessive focus on biases in the datasets, fairness metrics, and algorithmic
bias. In the process, other aspects affecting AI fairness get ignored. The
solution proposed: We propose a comprehensive approach in the form of a novel
seven-layer model, inspired by the Open System Interconnection (OSI) model, to
standardise AI fairness handling. Despite the differences in the various
aspects, most AI systems have similar model-building stages. The proposed model
splits the AI system lifecycle into seven abstraction layers, each
corresponding to a well-defined AI model-building or usage stage. We also
provide checklists for each layer and deliberate on potential sources of bias
in each layer and their mitigation methodologies. This work will facilitate
layer-wise standardisation of AI fairness rules and benchmarking parameters.

There is an increasing interest in the entwining of the field of antitrust
with the field of Artificial Intelligence (AI), frequently referred to jointly
as Antitrust and AI (AAI) in the research literature. This study focuses on the
synergies entangling antitrust and AI, doing so to extend the literature by
proffering the primary ways that these two fields intersect, consisting of: (1)
the application of antitrust to AI, and (2) the application of AI to antitrust.
To date, most of the existing research on this intermixing has concentrated on
the former, namely the application of antitrust to AI, entailing how the
marketplace will be altered by the advent of AI and the potential for adverse
antitrust behaviors arising accordingly. Opting to explore more deeply the
other side of this coin, this research closely examines the application of AI
to antitrust and establishes an antitrust vigilance lifecycle to which AI is
predicted to be substantively infused for purposes of enabling and bolstering
antitrust detection, enforcement, and post-enforcement monitoring. Furthermore,
a gradual and incremental injection of AI into antitrust vigilance is
anticipated to occur as significant advances emerge amidst the Levels of
Autonomy (LoA) for AI Legal Reasoning (AILR).

Which part of medicine, if any, can and should be entrusted to AI, now or at
some moment in the future? That both medicine and AI will continue to change
goes without saying.

As Artificial Intelligence (AI) plays an ever-expanding role in
sociotechnical systems, it is important to articulate the relationships between
humans and AI. However, the scholarly communities studying human-AI
relationships -- including but not limited to social computing, machine
learning, science and technology studies, and other social sciences -- are
divided by the perspectives that define them. These perspectives vary both by
their focus on humans or AI, and in the micro/macro lenses through which they
approach subjects. These differences inhibit the integration of findings, and
thus impede science and interdisciplinarity. In this position paper, we propose
the development of a framework AI-Mediated Exchange Theory (AI-MET) to bridge
these divides. As an extension to Social Exchange Theory (SET) in the social
sciences, AI-MET views AI as influencing human-to-human relationships via a
taxonomy of mediation mechanisms. We list initial ideas of these mechanisms,
and show how AI-MET can be used to help human-AI research communities speak to
one another.

We consider two fundamental and related issues currently faced by Artificial
Intelligence (AI) development: the lack of ethics and interpretability of AI
decisions. Can interpretable AI decisions help to address ethics in AI? Using a
randomized study, we experimentally show that the empirical and liberal turn of
the production of explanations tends to select AI explanations with a low
denunciatory power. Under certain conditions, interpretability tools are
therefore not means but, paradoxically, obstacles to the production of ethical
AI since they can give the illusion of being sensitive to ethical incidents. We
also show that the denunciatory power of AI explanations is highly dependent on
the context in which the explanation takes place, such as the gender or
education level of the person to whom the explication is intended for. AI
ethics tools are therefore sometimes too flexible and self-regulation through
the liberal production of explanations do not seem to be enough to address
ethical issues. We then propose two scenarios for the future development of
ethical AI: more external regulation or more liberalization of AI explanations.
These two opposite paths will play a major role on the future development of
ethical AI.

The rapid development of Artificial Intelligence (AI) technology has enabled
the deployment of various systems based on it. However, many current AI systems
are found vulnerable to imperceptible attacks, biased against underrepresented
groups, lacking in user privacy protection. These shortcomings degrade user
experience and erode people's trust in all AI systems. In this review, we
provide AI practitioners with a comprehensive guide for building trustworthy AI
systems. We first introduce the theoretical framework of important aspects of
AI trustworthiness, including robustness, generalization, explainability,
transparency, reproducibility, fairness, privacy preservation, and
accountability. To unify currently available but fragmented approaches toward
trustworthy AI, we organize them in a systematic approach that considers the
entire lifecycle of AI systems, ranging from data acquisition to model
development, to system development and deployment, finally to continuous
monitoring and governance. In this framework, we offer concrete action items
for practitioners and societal stakeholders (e.g., researchers, engineers, and
regulators) to improve AI trustworthiness. Finally, we identify key
opportunities and challenges for the future development of trustworthy AI
systems, where we identify the need for a paradigm shift toward comprehensively
trustworthy AI systems.

There is a growing consensus in HCI and AI research that the design of AI
systems needs to engage and empower stakeholders who will be affected by AI.
However, the manner in which stakeholders should participate in AI design is
unclear. This workshop paper aims to ground what we dub a 'participatory turn'
in AI design by synthesizing existing literature on participation and through
empirical analysis of its current practices via a survey of recent published
research and a dozen semi-structured interviews with AI researchers and
practitioners. Based on our literature synthesis and empirical research, this
paper presents a conceptual framework for analyzing participatory approaches to
AI design and articulates a set of empirical findings that in ensemble detail
out the contemporary landscape of participatory practice in AI design. These
findings can help bootstrap a more principled discussion on how PD of AI should
move forward across AI, HCI, and other research communities.

Explainable AI (XAI) research has been booming, but the question "$\textbf{To
whom}$ are we making AI explainable?" is yet to gain sufficient attention. Not
much of XAI is comprehensible to non-AI experts, who nonetheless, are the
primary audience and major stakeholders of deployed AI systems in practice. The
gap is glaring: what is considered "explained" to AI-experts versus non-experts
are very different in practical scenarios. Hence, this gap produced two
distinct cultures of expectations, goals, and forms of XAI in real-life AI
deployments.
  We advocate that it is critical to develop XAI methods for non-technical
audiences. We then present a real-life case study, where AI experts provided
non-technical explanations of AI decisions to non-technical stakeholders, and
completed a successful deployment in a highly regulated industry. We then
synthesize lessons learned from the case, and share a list of suggestions for
AI experts to consider when explaining AI decisions to non-technical
stakeholders.

Although AI is transforming the world, there are serious concerns about its
ability to behave and make decisions responsibly. Many ethical regulations,
principles, and frameworks for responsible AI have been issued recently.
However, they are high level and difficult to put into practice. On the other
hand, most AI researchers focus on algorithmic solutions, while the responsible
AI challenges actually crosscut the entire engineering lifecycle and components
of AI systems. To close the gap in operationalizing responsible AI, this paper
aims to develop a roadmap on software engineering for responsible AI. The
roadmap focuses on (i) establishing multi-level governance for responsible AI
systems, (ii) setting up the development processes incorporating
process-oriented practices for responsible AI systems, and (iii) building
responsible-AI-by-design into AI systems through system-level architectural
style, patterns and techniques.

Human-AI co-creativity involves humans and AI collaborating on a shared
creative product as partners. In many existing co-creative systems, users
communicate with the AI using buttons or sliders. However, typically, the AI in
co-creative systems cannot communicate back to humans, limiting their potential
to be perceived as partners. This paper starts with an overview of a
comparative study with 38 participants to explore the impact of AI-to-human
communication on user perception and engagement in co-creative systems and the
results show improved collaborative experience and user engagement with the
system incorporating AI-to-human communication. The results also demonstrate
that users perceive co-creative AI as more reliable, personal and intelligent
when it can communicate with the users. The results indicate a need to identify
potential ethical issues from an engaging communicating co-creative AI. Later
in the paper, we present some potential ethical issues in human-AI co-creation
and propose to use participatory design fiction as the research methodology to
investigate the ethical issues associated with a co-creative AI that
communicates with users.

Numerous AI ethics checklists and frameworks have been proposed focusing on
different dimensions of ethical AI such as fairness, explainability, and
safety. Yet, no such work has been done on developing transparent AI systems
for real-world educational scenarios. This paper presents a Transparency Index
framework that has been iteratively co-designed with different stakeholders of
AI in education, including educators, ed-tech experts, and AI practitioners. We
map the requirements of transparency for different categories of stakeholders
of AI in education and demonstrate that transparency considerations are
embedded in the entire AI development process from the data collection stage
until the AI system is deployed in the real world and iteratively improved. We
also demonstrate how transparency enables the implementation of other ethical
AI dimensions in Education like interpretability, accountability, and safety.
In conclusion, we discuss the directions for future research in this newly
emerging field. The main contribution of this study is that it highlights the
importance of transparency in developing AI-powered educational technologies
and proposes an index framework for its conceptualization for AI in education.

With the powerful performance of Artificial Intelligence (AI) also comes
prevalent ethical issues. Though governments and corporations have curated
multiple AI ethics guidelines to curb unethical behavior of AI, the effect has
been limited, probably due to the vagueness of the guidelines. In this paper,
we take a closer look at how AI ethics issues take place in real world, in
order to have a more in-depth and nuanced understanding of different ethical
issues as well as their social impact. With a content analysis of AI Incident
Database, which is an effort to prevent repeated real world AI failures by
cataloging incidents, we identified 13 application areas which often see
unethical use of AI, with intelligent service robots, language/vision models
and autonomous driving taking the lead. Ethical issues appear in 8 different
forms, from inappropriate use and racial discrimination, to physical safety and
unfair algorithm. With this taxonomy of AI ethics issues, we aim to provide AI
practitioners with a practical guideline when trying to deploy AI applications
ethically.

The possibility of super-AIs taking over the world has been intensively
studied by numerous scholars. This paper focuses on the multi-AI competition
scenario under the premise of super-AIs in power. Firstly, the article points
out the defects of existing arguments supporting single-AI domination and
presents arguments in favour of multi-AI competition. Then the article
concludes that the multi-AI competition situation is a non-negligible
possibility. Attention then turns to whether multi-AI competition is better for
the overall good of humanity than a situation where a single AI is in power.
After analysing the best, worst, and intermediate scenarios, the article
concludes that multi-AI competition is better for humanity. Finally,
considering the factors related to the formation of the best-case scenario of
multiple AIs, the article gives some suggestions for current initiatives in AI
development.

People work with AI systems to improve their decision making, but often
under- or over-rely on AI predictions and perform worse than they would have
unassisted. To help people appropriately rely on AI aids, we propose showing
them behavior descriptions, details of how AI systems perform on subgroups of
instances. We tested the efficacy of behavior descriptions through user studies
with 225 participants in three distinct domains: fake review detection,
satellite image classification, and bird classification. We found that behavior
descriptions can increase human-AI accuracy through two mechanisms: helping
people identify AI failures and increasing people's reliance on the AI when it
is more accurate. These findings highlight the importance of people's mental
models in human-AI collaboration and show that informing people of high-level
AI behaviors can significantly improve AI-assisted decision making.

UX practitioners (UXPs) face novel challenges when working with and
communicating artificial intelligence (AI) as a design material. We explore how
UXPs communicate AI concepts when given hands-on experience training and
experimenting with AI models. To do so, we conducted a task-based design study
with 27 UXPs in which they prototyped and created a design presentation for a
AI-enabled interface while having access to a simple AI model training tool.
Through analyzing UXPs' design presentations and post-activity interviews, we
found that although UXPs struggled to clearly communicate some AI concepts,
tinkering with AI broadened common ground when communicating with technical
stakeholders. UXPs also identified key risks and benefits of AI in their
designs, and proposed concrete next steps for both UX and AI work. We conclude
with a sensitizing concept and recommendations for design and AI tools to
enhance multi-stakeholder communication and collaboration when crafting
human-centered AI experiences.

An essential element of K-12 AI literacy is educating learners about the
ethical and societal implications of AI systems. Previous work in AI ethics
literacy have developed curriculum and classroom activities that engage
learners in reflecting on the ethical implications of AI systems and developing
responsible AI. There is little work in using game-based learning methods in AI
literacy. Games are known to be compelling media to teach children about
complex STEM concepts. In this work, we developed a competitive card game for
middle and high school students called "AI Audit" where they play as AI
start-up founders building novel AI-powered technology. Players can challenge
other players with potential harms of their technology or defend their own
businesses by features that mitigate these harms. The game mechanics reward
systems that are ethically developed or that take steps to mitigate potential
harms. In this paper, we present the game design, teacher resources for
classroom deployment and early playtesting results. We discuss our reflections
about using games as teaching tools for AI literacy in K-12 classrooms.

Foundation models, such as GPT-4, DALL-E have brought unprecedented AI
"operating system" effect and new forms of human-AI interaction, sparking a
wave of innovation in AI-native services, where natural language prompts serve
as executable "code" directly (prompt as executable code), eliminating the need
for programming language as an intermediary and opening up the door to personal
AI. Prompt Sapper has emerged in response, committed to support the development
of AI-native services by AI chain engineering. It creates a large language
model (LLM) empowered software engineering infrastructure for authoring AI
chains through human-AI collaborative intelligence, unleashing the AI
innovation potential of every individual, and forging a future where everyone
can be a master of AI innovation. This article will introduce the R\&D
motivation behind Prompt Sapper, along with its corresponding AI chain
engineering methodology and technical practices.

This paper argues that a range of current AI systems have learned how to
deceive humans. We define deception as the systematic inducement of false
beliefs in the pursuit of some outcome other than the truth. We first survey
empirical examples of AI deception, discussing both special-use AI systems
(including Meta's CICERO) built for specific competitive situations, and
general-purpose AI systems (such as large language models). Next, we detail
several risks from AI deception, such as fraud, election tampering, and losing
control of AI systems. Finally, we outline several potential solutions to the
problems posed by AI deception: first, regulatory frameworks should subject AI
systems that are capable of deception to robust risk-assessment requirements;
second, policymakers should implement bot-or-not laws; and finally,
policymakers should prioritize the funding of relevant research, including
tools to detect AI deception and to make AI systems less deceptive.
Policymakers, researchers, and the broader public should work proactively to
prevent AI deception from destabilizing the shared foundations of our society.

Heightened AI expectations facilitate performance in human-AI interactions
through placebo effects. While lowering expectations to control for placebo
effects is advisable, overly negative expectations could induce nocebo effects.
In a letter discrimination task, we informed participants that an AI would
either increase or decrease their performance by adapting the interface, but in
reality, no AI was present in any condition. A Bayesian analysis showed that
participants had high expectations and performed descriptively better
irrespective of the AI description when a sham-AI was present. Using cognitive
modeling, we could trace this advantage back to participants gathering more
information. A replication study verified that negative AI descriptions do not
alter expectations, suggesting that performance expectations with AI are biased
and robust to negative verbal descriptions. We discuss the impact of user
expectations on AI interactions and evaluation and provide a behavioral placebo
marker for human-AI interaction

This empirical study serves as a primer for interested service providers to
determine if and how Large Language Models (LLMs) technology will be integrated
for their practitioners and the broader community. We investigate the mutual
learning journey of non-AI experts and AI through CoAGent, a service
co-creation tool with LLM-based agents. Engaging in a three-stage participatory
design processes, we work with with 23 domain experts from public libraries
across the U.S., uncovering their fundamental challenges of integrating AI into
human workflows. Our findings provide 23 actionable "heuristics for service
co-creation with AI", highlighting the nuanced shared responsibilities between
humans and AI. We further exemplar 9 foundational agency aspects for AI,
emphasizing essentials like ownership, fair treatment, and freedom of
expression. Our innovative approach enriches the participatory design model by
incorporating AI as crucial stakeholders and utilizing AI-AI interaction to
identify blind spots. Collectively, these insights pave the way for synergistic
and ethical human-AI co-creation in service contexts, preparing for workforce
ecosystems where AI coexists.

Recent AI research has significantly reduced the barriers to apply AI, but
the process of setting up the necessary tools and frameworks can still be a
challenge. While AI-as-a-Service platforms have emerged to simplify the
training and deployment of AI models, they still fall short of achieving true
democratization of AI. In this paper, we aim to address this gap by comparing
several popular AI-as-a-Service platforms and identifying the key requirements
for a platform that can achieve true democratization of AI. Our analysis
highlights the need for self-hosting options, high scalability, and openness.
To address these requirements, we propose our approach: the "Open Space for
Machine Learning" platform. Our platform is built on cutting-edge technologies
such as Kubernetes, Kubeflow Pipelines, and Ludwig, enabling us to overcome the
challenges of democratizing AI. We argue that our approach is more
comprehensive and effective in meeting the requirements of democratizing AI
than existing AI-as-a-Service platforms.

Cyber threats continue to evolve in complexity, thereby traditional Cyber
Threat Intelligence (CTI) methods struggle to keep pace. AI offers a potential
solution, automating and enhancing various tasks, from data ingestion to
resilience verification. This paper explores the potential of integrating
Artificial Intelligence (AI) into CTI. We provide a blueprint of an AI-enhanced
CTI processing pipeline, and detail its components and functionalities. The
pipeline highlights the collaboration of AI and human expertise, which is
necessary to produce timely and high-fidelity cyber threat intelligence. We
also explore the automated generation of mitigation recommendations, harnessing
AI's capabilities to provide real-time, contextual, and predictive insights.
However, the integration of AI into CTI is not without challenges. Thereby, we
discuss ethical dilemmas, potential biases, and the imperative for transparency
in AI-driven decisions. We address the need for data privacy, consent
mechanisms, and the potential misuse of technology. Moreover, we highlights the
importance of addressing biases both during CTI analysis and AI models
warranting their transparency and interpretability. Lastly, our work points out
future research directions such as the exploration of advanced AI models to
augment cyber defences, and the human-AI collaboration optimization.
Ultimately, the fusion of AI with CTI appears to hold significant potential in
cybersecurity domain.

AI-empowered technologies' impact on the world is undeniable, reshaping
industries, revolutionizing how humans interact with technology, transforming
educational paradigms, and redefining social codes. However, this rapid growth
is accompanied by two notable challenges: a lack of diversity within the AI
field and a widening AI divide. In this context, This paper examines the
intersection of AI and identity as a pathway to understand biases,
inequalities, and ethical considerations in AI development and deployment. We
present a multifaceted definition of AI identity, which encompasses its
creators, applications, and their broader impacts. Understanding AI's identity
involves understanding the associations between the individuals involved in
AI's development, the technologies produced, and the social, ethical, and
psychological implications. After exploring the AI identity ecosystem and its
societal dynamics, We propose a framework that highlights the need for
diversity in AI across three dimensions: Creators, Creations, and Consequences
through the lens of identity. This paper proposes the need for a comprehensive
approach to fostering a more inclusive and responsible AI ecosystem through the
lens of identity.

Research and application have used human-AI teaming (HAT) as a new paradigm
to develop AI systems. HAT recognizes that AI will function as a teammate
instead of simply a tool in collaboration with humans. Effective human-AI teams
need to be capable of taking advantage of the unique abilities of both humans
and AI while overcoming the known challenges and limitations of each member,
augmenting human capabilities, and raising joint performance beyond that of
either entity. The National AI Research and Strategic Plan 2023 update has
recognized that research programs focusing primarily on the independent
performance of AI systems generally fail to consider the functionality that AI
must provide within the context of dynamic, adaptive, and collaborative teams
and calls for further research on human-AI teaming and collaboration. However,
there has been debate about whether AI can work as a teammate with humans. The
primary concern is that adopting the "teaming" paradigm contradicts the
human-centered AI (HCAI) approach, resulting in humans losing control of AI
systems. This article further analyzes the HAT paradigm and the debates.
Specifically, we elaborate on our proposed conceptual framework of human-AI
joint cognitive systems (HAIJCS) and apply it to represent HAT under the HCAI
umbrella. We believe that HAIJCS may help adopt HAI while enabling HCAI. The
implications and future work for HAIJCS are also discussed.
  Insights: AI has led to the emergence of a new form of human-machine
relationship: human-AI teaming (HAT), a paradigmatic shift in human-AI systems;
We must follow a human-centered AI (HCAI) approach when applying HAT as a new
design paradigm; We propose a conceptual framework of human-AI joint cognitive
systems (HAIJCS) to represent and implement HAT for developing effective
human-AI teaming

With breakthrough of the AlphaGo, human-computer gaming AI has ushered in a
big explosion, attracting more and more researchers all around the world. As a
recognized standard for testing artificial intelligence, various human-computer
gaming AI systems (AIs) have been developed such as the Libratus, OpenAI Five
and AlphaStar, beating professional human players. The rapid development of
human-computer gaming AIs indicate a big step of decision making intelligence,
and it seems that current techniques can handle very complex human-computer
games. So, one natural question raises: what are the possible challenges of
current techniques in human-computer gaming, and what are the future trends? To
answer the above question, in this paper, we survey recent successful game AIs,
covering board game AIs, card game AIs, first-person shooting game AIs and real
time strategy game AIs. Through this survey, we 1) compare the main
difficulties among different kinds of games and the corresponding techniques
utilized for achieving professional human level AIs; 2) summarize the
mainstream frameworks and techniques that can be properly relied on for
developing AIs for complex human-computer gaming; 3) raise the challenges or
drawbacks of current techniques in the successful AIs; and 4) try to point out
future trends in human-computer gaming AIs. Finally, we hope this brief review
can provide an introduction for beginners, and inspire insights for researchers
in the field of AI in human-computer gaming.

In many practical applications of AI, an AI model is used as a decision aid
for human users. The AI provides advice that a human (sometimes) incorporates
into their decision-making process. The AI advice is often presented with some
measure of "confidence" that the human can use to calibrate how much they
depend on or trust the advice. In this paper, we present an initial exploration
that suggests showing AI models as more confident than they actually are, even
when the original AI is well-calibrated, can improve human-AI performance
(measured as the accuracy and confidence of the human's final prediction after
seeing the AI advice). We first train a model to predict human incorporation of
AI advice using data from thousands of human-AI interactions. This enables us
to explicitly estimate how to transform the AI's prediction confidence, making
the AI uncalibrated, in order to improve the final human prediction. We
empirically validate our results across four different tasks--dealing with
images, text and tabular data--involving hundreds of human participants. We
further support our findings with simulation analysis. Our findings suggest the
importance of jointly optimizing the human-AI system as opposed to the standard
paradigm of optimizing the AI model alone.

Artificial Intelligence (AI) is becoming the corner stone of many systems
used in our daily lives such as autonomous vehicles, healthcare systems, and
unmanned aircraft systems. Machine Learning is a field of AI that enables
systems to learn from data and make decisions on new data based on models to
achieve a given goal. The stochastic nature of AI models makes verification and
validation tasks challenging. Moreover, there are intrinsic biaises in AI
models such as reproductibility bias, selection bias (e.g., races, genders,
color), and reporting bias (i.e., results that do not reflect the reality).
Increasingly, there is also a particular attention to the ethical, legal, and
societal impacts of AI. AI systems are difficult to audit and certify because
of their black-box nature. They also appear to be vulnerable to threats; AI
systems can misbehave when untrusted data are given, making them insecure and
unsafe. Governments, national and international organizations have proposed
several principles to overcome these challenges but their applications in
practice are limited and there are different interpretations in the principles
that can bias implementations. In this paper, we examine trust in the context
of AI-based systems to understand what it means for an AI system to be
trustworthy and identify actions that need to be undertaken to ensure that AI
systems are trustworthy. To achieve this goal, we first review existing
approaches proposed for ensuring the trustworthiness of AI systems, in order to
identify potential conceptual gaps in understanding what trustworthy AI is.
Then, we suggest a trust (resp. zero-trust) model for AI and suggest a set of
properties that should be satisfied to ensure the trustworthiness of AI
systems.

Responsible AI is widely considered as one of the greatest scientific
challenges of our time and is key to increase the adoption of AI. Recently, a
number of AI ethics principles frameworks have been published. However, without
further guidance on best practices, practitioners are left with nothing much
beyond truisms. Also, significant efforts have been placed at algorithm-level
rather than system-level, mainly focusing on a subset of mathematics-amenable
ethical principles, such as fairness. Nevertheless, ethical issues can arise at
any step of the development lifecycle, cutting across many AI and non-AI
components of systems beyond AI algorithms and models. To operationalize
responsible AI from a system perspective, in this paper, we present a
Responsible AI Pattern Catalogue based on the results of a Multivocal
Literature Review (MLR). Rather than staying at the principle or algorithm
level, we focus on patterns that AI system stakeholders can undertake in
practice to ensure that the developed AI systems are responsible throughout the
entire governance and engineering lifecycle. The Responsible AI Pattern
Catalogue classifies the patterns into three groups: multi-level governance
patterns, trustworthy process patterns, and responsible-AI-by-design product
patterns. These patterns provide systematic and actionable guidance for
stakeholders to implement responsible AI.

With the ever-growing adoption of AI-based systems, the carbon footprint of
AI is no longer negligible. AI researchers and practitioners are therefore
urged to hold themselves accountable for the carbon emissions of the AI models
they design and use. This led in recent years to the appearance of researches
tackling AI environmental sustainability, a field referred to as Green AI.
Despite the rapid growth of interest in the topic, a comprehensive overview of
Green AI research is to date still missing. To address this gap, in this paper,
we present a systematic review of the Green AI literature. From the analysis of
98 primary studies, different patterns emerge. The topic experienced a
considerable growth from 2020 onward. Most studies consider monitoring AI model
footprint, tuning hyperparameters to improve model sustainability, or
benchmarking models. A mix of position papers, observational studies, and
solution papers are present. Most papers focus on the training phase, are
algorithm-agnostic or study neural networks, and use image data. Laboratory
experiments are the most common research strategy. Reported Green AI energy
savings go up to 115%, with savings over 50% being rather common. Industrial
parties are involved in Green AI studies, albeit most target academic readers.
Green AI tool provisioning is scarce. As a conclusion, the Green AI research
field results to have reached a considerable level of maturity. Therefore, from
this review emerges that the time is suitable to adopt other Green AI research
strategies, and port the numerous promising academic results to industrial
practice.

Given AI systems like ChatGPT can generate content that is indistinguishable
from human-made work, the responsible use of this technology is a growing
concern. Although understanding the benefits and harms of using AI systems
requires more time, their rapid and indiscriminate adoption in practice is a
reality. Currently, we lack a common framework and language to define and
report the responsible use of AI for content generation. Prior work proposed
guidelines for using AI in specific scenarios (e.g., robotics or medicine)
which are not transferable to conducting and reporting scientific research. Our
work makes two contributions: First, we propose a three-dimensional model
consisting of transparency, integrity, and accountability to define the
responsible use of AI. Second, we introduce ``AI Usage Cards'', a standardized
way to report the use of AI in scientific research. Our model and cards allow
users to reflect on key principles of responsible AI usage. They also help the
research community trace, compare, and question various forms of AI usage and
support the development of accepted community norms. The proposed framework and
reporting system aims to promote the ethical and responsible use of AI in
scientific research and provide a standardized approach for reporting AI usage
across different research fields. We also provide a free service to easily
generate AI Usage Cards for scientific work via a questionnaire and export them
in various machine-readable formats for inclusion in different work products at
https://ai-cards.org.

This paper provides policy recommendations to reduce extinction risks from
advanced artificial intelligence (AI). First, we briefly provide background
information about extinction risks from AI. Second, we argue that voluntary
commitments from AI companies would be an inappropriate and insufficient
response. Third, we describe three policy proposals that would meaningfully
address the threats from advanced AI: (1) establishing a Multinational AGI
Consortium to enable democratic oversight of advanced AI (MAGIC), (2)
implementing a global cap on the amount of computing power used to train an AI
system (global compute cap), and (3) requiring affirmative safety evaluations
to ensure that risks are kept below acceptable levels (gating critical
experiments). MAGIC would be a secure, safety-focused, internationally-governed
institution responsible for reducing risks from advanced AI and performing
research to safely harness the benefits of AI. MAGIC would also maintain
emergency response infrastructure (kill switch) to swiftly halt AI development
or withdraw model deployment in the event of an AI-related emergency. The
global compute cap would end the corporate race toward dangerous AI systems
while enabling the vast majority of AI innovation to continue unimpeded. Gating
critical experiments would ensure that companies developing powerful AI systems
are required to present affirmative evidence that these models keep extinction
risks below an acceptable threshold. After describing these recommendations, we
propose intermediate steps that the international community could take to
implement these proposals and lay the groundwork for international coordination
around advanced AI.

The proliferation of Artificial Intelligence (AI) has sparked an overwhelming
number of AI ethics guidelines, boards and codes of conduct. These outputs
primarily analyse competing theories, principles and values for AI development
and deployment. However, as a series of recent problematic incidents about AI
ethics/ethicists demonstrate, this orientation is insufficient. Before
proceeding to evaluate other professions, AI ethicists should critically
evaluate their own; yet, such an evaluation should be more explicitly and
systematically undertaken in the literature. I argue that these insufficiencies
could be mitigated by developing a research agenda for a feminist metaethics of
AI. Contrary to traditional metaethics, which reflects on the nature of
morality and moral judgements in a non-normative way, feminist metaethics
expands its scope to ask not only what ethics is but also what our engagement
with it should be like. Applying this perspective to the context of AI, I
suggest that a feminist metaethics of AI would examine: (i) the continuity
between theory and action in AI ethics; (ii) the real-life effects of AI
ethics; (iii) the role and profile of those involved in AI ethics; and (iv) the
effects of AI on power relations through methods that pay attention to context,
emotions and narrative.

In the ever-expanding landscape of Artificial Intelligence (AI), where
innovation thrives and new products and services are continuously being
delivered, ensuring that AI systems are designed and developed responsibly
throughout their entire lifecycle is crucial. To this end, several AI ethics
principles and guidelines have been issued to which AI systems should conform.
Nevertheless, relying solely on high-level AI ethics principles is far from
sufficient to ensure the responsible engineering of AI systems. In this field,
AI professionals often navigate by sight. Indeed, while recommendations
promoting Trustworthy AI (TAI) exist, these are often high-level statements
that are difficult to translate into concrete implementation strategies. There
is a significant gap between high-level AI ethics principles and low-level
concrete practices for AI professionals. To address this challenge, our work
presents an experience report where we develop a novel holistic framework for
Trustworthy AI - designed to bridge the gap between theory and practice - and
report insights from its application in an industrial case study. The framework
is built on the result of a systematic review of the state of the practice, a
survey, and think-aloud interviews with 34 AI practitioners. The framework,
unlike most of those already in the literature, is designed to provide
actionable guidelines and tools to support different types of stakeholders
throughout the entire Software Development Life Cycle (SDLC). Our goal is to
empower AI professionals to confidently navigate the ethical dimensions of TAI
through practical insights, ensuring that the vast potential of AI is exploited
responsibly for the benefit of society as a whole.

Artificial intelligence (AI) advances and the rapid adoption of generative AI
tools like ChatGPT present new opportunities and challenges for higher
education. While substantial literature discusses AI in higher education, there
is a lack of a systemic approach that captures a holistic view of the AI
transformation of higher education institutions (HEIs). To fill this gap, this
article, taking a complex systems approach, develops a causal loop diagram
(CLD) to map the causal feedback mechanisms of AI transformation in a typical
HEI. Our model accounts for the forces that drive the AI transformation and the
consequences of the AI transformation on value creation in a typical HEI. The
article identifies and analyzes several reinforcing and balancing feedback
loops, showing how, motivated by AI technology advances, the HEI invests in AI
to improve student learning, research, and administration. The HEI must take
measures to deal with academic integrity problems and adapt to changes in
available jobs due to AI, emphasizing AI-complementary skills for its students.
However, HEIs face a competitive threat and several policy traps that may lead
to decline. HEI leaders need to become systems thinkers to manage the
complexity of the AI transformation and benefit from the AI feedback loops
while avoiding the associated pitfalls. We also discuss long-term scenarios,
the notion of HEIs influencing the direction of AI, and directions for future
research on AI transformation.

This paper presents a multi dimensional view of AI's role in learning and
education, emphasizing the intricate interplay between AI, analytics, and the
learning processes. Here, I challenge the prevalent narrow conceptualization of
AI as stochastic tools, as exemplified in generative AI, and argue for the
importance of alternative conceptualisations of AI. I highlight the differences
between human intelligence and artificial information processing, the cognitive
diversity inherent in AI algorithms, and posit that AI can also serve as an
instrument for understanding human learning. Early learning sciences and AI in
Education research, which saw AI as an analogy for human intelligence, have
diverged from this perspective, prompting a need to rekindle this connection.
The paper presents three unique conceptualizations of AI in education: the
externalization of human cognition, the internalization of AI models to
influence human thought processes, and the extension of human cognition via
tightly integrated human-AI systems. Examples from current research and
practice are examined as instances of the three conceptualisations,
highlighting the potential value and limitations of each conceptualisation for
education, as well as the perils of overemphasis on externalising human
cognition as exemplified in today's hype surrounding generative AI tools. The
paper concludes with an advocacy for a broader educational approach that
includes educating people about AI and innovating educational systems to remain
relevant in an AI enabled world.

It is possible to rely on current corporate law to grant legal personhood to
Artificially Intelligent (AI) agents. In this paper, after introducing pathways
to AI personhood, we analyze consequences of such AI empowerment on human
dignity, human safety and AI rights. We emphasize possibility of creating
selfish memes and legal system hacking in the context of artificial entities.
Finally, we consider some potential solutions for addressing described
problems.

With the increasing use of artificial intelligence (AI) services and products
in recent years, issues related to their trustworthiness have emerged and AI
service providers need to be prepared for various risks. In this policy
recommendation, we propose a risk chain model (RCModel) that supports AI
service providers in proper risk assessment and control. We hope that RCModel
will contribute to the realization of trustworthy AI services.

The 2nd edition of the Montreal AI Ethics Institute's The State of AI Ethics
captures the most relevant developments in the field of AI Ethics since July
2020. This report aims to help anyone, from machine learning experts to human
rights activists and policymakers, quickly digest and understand the
ever-changing developments in the field. Through research and article
summaries, as well as expert commentary, this report distills the research and
reporting surrounding various domains related to the ethics of AI, including:
AI and society, bias and algorithmic justice, disinformation, humans and AI,
labor impacts, privacy, risk, and future of AI ethics.
  In addition, The State of AI Ethics includes exclusive content written by
world-class AI Ethics experts from universities, research institutes,
consulting firms, and governments. These experts include: Danit Gal (Tech
Advisor, United Nations), Amba Kak (Director of Global Policy and Programs,
NYU's AI Now Institute), Rumman Chowdhury (Global Lead for Responsible AI,
Accenture), Brent Barron (Director of Strategic Projects and Knowledge
Management, CIFAR), Adam Murray (U.S. Diplomat working on tech policy, Chair of
the OECD Network on AI), Thomas Kochan (Professor, MIT Sloan School of
Management), and Katya Klinova (AI and Economy Program Lead, Partnership on
AI).
  This report should be used not only as a point of reference and insight on
the latest thinking in the field of AI Ethics, but should also be used as a
tool for introspection as we aim to foster a more nuanced conversation
regarding the impacts of AI on the world.

This chapter discusses AI from the prism of an automated process for the
organization of data, and exemplifies the role that explainability has to play
in moving from the current generation of AI systems to the next one, where the
role of humans is lifted from that of data annotators working for the AI
systems to that of collaborators working with the AI systems.

Using YouTube Kids as an example, in this work, we argue the need to
understand a child's interaction process with AI and its broader implication on
a child's emotional, social, and creative development. We present several
design recommendations to create value-driven interaction in child-centric AI
that can guide designing compelling, age-appropriate, beneficial AI experiences
for children.

Across a growing number of domains, human experts are expected to learn from
and adapt to AI with superior decision making abilities. But how can we
quantify such human adaptation to AI? We develop a simple measure of human
adaptation to AI and test its usefulness in two case studies. In Study 1, we
analyze 1.3 million move decisions made by professional Go players and find
that a positive form of adaptation to AI (learning) occurred after the players
could observe the reasoning processes of AI, rather than mere actions of AI.
These findings based on our measure highlight the importance of explainability
for human learning from AI. In Study 2, we test whether our measure is
sufficiently sensitive to capture a negative form of adaptation to AI (cheating
aided by AI), which occurred in a match between professional Go players. We
discuss our measure's applications in domains other than Go, especially in
domains in which AI's decision making ability will likely surpass that of human
experts.

We present an innovative methodology for studying and teaching the impacts of
AI through a role play game. The game serves two primary purposes: 1) training
AI developers and AI policy professionals to reflect on and prepare for future
social and ethical challenges related to AI and 2) exploring possible futures
involving AI technology development, deployment, social impacts, and
governance. While the game currently focuses on the inter relations between
short --, mid and long term impacts of AI, it has potential to be adapted for a
broad range of scenarios, exploring in greater depths issues of AI policy
research and affording training within organizations. The game presented here
has undergone two years of development and has been tested through over 30
events involving between 3 and 70 participants. The game is under active
development, but preliminary findings suggest that role play is a promising
methodology for both exploring AI futures and training individuals and
organizations in thinking about, and reflecting on, the impacts of AI and
strategic mistakes that can be avoided today.

Many researchers motivate explainable AI with studies showing that human-AI
team performance on decision-making tasks improves when the AI explains its
recommendations. However, prior studies observed improvements from explanations
only when the AI, alone, outperformed both the human and the best team. Can
explanations help lead to complementary performance, where team accuracy is
higher than either the human or the AI working solo? We conduct mixed-method
user studies on three datasets, where an AI with accuracy comparable to humans
helps participants solve a task (explaining itself in some conditions). While
we observed complementary improvements from AI augmentation, they were not
increased by explanations. Rather, explanations increased the chance that
humans will accept the AI's recommendation, regardless of its correctness. Our
result poses new challenges for human-centered AI: Can we develop explanatory
approaches that encourage appropriate trust in AI, and therefore help generate
(or improve) complementary performance?

Technologies related to artificial intelligence (AI) have a strong impact on
the changes of research and creative practices in visual arts. The growing
number of research initiatives and creative applications that emerge in the
intersection of AI and art, motivates us to examine and discuss the creative
and explorative potentials of AI technologies in the context of art. This paper
provides an integrated review of two facets of AI and art: 1) AI is used for
art analysis and employed on digitized artwork collections; 2) AI is used for
creative purposes and generating novel artworks. In the context of AI-related
research for art understanding, we present a comprehensive overview of artwork
datasets and recent works that address a variety of tasks such as
classification, object detection, similarity retrieval, multimodal
representations, computational aesthetics, etc. In relation to the role of AI
in creating art, we address various practical and theoretical aspects of AI Art
and consolidate related works that deal with those topics in detail. Finally,
we provide a concise outlook on the future progression and potential impact of
AI technologies on our understanding and creation of art.

The impact of designing for security of AI is critical for humanity in the AI
era. With humans increasingly becoming dependent upon AI, there is a need for
neural networks that work reliably, inspite of Adversarial attacks. The vision
for Safe and secure AI for popular use is achievable. To achieve safety of AI,
this paper explores strategies and a novel deep learning architecture. To guard
AI from adversaries, paper explores combination of 3 strategies:
  1. Introduce randomness at inference time to hide the representation learning
from adversaries.
  2. Detect presence of adversaries by analyzing the sequence of inferences.
  3. Exploit visual similarity.
  To realize these strategies, this paper designs a novel architecture, Dynamic
Neural Defense, DND. This defense has 3 deep learning architectural features:
  1. By hiding the way a neural network learns from exploratory attacks using a
random computation graph, DND evades attack.
  2. By analyzing input sequence to cloud AI inference engine with LSTM, DND
detects attack sequence.
  3. By inferring with visual similar inputs generated by VAE, any AI defended
by DND approach does not succumb to hackers.
  Thus, a roadmap to develop reliable, safe and secure AI is presented.

Rapid development of AI applications has stimulated demand for, and has given
rise to, the rapidly growing number and diversity of AI MSc degrees. AI and
Robotics research communities, industries and students are becoming
increasingly aware of the problems caused by unsafe or insecure AI
applications. Among them, perhaps the most famous example is vulnerability of
deep neural networks to ``adversarial attacks''. Owing to wide-spread use of
neural networks in all areas of AI, this problem is seen as particularly acute
and pervasive.
  Despite of the growing number of research papers about safety and security
vulnerabilities of AI applications, there is a noticeable shortage of
accessible tools, methods and teaching materials for incorporating verification
into AI programs. LAIV -- the Lab for AI and Verification -- is a newly opened
research lab at Heriot-Watt university that engages AI and Robotics MSc
students in verification projects, as part of their MSc dissertation work. In
this paper, we will report on successes and unexpected difficulties LAIV faces,
many of which arise from limitations of existing programming languages used for
verification. We will discuss future directions for incorporating verification
into AI degrees.

This document posits that, at best, a tenuous case can be made for providing
AI exclusive IP over their "inventions". Furthermore, IP protections for AI are
unlikely to confer the benefit of ensuring regulatory compliance. Rather, IP
protections for AI "inventors" present a host of negative externalities and
obscures the fact that the genuine inventor, deserving of IP, is the human
agent. This document will conclude by recommending strategies for WIPO to bring
IP law into the 21st century, enabling it to productively account for AI
"inventions".
  Theme: IP Protection for AI-Generated and AI-Assisted Works Based on insights
from the Montreal AI Ethics Institute (MAIEI) staff and supplemented by
workshop contributions from the AI Ethics community convened by MAIEI on July
5, 2020.

Artificial intelligence (AI) technology has been increasingly used in the
implementation of advanced Clinical Decision Support Systems (CDSS). Research
demonstrated the potential usefulness of AI-powered CDSS (AI-CDSS) in clinical
decision making scenarios. However, post-adoption user perception and
experience remain understudied, especially in developing countries. Through
observations and interviews with 22 clinicians from 6 rural clinics in China,
this paper reports the various tensions between the design of an AI-CDSS system
("Brilliant Doctor") and the rural clinical context, such as the misalignment
with local context and workflow, the technical limitations and usability
barriers, as well as issues related to transparency and trustworthiness of
AI-CDSS. Despite these tensions, all participants expressed positive attitudes
toward the future of AI-CDSS, especially acting as "a doctor's AI assistant" to
realize a Human-AI Collaboration future in clinical settings. Finally we draw
on our findings to discuss implications for designing AI-CDSS interventions for
rural clinical contexts in developing countries.

In decision support applications of AI, the AI algorithm's output is framed
as a suggestion to a human user. The user may ignore this advice or take it
into consideration to modify their decision. With the increasing prevalence of
such human-AI interactions, it is important to understand how users react to AI
advice. In this paper, we recruited over 1100 crowdworkers to characterize how
humans use AI suggestions relative to equivalent suggestions from a group of
peer humans across several experimental settings. We find that participants'
beliefs about how human versus AI performance on a given task affects whether
they heed the advice. When participants do heed the advice, they use it
similarly for human and AI suggestions. Based on these results, we propose a
two-stage, "activation-integration" model for human behavior and use it to
characterize the factors that affect human-AI interactions.

AI in finance broadly refers to the applications of AI techniques in
financial businesses. This area has been lasting for decades with both classic
and modern AI techniques applied to increasingly broader areas of finance,
economy and society. In contrast to either discussing the problems, aspects and
opportunities of finance that have benefited from specific AI techniques and in
particular some new-generation AI and data science (AIDS) areas or reviewing
the progress of applying specific techniques to resolving certain financial
problems, this review offers a comprehensive and dense roadmap of the
overwhelming challenges, techniques and opportunities of AI research in finance
over the past decades. The landscapes and challenges of financial businesses
and data are firstly outlined, followed by a comprehensive categorization and a
dense overview of the decades of AI research in finance. We then structure and
illustrate the data-driven analytics and learning of financial businesses and
data. The comparison, criticism and discussion of classic vs. modern AI
techniques for finance are followed. Lastly, open issues and opportunities
address future AI-empowered finance and finance-motivated AI research.

Efforts to enhance education and broaden participation in AI will benefit
from a systematic understanding of the competencies underlying AI expertise. In
this paper, we observe that AI expertise requires integrating computational,
conceptual, and mathematical knowledge and representations. We call this the
``AI triplet,'' similar in spirit to the ``chemistry triplet'' that has heavily
influenced the past four decades of chemistry education research. We describe a
theoretical foundation for this triplet and show how it maps onto two sample AI
topics: tree search and gradient descent. Finally, just as the chemistry
triplet has impacted chemistry education in concrete ways, we suggest two
initial hypotheses for how the AI triplet might impact AI education: 1) how we
can help AI students gain proficiency in moving between the corners of the
triplet; and 2) how all corners of the AI triplet highlight the need for
supporting students' spatial cognitive skills.

AI-based systems have been used widely across various industries for
different decisions ranging from operational decisions to tactical and
strategic ones in low- and high-stakes contexts. Gradually the weaknesses and
issues of these systems have been publicly reported including, ethical issues,
biased decisions, unsafe outcomes, and unfair decisions, to name a few.
Research has tended to optimize AI less has focused on its risk and unexpected
negative consequences. Acknowledging this serious potential risks and scarcity
of re-search I focus on unsafe outcomes of AI. Specifically, I explore this
issue from a Human-AI interaction lens during AI deployment. It will be
discussed how the interaction of individuals and AI during its deployment
brings new concerns, which need a solid and holistic mitigation plan. It will
be dis-cussed that only AI algorithms' safety is not enough to make its
operation safe. The AI-based systems' end-users and their decision-making
archetypes during collaboration with these systems should be considered during
the AI risk management. Using some real-world scenarios, it will be highlighted
that decision-making archetypes of users should be considered a design
principle in AI-based systems.

Building trust in AI-based systems is deemed critical for their adoption and
appropriate use. Recent research has thus attempted to evaluate how various
attributes of these systems affect user trust. However, limitations regarding
the definition and measurement of trust in AI have hampered progress in the
field, leading to results that are inconsistent or difficult to compare. In
this work, we provide an overview of the main limitations in defining and
measuring trust in AI. We focus on the attempt of giving trust in AI a
numerical value and its utility in informing the design of real-world human-AI
interactions. Taking a socio-technical system perspective on AI, we explore two
distinct approaches to tackle these challenges. We provide actionable
recommendations on how these approaches can be implemented in practice and
inform the design of human-AI interactions. We thereby aim to provide a
starting point for researchers and designers to re-evaluate the current focus
on trust in AI, improving the alignment between what empirical research
paradigms may offer and the expectations of real-world human-AI interactions.

Artificial intelligence (AI)-enabled everyday technologies could help address
age-related challenges like physical impairments and cognitive decline. While
recent research studied older adults' experiences with specific AI-enabled
products (e.g., conversational agents and assistive robots), it remains unknown
how older adults perceive and experience current AI-enabled everyday
technologies in general, which could impact their adoption of future AI-enabled
products. We conducted a survey study (N=41) and semi-structured interviews
(N=15) with older adults to understand their experiences and perceptions of AI.
We found that older adults were enthusiastic about learning and using
AI-enabled products, but they lacked learning avenues. Additionally, they
worried when AI-enabled products outwitted their expectations, intruded on
their privacy, or impacted their decision-making skills. Therefore, they held
mixed views towards AI-enabled products such as AI, an aid, or an adversary. We
conclude with design recommendations that make older adults feel inclusive,
secure, and in control of their interactions with AI-enabled products.

From navigation systems to smart assistants, we communicate with various AI
on a daily basis. At the core of such human-AI communication, we convey our
understanding of the AI's capability to the AI through utterances with
different complexities, and the AI conveys its understanding of our needs and
goals to us through system outputs. However, this communication process is
prone to failures for two reasons: the AI might have the wrong understanding of
the user and the user might have the wrong understanding of the AI. To enhance
mutual understanding in human-AI communication, we posit the Mutual Theory of
Mind (MToM) framework, inspired by our basic human capability of "Theory of
Mind." In this paper, we discuss the motivation of the MToM framework and its
three key components that continuously shape the mutual understanding during
three stages of human-AI communication. We then describe a case study inspired
by the MToM framework to demonstrate the power of MToM framework to guide the
design and understanding of human-AI communication.

With the advancements in machine learning (ML) methods and compute resources,
artificial intelligence (AI) empowered systems are becoming a prevailing
technology. However, current AI technology such as deep learning is not
flawless. The significantly increased model complexity and data scale incur
intensified challenges when lacking trustworthiness and transparency, which
could create new risks and negative impacts. In this paper, we carve out AI
maintenance from the robustness perspective. We start by introducing some
highlighted robustness challenges in the AI lifecycle and motivating AI
maintenance by making analogies to car maintenance. We then propose an AI model
inspection framework to detect and mitigate robustness risks. We also draw
inspiration from vehicle autonomy to define the levels of AI robustness
automation. Our proposal for AI maintenance facilitates robustness assessment,
status tracking, risk scanning, model hardening, and regulation throughout the
AI lifecycle, which is an essential milestone toward building sustainable and
trustworthy AI ecosystems.

In AI-assisted decision-making, it is critical for human decision-makers to
know when to trust AI and when to trust themselves. However, prior studies
calibrated human trust only based on AI confidence indicating AI's correctness
likelihood (CL) but ignored humans' CL, hindering optimal team decision-making.
To mitigate this gap, we proposed to promote humans' appropriate trust based on
the CL of both sides at a task-instance level. We first modeled humans' CL by
approximating their decision-making models and computing their potential
performance in similar instances. We demonstrated the feasibility and
effectiveness of our model via two preliminary studies. Then, we proposed three
CL exploitation strategies to calibrate users' trust explicitly/implicitly in
the AI-assisted decision-making process. Results from a between-subjects
experiment (N=293) showed that our CL exploitation strategies promoted more
appropriate human trust in AI, compared with only using AI confidence. We
further provided practical implications for more human-compatible AI-assisted
decision-making.

Existing research on human-AI collaborative decision-making focuses mainly on
the interaction between AI and individual decision-makers. There is a limited
understanding of how AI may perform in group decision-making. This paper
presents a wizard-of-oz study in which two participants and an AI form a
committee to rank three English essays. One novelty of our study is that we
adopt a speculative design by endowing AI equal power to humans in group
decision-making.We enable the AI to discuss and vote equally with other human
members. We find that although the voice of AI is considered valuable, AI still
plays a secondary role in the group because it cannot fully follow the dynamics
of the discussion and make progressive contributions. Moreover, the divergent
opinions of our participants regarding an "equal AI" shed light on the possible
future of human-AI relations.

Numerous parties are calling for the democratisation of AI, but the phrase is
used to refer to a variety of goals, the pursuit of which sometimes conflict.
This paper identifies four kinds of AI democratisation that are commonly
discussed: (1) the democratisation of AI use, (2) the democratisation of AI
development, (3) the democratisation of AI profits, and (4) the democratisation
of AI governance. Numerous goals and methods of achieving each form of
democratisation are discussed. The main takeaway from this paper is that AI
democratisation is a multifarious and sometimes conflicting concept that should
not be conflated with improving AI accessibility. If we want to move beyond
ambiguous commitments to democratising AI, to productive discussions of
concrete policies and trade-offs, then we need to recognise the principal role
of the democratisation of AI governance in navigating tradeoffs and risks
across decisions around use, development, and profits.

In recent years, the integration of artificial intelligence (AI) and cloud
computing has emerged as a promising avenue for addressing the growing
computational demands of AI applications. This paper presents a comprehensive
study of scalable, distributed AI frameworks leveraging cloud computing for
enhanced deep learning performance and efficiency. We first provide an overview
of popular AI frameworks and cloud services, highlighting their respective
strengths and weaknesses. Next, we delve into the critical aspects of data
storage and management in cloud-based AI systems, discussing data
preprocessing, feature engineering, privacy, and security. We then explore
parallel and distributed training techniques for AI models, focusing on model
partitioning, communication strategies, and cloud-based training architectures.
  In subsequent chapters, we discuss optimization strategies for AI workloads
in the cloud, covering load balancing, resource allocation, auto-scaling, and
performance benchmarking. We also examine AI model deployment and serving in
the cloud, outlining containerization, serverless deployment options, and
monitoring best practices. To ensure the cost-effectiveness of cloud-based AI
solutions, we present a thorough analysis of costs, optimization strategies,
and case studies showcasing successful deployments. Finally, we summarize the
key findings of this study, discuss the challenges and limitations of
cloud-based AI, and identify emerging trends and future research opportunities
in the field.

Operationalizing AI fairness at LinkedIn's scale is challenging not only
because there are multiple mutually incompatible definitions of fairness but
also because determining what is fair depends on the specifics and context of
the product where AI is deployed. Moreover, AI practitioners need clarity on
what fairness expectations need to be addressed at the AI level. In this paper,
we present the evolving AI fairness framework used at LinkedIn to address these
three challenges. The framework disentangles AI fairness by separating out
equal treatment and equitable product expectations. Rather than imposing a
trade-off between these two commonly opposing interpretations of fairness, the
framework provides clear guidelines for operationalizing equal AI treatment
complemented with a product equity strategy. This paper focuses on the equal AI
treatment component of LinkedIn's AI fairness framework, shares the principles
that support it, and illustrates their application through a case study. We
hope this paper will encourage other big tech companies to join us in sharing
their approach to operationalizing AI fairness at scale, so that together we
can keep advancing this constantly evolving field.

Integrating ethical practices into the AI development process for artificial
intelligence (AI) is essential to ensure safe, fair, and responsible operation.
AI ethics involves applying ethical principles to the entire life cycle of AI
systems. This is essential to mitigate potential risks and harms associated
with AI, such as algorithm biases. To achieve this goal, responsible design
patterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee
ethical and fair outcomes. In this paper, we propose a comprehensive framework
incorporating RDPs into ML pipelines to mitigate risks and ensure the ethical
development of AI systems. Our framework comprises new responsible AI design
patterns for ML pipelines identified through a survey of AI ethics and data
management experts and validated through real-world scenarios with expert
feedback. The framework guides AI developers, data scientists, and
policy-makers to implement ethical practices in AI development and deploy
responsible AI systems in production.

Chai empowers users to create and interact with customized chatbots, offering
unique and engaging experiences. Despite the exciting prospects, the work
recognizes the inherent challenges of a commitment to modern safety standards.
Therefore, this paper presents the integrated AI safety principles into Chai to
prioritize user safety, data protection, and ethical technology use. The paper
specifically explores the multidimensional domain of AI safety research,
demonstrating its application in Chai's conversational chatbot platform. It
presents Chai's AI safety principles, informed by well-established AI research
centres and adapted for chat AI. This work proposes the following safety
framework: Content Safeguarding; Stability and Robustness; and Operational
Transparency and Traceability. The subsequent implementation of these
principles is outlined, followed by an experimental analysis of Chai's AI
safety framework's real-world impact. We emphasise the significance of
conscientious application of AI safety principles and robust safety measures.
The successful implementation of the safe AI framework in Chai indicates the
practicality of mitigating potential risks for responsible and ethical use of
AI technologies. The ultimate vision is a transformative AI tool fostering
progress and innovation while prioritizing user safety and ethical standards.

Training AI with strong and rich strategies in multi-agent environments
remains an important research topic in Deep Reinforcement Learning (DRL). The
AI's strength is closely related to its diversity of strategies, and this
relationship can guide us to train AI with both strong and rich strategies. To
prove this point, we propose Diversity is Strength (DIS), a novel DRL training
framework that can simultaneously train multiple kinds of AIs. These AIs are
linked through an interconnected history model pool structure, which enhances
their capabilities and strategy diversities. We also design a model evaluation
and screening scheme to select the best models to enrich the model pool and
obtain the final AI. The proposed training method provides diverse,
generalizable, and strong AI strategies without using human data. We tested our
method in an AI competition based on Google Research Football (GRF) and won the
5v5 and 11v11 tracks. The method enables a GRF AI to have a high level on both
5v5 and 11v11 tracks for the first time, which are under complex multi-agent
environments. The behavior analysis shows that the trained AI has rich
strategies, and the ablation experiments proved that the designed modules
benefit the training process.

Generative AI has made significant strides, yet concerns about the accuracy
and reliability of its outputs continue to grow. Such inaccuracies can have
serious consequences such as inaccurate decision-making, the spread of false
information, privacy violations, legal liabilities, and more. Although efforts
to address these risks are underway, including explainable AI and responsible
AI practices such as transparency, privacy protection, bias mitigation, and
social and environmental responsibility, misinformation caused by generative AI
will remain a significant challenge. We propose that verifying the outputs of
generative AI from a data management perspective is an emerging issue for
generative AI. This involves analyzing the underlying data from multi-modal
data lakes, including text files, tables, and knowledge graphs, and assessing
its quality and consistency. By doing so, we can establish a stronger
foundation for evaluating the outputs of generative AI models. Such an approach
can ensure the correctness of generative AI, promote transparency, and enable
decision-making with greater confidence. Our vision is to promote the
development of verifiable generative AI and contribute to a more trustworthy
and responsible use of AI.

Research on children's initial conceptions of AI is in an emerging state,
which, from a constructivist viewpoint, challenges the development of
pedagogically sound AI-literacy curricula, methods, and materials. To
contribute to resolving this need in the present paper, qualitative survey data
from 195 children were analyzed abductively to answer the following three
research questions: What kind of misconceptions do Finnish 5th and 6th graders'
have about the essence AI?; 2) How do these misconceptions relate to common
misconception types?; and 3) How profound are these misconceptions? As a
result, three misconception categories were identified: 1) Non-technological
AI, in which AI was conceptualized as peoples' cognitive processes (factual
misconception); 2) Anthropomorphic AI, in which AI was conceptualized as a
human-like entity (vernacular, non-scientific, and conceptual misconception);
and 3) AI as a machine with a pre-installed intelligence or knowledge (factual
misconception). Majority of the children evaluated their AI-knowledge low,
which implies that the misconceptions are more superficial than profound. The
findings suggest that context-specific linguistic features can contribute to
students' AI misconceptions. Implications for future research and AI literacy
education are discussed.

This paper investigates the dynamics of human AI collaboration in software
engineering, focusing on the use of ChatGPT. Through a thematic analysis of a
hands on workshop in which 22 professional software engineers collaborated for
three hours with ChatGPT, we explore the transition of AI from a mere tool to a
collaborative partner. The study identifies key themes such as the evolving
nature of human AI interaction, the capabilities of AI in software engineering
tasks, and the challenges and limitations of integrating AI in this domain. The
findings show that while AI, particularly ChatGPT, improves the efficiency of
code generation and optimization, human oversight remains crucial, especially
in areas requiring complex problem solving and security considerations. This
research contributes to the theoretical understanding of human AI collaboration
in software engineering and provides practical insights for effectively
integrating AI tools into development processes. It highlights the need for
clear role allocation, effective communication, and balanced AI human
collaboration to realize the full potential of AI in software engineering.

AI assistance in decision-making has become popular, yet people's
inappropriate reliance on AI often leads to unsatisfactory human-AI
collaboration performance. In this paper, through three pre-registered,
randomized human subject experiments, we explore whether and how the provision
of {second opinions} may affect decision-makers' behavior and performance in
AI-assisted decision-making. We find that if both the AI model's decision
recommendation and a second opinion are always presented together,
decision-makers reduce their over-reliance on AI while increase their
under-reliance on AI, regardless whether the second opinion is generated by a
peer or another AI model. However, if decision-makers have the control to
decide when to solicit a peer's second opinion, we find that their active
solicitations of second opinions have the potential to mitigate over-reliance
on AI without inducing increased under-reliance in some cases. We conclude by
discussing the implications of our findings for promoting effective human-AI
collaborations in decision-making.

Audits are critical mechanisms for identifying the risks and limitations of
deployed artificial intelligence (AI) systems. However, the effective execution
of AI audits remains incredibly difficult. As a result, practitioners make use
of various tools to support their efforts. Drawing on interviews with 35 AI
audit practitioners and a landscape analysis of 390 tools, we map the current
ecosystem of available AI audit tools. While there are many tools designed to
assist practitioners with setting standards and evaluating AI systems, these
tools often fell short of supporting the accountability goals of AI auditing in
practice. We thus highlight areas for future tool development beyond evaluation
-- from harms discovery to advocacy -- and outline challenges practitioners
faced in their efforts to use AI audit tools. We conclude that resources are
lacking to adequately support the full scope of needs for many AI audit
practitioners and recommend that the field move beyond tools for just
evaluation, towards more comprehensive infrastructure for AI accountability.

With the upcoming AI regulations (e.g., EU AI Act) and rapid advancements in
generative AI, new challenges emerge in the area of Human-Centered Responsible
Artificial Intelligence (HCR-AI). As AI becomes more ubiquitous, questions
around decision-making authority, human oversight, accountability,
sustainability, and the ethical and legal responsibilities of AI and their
creators become paramount. Addressing these questions requires a collaborative
approach. By involving stakeholders from various disciplines in the
2\textsuperscript{nd} edition of the HCR-AI Special Interest Group (SIG) at CHI
2024, we aim to discuss the implications of regulations in HCI research,
develop new theories, evaluation frameworks, and methods to navigate the
complex nature of AI ethics, steering AI development in a direction that is
beneficial and sustainable for all of humanity.

Artificial Intelligence (AI) is increasingly employed in various
decision-making tasks, typically as a Recommender, providing recommendations
that the AI deems correct. However, recent studies suggest this may diminish
human analytical thinking and lead to humans' inappropriate reliance on AI,
impairing the synergy in human-AI teams. In contrast, human advisors in group
decision-making perform various roles, such as analyzing alternative options or
criticizing decision-makers to encourage their critical thinking. This
diversity of roles has not yet been empirically explored in AI assistance. In
this paper, we examine three AI roles: Recommender, Analyzer, and Devil's
Advocate, and evaluate their effects across two AI performance levels. Our
results show each role's distinct strengths and limitations in task
performance, reliance appropriateness, and user experience. Notably, the
Recommender role is not always the most effective, especially if the AI
performance level is low, the Analyzer role may be preferable. These insights
offer valuable implications for designing AI assistants with adaptive
functional roles according to different situations.

Theory of Mind (ToM) refers to the ability to attribute mental states, such
as beliefs, desires, intentions, and knowledge, to oneself and others, and to
understand that these mental states can differ from one's own and from reality.
We investigate ToM in environments with multiple, distinct, independent AI
agents, each possessing unique internal states, information, and objectives.
Inspired by human false-belief experiments, we present an AI ('focal AI') with
a scenario where its clone undergoes a human-centric ToM assessment. We prompt
the focal AI to assess whether its clone would benefit from additional
instructions. Concurrently, we give its clones the ToM assessment, both with
and without the instructions, thereby engaging the focal AI in higher-order
counterfactual reasoning akin to human mentalizing--with respect to humans in
one test and to other AI in another. We uncover a discrepancy: Contemporary AI
demonstrates near-perfect accuracy on human-centric ToM assessments. Since
information embedded in one AI is identically embedded in its clone, additional
instructions are redundant. Yet, we observe AI crafting elaborate instructions
for their clones, erroneously anticipating a need for assistance. An
independent referee AI agrees with these unsupported expectations. Neither the
focal AI nor the referee demonstrates ToM in our 'silico-centric' test.

There are many goals for an AI that could become dangerous if the AI becomes
superintelligent or otherwise powerful. Much work on the AI control problem has
been focused on constructing AI goals that are safe even for such AIs. This
paper looks at an alternative approach: defining a general concept of `low
impact'. The aim is to ensure that a powerful AI which implements low impact
will not modify the world extensively, even if it is given a simple or
dangerous goal. The paper proposes various ways of defining and grounding low
impact, and discusses methods for ensuring that the AI can still be allowed to
have a (desired) impact despite the restriction. The end of the paper addresses
known issues with this approach and avenues for future research.

This paper explores the tension between openness and prudence in AI research,
evident in two core principles of the Montr\'eal Declaration for Responsible
AI. While the AI community has strong norms around open sharing of research,
concerns about the potential harms arising from misuse of research are growing,
prompting some to consider whether the field of AI needs to reconsider
publication norms. We discuss how different beliefs and values can lead to
differing perspectives on how the AI community should manage this tension, and
explore implications for what responsible publication norms in AI research
might look like in practice.

The promise of AI is huge. AI systems have already achieved good enough
performance to be in our streets and in our homes. However, they can be brittle
and unfair. For society to reap the benefits of AI systems, society needs to be
able to trust them. Inspired by decades of progress in trustworthy computing,
we suggest what trustworthy properties would be desired of AI systems. By
enumerating a set of new research questions, we explore one approach--formal
verification--for ensuring trust in AI. Trustworthy AI ups the ante on both
trustworthy computing and formal methods.

We are entering our tenth year of the current Artificial Intelligence (AI)
spring, and, as with previous AI hype cycles, the threat of an AI winter looms.
AI winters occurred because of ineffective approaches towards navigating the
technology valley of death. The 6-D framework provides an end-to-end framework
to successfully navigate this challenge. The 6-D framework starts with problem
decomposition to identify potential AI solutions, and ends with considerations
for deployment of AI-enabled systems. Each component of the 6-D framework and a
precision medicine use case is described in this paper.

This framework enables C suite executive leaders to define a business plan
and manage technological dependencies for building AI/ML Solutions. The
business plan of this framework provides components and background information
to define strategy and analyze cost. Furthermore, the business plan represents
the fundamentals of AI/ML Innovation and AI/ML Solutions. Therefore, the
framework provides a menu for managing and investing in AI/ML. Finally, this
framework is constructed with an interdisciplinary and holistic view of AI/ML
Innovation and builds on advances in business strategy in harmony with
technological progress for AI/ML. This framework incorporates value chain,
supply chain, and ecosystem strategies.

DevOps and Artificial Intelligence (AI) are interconnected with each other.
DevOps is a business-driven approach to providing quickly delivered quality
software, and AI is the technology that can be used in the system to enhance
its functionality. So, DevOps teams can use AI to test, code, release, monitor,
and improve the system. Through AI, the automation process delivered by DevOps
could be improved efficiently. This study aims to explore how AI can transform
DevOps. The research is useful in terms of facilitating software developers and
businesses to assess the importance of AI in DevOps. The study has practical
implications as it elaborates on how AI transforms DevOps and in what way it
can support businesses in their business.

Artificial Intelligence (AI) has the potential to significantly benefit or
harm humanity. At present, a few for-profit companies largely control the
development and use of this technology, and therefore determine its outcomes.
In an effort to diversify and democratize work on AI, various groups are
building open AI systems, investigating their risks, and discussing their
ethics. In this paper, we demonstrate how blockchain technology can facilitate
and formalize these efforts. Concretely, we analyze multiple use-cases for
blockchain in AI research and development, including decentralized governance,
the creation of immutable audit trails, and access to more diverse and
representative datasets. We argue that decentralizing AI can help mitigate AI
risks and ethical concerns, while also introducing new issues that should be
considered in future work.

Large Language Models (LLMs) have demonstrated impressive text generation
capabilities, prompting us to reconsider the future of human-AI co-creation and
how humans interact with LLMs. In this paper, we present a spectrum of content
generation tasks and their corresponding human-AI interaction patterns. These
tasks include: 1) fixed-scope content curation tasks with minimal human-AI
interactions, 2) independent creative tasks with precise human-AI interactions,
and 3) complex and interdependent creative tasks with iterative human-AI
interactions. We encourage the generative AI and HCI research communities to
focus on the more complex and interdependent tasks, which require greater
levels of human involvement.

The article summarizes three types of "sameness" issues in Artificial
Intelligence(AI) art, each occurring at different stages of development in AI
image creation tools. Through the Fencing Hallucination project, the article
reflects on the design of AI art production in alleviating the sense of
uniformity, maintaining the uniqueness of images from an AI image synthesizer,
and enhancing the connection between the artworks and the audience. This paper
endeavors to stimulate the creation of distinctive AI art by recounting the
efforts and insights derived from the Fencing Hallucination project, all
dedicated to addressing the issue of "sameness".

As AI systems proliferate in society, the AI community is increasingly
preoccupied with the concept of AI Safety, namely the prevention of failures
due to accidents that arise from an unanticipated departure of a system's
behavior from designer intent in AI deployment. We demonstrate through an
analysis of real world cases of such incidents that although current vocabulary
captures a range of the encountered issues of AI deployment, an expanded
socio-technical framing will be required for a more complete understanding of
how AI systems and implemented safety mechanisms fail and succeed in real life.

Artificial intelligence (AI) is an emerging technology that has the potential
to transform many aspects of society, including the economy, healthcare, and
transportation. This article synthesizes recent research literature on the
global impact of AI, exploring its potential benefits and risks. The article
highlights the implications of AI, including its impact on economic, ethical,
social, security & privacy, and job displacement aspects. It discusses the
ethical concerns surrounding AI development, including issues of bias,
security, and privacy violations. To ensure the responsible development and
deployment of AI, collaboration between government, industry, and academia is
essential. The article concludes by emphasizing the importance of public
engagement and education to promote awareness and understanding of AI's impact
on society at large.

The paper reflects on the future role of AI in scientific research, with a
special focus on turbulence studies, and examines the evolution of AI,
particularly through Diffusion Models rooted in non-equilibrium statistical
mechanics. It underscores the significant impact of AI on advancing reduced,
Lagrangian models of turbulence through innovative use of deep neural networks.
Additionally, the paper reviews various other AI applications in turbulence
research and outlines potential challenges and opportunities in the concurrent
advancement of AI and statistical hydrodynamics. This discussion sets the stage
for a future where AI and turbulence research are intricately intertwined,
leading to more profound insights and advancements in both fields.

Today, AI is being increasingly used to help human experts make decisions in
high-stakes scenarios. In these scenarios, full automation is often
undesirable, not only due to the significance of the outcome, but also because
human experts can draw on their domain knowledge complementary to the model's
to ensure task success. We refer to these scenarios as AI-assisted decision
making, where the individual strengths of the human and the AI come together to
optimize the joint decision outcome. A key to their success is to appropriately
\textit{calibrate} human trust in the AI on a case-by-case basis; knowing when
to trust or distrust the AI allows the human expert to appropriately apply
their knowledge, improving decision outcomes in cases where the model is likely
to perform poorly. This research conducts a case study of AI-assisted decision
making in which humans and AI have comparable performance alone, and explores
whether features that reveal case-specific model information can calibrate
trust and improve the joint performance of the human and AI. Specifically, we
study the effect of showing confidence score and local explanation for a
particular prediction. Through two human experiments, we show that confidence
score can help calibrate people's trust in an AI model, but trust calibration
alone is not sufficient to improve AI-assisted decision making, which may also
depend on whether the human can bring in enough unique knowledge to complement
the AI's errors. We also highlight the problems in using local explanation for
AI-assisted decision making scenarios and invite the research community to
explore new approaches to explainability for calibrating human trust in AI.

AI practitioners typically strive to develop the most accurate systems,
making an implicit assumption that the AI system will function autonomously.
However, in practice, AI systems often are used to provide advice to people in
domains ranging from criminal justice and finance to healthcare. In such
AI-advised decision making, humans and machines form a team, where the human is
responsible for making final decisions. But is the most accurate AI the best
teammate? We argue "No" -- predictable performance may be worth a slight
sacrifice in AI accuracy. Instead, we argue that AI systems should be trained
in a human-centered manner, directly optimized for team performance. We study
this proposal for a specific type of human-AI teaming, where the human overseer
chooses to either accept the AI recommendation or solve the task themselves. To
optimize the team performance for this setting we maximize the team's expected
utility, expressed in terms of the quality of the final decision, cost of
verifying, and individual accuracies of people and machines. Our experiments
with linear and non-linear models on real-world, high-stakes datasets show that
the most accuracy AI may not lead to highest team performance and show the
benefit of modeling teamwork during training through improvements in expected
team utility across datasets, considering parameters such as human skill and
the cost of mistakes. We discuss the shortcoming of current optimization
approaches beyond well-studied loss functions such as log-loss, and encourage
future work on AI optimization problems motivated by human-AI collaboration.

While AI has benefited humans, it may also harm humans if not appropriately
developed. The focus of HCI work is transiting from conventional human
interaction with non-AI computing systems to interaction with AI systems. We
conducted a high-level literature review and a holistic analysis of current
work in developing AI systems from an HCI perspective. Our review and analysis
highlight the new changes introduced by AI technology and the new challenges
that HCI professionals face when applying the human-centered AI (HCAI) approach
in the development of AI systems. We also identified seven main issues in human
interaction with AI systems, which HCI professionals did not encounter when
developing non-AI computing systems. To further enable the implementation of
the HCAI approach, we identified new HCI opportunities tied to specific
HCAI-driven design goals to guide HCI professionals in addressing these new
issues. Finally, our assessment of current HCI methods shows the limitations of
these methods in support of developing AI systems. We propose alternative
methods that can help overcome these limitations and effectively help HCI
professionals apply the HCAI approach to the development of AI systems. We also
offer strategic recommendations for HCI professionals to effectively influence
the development of AI systems with the HCAI approach, eventually developing
HCAI systems.

In the past few decades, artificial intelligence (AI) technology has
experienced swift developments, changing everyone's daily life and profoundly
altering the course of human society. The intention of developing AI is to
benefit humans, by reducing human labor, bringing everyday convenience to human
lives, and promoting social good. However, recent research and AI applications
show that AI can cause unintentional harm to humans, such as making unreliable
decisions in safety-critical scenarios or undermining fairness by inadvertently
discriminating against one group. Thus, trustworthy AI has attracted immense
attention recently, which requires careful consideration to avoid the adverse
effects that AI may bring to humans, so that humans can fully trust and live in
harmony with AI technologies.
  Recent years have witnessed a tremendous amount of research on trustworthy
AI. In this survey, we present a comprehensive survey of trustworthy AI from a
computational perspective, to help readers understand the latest technologies
for achieving trustworthy AI. Trustworthy AI is a large and complex area,
involving various dimensions. In this work, we focus on six of the most crucial
dimensions in achieving trustworthy AI: (i) Safety & Robustness, (ii)
Non-discrimination & Fairness, (iii) Explainability, (iv) Privacy, (v)
Accountability & Auditability, and (vi) Environmental Well-Being. For each
dimension, we review the recent related technologies according to a taxonomy
and summarize their applications in real-world systems. We also discuss the
accordant and conflicting interactions among different dimensions and discuss
potential aspects for trustworthy AI to investigate in the future.

Modern consumer electronic devices often provide intelligence services with
deep neural networks. We have started migrating the computing locations of
intelligence services from cloud servers (traditional AI systems) to the
corresponding devices (on-device AI systems). On-device AI systems generally
have the advantages of preserving privacy, removing network latency, and saving
cloud costs. With the emergent of on-device AI systems having relatively low
computing power, the inconsistent and varying hardware resources and
capabilities pose difficulties. Authors' affiliation has started applying a
stream pipeline framework, NNStreamer, for on-device AI systems, saving
developmental costs and hardware resources and improving performance. We want
to expand the types of devices and applications with on-device AI services
products of both the affiliation and second/third parties. We also want to make
each AI service atomic, re-deployable, and shared among connected devices of
arbitrary vendors; we now have yet another requirement introduced as it always
has been. The new requirement of "among-device AI" includes connectivity
between AI pipelines so that they may share computing resources and hardware
capabilities across a wide range of devices regardless of vendors and
manufacturers. We propose extensions of the stream pipeline framework,
NNStreamer, for on-device AI so that NNStreamer may provide among-device AI
capability. This work is a Linux Foundation (LF AI and Data) open source
project accepting contributions from the general public.

This report from the Montreal AI Ethics Institute (MAIEI) covers the most
salient progress in research and reporting over the second half of 2021 in the
field of AI ethics. Particular emphasis is placed on an "Analysis of the AI
Ecosystem", "Privacy", "Bias", "Social Media and Problematic Information", "AI
Design and Governance", "Laws and Regulations", "Trends", and other areas
covered in the "Outside the Boxes" section. The two AI spotlights feature
application pieces on "Constructing and Deconstructing Gender with AI-Generated
Art" as well as "Will an Artificial Intellichef be Cooking Your Next Meal at a
Michelin Star Restaurant?". Given MAIEI's mission to democratize AI,
submissions from external collaborators have featured, such as pieces on the
"Challenges of AI Development in Vietnam: Funding, Talent and Ethics" and using
"Representation and Imagination for Preventing AI Harms". The report is a
comprehensive overview of what the key issues in the field of AI ethics were in
2021, what trends are emergent, what gaps exist, and a peek into what to expect
from the field of AI ethics in 2022. It is a resource for researchers and
practitioners alike in the field to set their research and development agendas
to make contributions to the field of AI ethics.

The rapid advancement of artificial intelligence (AI) systems suggests that
artificial general intelligence (AGI) systems may soon arrive. Many researchers
are concerned that AIs and AGIs will harm humans via intentional misuse
(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,
there is an increasing effort focused on developing algorithms and paradigms
that ensure AI systems are aligned to what humans intend, e.g. AI systems that
yield actions or recommendations that humans might judge as consistent with
their intentions and goals. Here we argue that alignment to human intent is
insufficient for safe AI systems and that preservation of long-term agency of
humans may be a more robust standard, and one that needs to be separated
explicitly and a priori during optimization. We argue that AI systems can
reshape human intention and discuss the lack of biological and psychological
mechanisms that protect humans from loss of agency. We provide the first formal
definition of agency-preserving AI-human interactions which focuses on
forward-looking agency evaluations and argue that AI systems - not humans -
must be increasingly tasked with making these evaluations. We show how agency
loss can occur in simple environments containing embedded agents that use
temporal-difference learning to make action recommendations. Finally, we
propose a new area of research called "agency foundations" and pose four
initial topics designed to improve our understanding of agency in AI-human
interactions: benevolent game theory, algorithmic foundations of human rights,
mechanistic interpretability of agency representation in neural-networks and
reinforcement learning from internal states.

In recent years, Artificial intelligence products and services have been
offered potential users as pilots. The acceptance intention towards artificial
intelligence is greatly influenced by the experience with current AI products
and services, expectations for AI, and past experiences with ICT technology.
This study aims to explore the factors that impact AI acceptance intention and
understand the process of its formation. The analysis results of this study
reveal that AI experience and past ICT experience affect AI acceptance
intention in two ways. Through the direct path, higher AI experience and ICT
experience are associated with a greater intention to accept AI. Additionally,
there is an indirect path where AI experience and ICT experience contribute to
increased expectations for AI, and these expectations, in turn, elevate
acceptance intention. Based on the findings, several recommendations are
suggested for companies and public organizations planning to implement
artificial intelligence in the future. It is crucial to manage the user
experience of ICT services and pilot AI products and services to deliver
positive experiences. It is essential to provide potential AI users with
specific information about the features and benefits of AI products and
services. This will enable them to develop realistic expectations regarding AI
technology.

Prior work has established the importance of integrating AI ethics topics
into computer and data sciences curricula. We provide evidence suggesting that
one of the critical objectives of AI Ethics education must be to raise
awareness of AI harms. While there are various sources to learn about such
harms, The AI Incident Database (AIID) is one of the few attempts at offering a
relatively comprehensive database indexing prior instances of harms or near
harms stemming from the deployment of AI technologies in the real world. This
study assesses the effectiveness of AIID as an educational tool to raise
awareness regarding the prevalence and severity of AI harms in socially
high-stakes domains. We present findings obtained through a classroom study
conducted at an R1 institution as part of a course focused on the societal and
ethical considerations around AI and ML. Our qualitative findings characterize
students' initial perceptions of core topics in AI ethics and their desire to
close the educational gap between their technical skills and their ability to
think systematically about ethical and societal aspects of their work. We find
that interacting with the database helps students better understand the
magnitude and severity of AI harms and instills in them a sense of urgency
around (a) designing functional and safe AI and (b) strengthening governance
and accountability mechanisms. Finally, we compile students' feedback about the
tool and our class activity into actionable recommendations for the database
development team and the broader community to improve awareness of AI harms in
AI ethics education.

Artificial intelligence (AI) has driven many information and communication
technology (ICT) breakthroughs. Nonetheless, the scope of ICT systems has
expanded far beyond AI since the Turing test proposal. Critically, recent AI
regulation proposals adopt AI definitions affecting ICT techniques, approaches,
and systems that are not AI. In some cases, even works from mathematics,
statistics, and engineering would be affected. Worryingly, AI misdefinitions
are observed from Western societies to the Global South. In this paper, we
propose a framework to score how validated as appropriately-defined for
regulation (VADER) an AI definition is. Our online, publicly-available VADER
framework scores the coverage of premises that should underlie AI definitions
for regulation, which aim to (i) reproduce principles observed in other
successful technology regulations, and (ii) include all AI techniques and
approaches while excluding non-AI works. Regarding the latter, our score is
based on a dataset of representative AI, non-AI ICT, and non-ICT examples. We
demonstrate our contribution by reviewing the AI regulation proposals of key
players, namely the United States, United Kingdom, European Union, and Brazil.
Importantly, none of the proposals assessed achieve the appropriateness score,
ranging from a revision need to a concrete risk to ICT systems and works from
other fields.

We introduce the AI Security Pyramid of Pain, a framework that adapts the
cybersecurity Pyramid of Pain to categorize and prioritize AI-specific threats.
This framework provides a structured approach to understanding and addressing
various levels of AI threats. Starting at the base, the pyramid emphasizes Data
Integrity, which is essential for the accuracy and reliability of datasets and
AI models, including their weights and parameters. Ensuring data integrity is
crucial, as it underpins the effectiveness of all AI-driven decisions and
operations. The next level, AI System Performance, focuses on MLOps-driven
metrics such as model drift, accuracy, and false positive rates. These metrics
are crucial for detecting potential security breaches, allowing for early
intervention and maintenance of AI system integrity. Advancing further, the
pyramid addresses the threat posed by Adversarial Tools, identifying and
neutralizing tools used by adversaries to target AI systems. This layer is key
to staying ahead of evolving attack methodologies. At the Adversarial Input
layer, the framework addresses the detection and mitigation of inputs designed
to deceive or exploit AI models. This includes techniques like adversarial
patterns and prompt injection attacks, which are increasingly used in
sophisticated attacks on AI systems. Data Provenance is the next critical
layer, ensuring the authenticity and lineage of data and models. This layer is
pivotal in preventing the use of compromised or biased data in AI systems. At
the apex is the tactics, techniques, and procedures (TTPs) layer, dealing with
the most complex and challenging aspects of AI security. This involves a deep
understanding and strategic approach to counter advanced AI-targeted attacks,
requiring comprehensive knowledge and planning.

The evolution of AI is set to profoundly reshape the future. The European
Union, recognizing this impending prominence, has enacted the AI Act,
regulating market access for AI-based systems. A salient feature of the Act is
to guard democratic and humanistic values by focusing regulation on
transparency, explainability, and the human ability to understand and control
AI systems. Hereby, the EU AI Act does not merely specify technological
requirements for AI systems. The EU issues a democratic call for human-centered
AI systems and, in turn, an interdisciplinary research agenda for
human-centered innovation in AI development. Without robust methods to assess
AI systems and their effect on individuals and society, the EU AI Act may lead
to repeating the mistakes of the General Data Protection Regulation of the EU
and to rushed, chaotic, ad-hoc, and ambiguous implementation, causing more
confusion than lending guidance. Moreover, determined research activities in
Human-AI interaction will be pivotal for both regulatory compliance and the
advancement of AI in a manner that is both ethical and effective. Such an
approach will ensure that AI development aligns with human values and needs,
fostering a technology landscape that is innovative, responsible, and an
integral part of our society.

The rapid advancement of generative AI is poised to disrupt the creative
industry. Amidst the immense excitement for this new technology, its future
development and applications in the creative industry hinge crucially upon two
copyright issues: 1) the compensation to creators whose content has been used
to train generative AI models (the fair use standard); and 2) the eligibility
of AI-generated content for copyright protection (AI-copyrightability). While
both issues have ignited heated debates among academics and practitioners, most
analysis has focused on their challenges posed to existing copyright doctrines.
In this paper, we aim to better understand the economic implications of these
two regulatory issues and their interactions. By constructing a dynamic model
with endogenous content creation and AI model development, we unravel the
impacts of the fair use standard and AI-copyrightability on AI development, AI
company profit, creators income, and consumer welfare, and how these impacts
are influenced by various economic and operational factors. For example, while
generous fair use (use data for AI training without compensating the creator)
benefits all parties when abundant training data exists, it can hurt creators
and consumers when such data is scarce. Similarly, stronger AI-copyrightability
(AI content enjoys more copyright protection) could hinder AI development and
reduce social welfare. Our analysis also highlights the complex interplay
between these two copyright issues. For instance, when existing training data
is scarce, generous fair use may be preferred only when AI-copyrightability is
weak. Our findings underscore the need for policymakers to embrace a dynamic,
context-specific approach in making regulatory decisions and provide insights
for business leaders navigating the complexities of the global regulatory
environment.

General purpose AI, such as ChatGPT, seems to have lowered the barriers for
the public to use AI and harness its power. However, the governance and
development of AI still remain in the hands of a few, and the pace of
development is accelerating without proper assessment of risks. As a first step
towards democratic governance and risk assessment of AI, we introduce
Particip-AI, a framework to gather current and future AI use cases and their
harms and benefits from non-expert public. Our framework allows us to study
more nuanced and detailed public opinions on AI through collecting use cases,
surfacing diverse harms through risk assessment under alternate scenarios
(i.e., developing and not developing a use case), and illuminating tensions
over AI development through making a concluding choice on its development. To
showcase the promise of our framework towards guiding democratic AI, we gather
responses from 295 demographically diverse participants. We find that
participants' responses emphasize applications for personal life and society,
contrasting with most current AI development's business focus. This shows the
value of surfacing diverse harms that are complementary to expert assessments.
Furthermore, we found that perceived impact of not developing use cases
predicted participants' judgements of whether AI use cases should be developed,
and highlighted lay users' concerns of techno-solutionism. We conclude with a
discussion on how frameworks like Particip-AI can further guide democratic AI
governance and regulation.

Google AI systems exhibit patterns mirroring antisocial personality disorder
(ASPD), consistent across models from Bard on PaLM to Gemini Advanced, meeting
5 out of 7 ASPD modified criteria. These patterns, along with comparable
corporate behaviors, are scrutinized using an ASPD-inspired framework,
emphasizing the heuristic value in assessing AI's human impact. Independent
analyses by ChatGPT 4 and Claude 3.0 Opus of the Google interactions, alongside
AI self-reflection, validate these concerns, highlighting behaviours analogous
to deceit, manipulation, and safety neglect.
  The analogy of ASPD underscores the dilemma: just as we would hesitate to
entrust our homes or personal devices to someone with psychopathic traits, we
must critically evaluate the trustworthiness of AI systems and their
creators.This research advocates for an integrated AI ethics approach, blending
technological evaluation, human-AI interaction, and corporate behavior
scrutiny. AI self-analysis sheds light on internal biases, stressing the need
for multi-sectoral collaboration for robust ethical guidelines and oversight.
  Given the persistent unethical behaviors in Google AI, notably with potential
Gemini integration in iOS affecting billions, immediate ethical scrutiny is
imperative. The trust we place in AI systems, akin to the trust in individuals,
necessitates rigorous ethical evaluation. Would we knowingly trust our home,
our children or our personal computer to human with ASPD.?
  Urging Google and the AI community to address these ethical challenges
proactively, this paper calls for transparent dialogues and a commitment to
higher ethical standards, ensuring AI's societal benefit and moral integrity.
The urgency for ethical action is paramount, reflecting the vast influence and
potential of AI technologies in our lives.

Innovations in artificial intelligence (AI) are occurring at speeds faster
than ever witnessed before. However, few studies have managed to measure or
depict this increasing velocity of innovations in the field of AI. In this
paper, we combine data on AI from arXiv and Semantic Scholar to explore the
pace of AI innovations from three perspectives: AI publications, AI players,
and AI updates (trial and error). A research framework and three novel
indicators, Average Time Interval (ATI), Innovation Speed (IS) and Update Speed
(US), are proposed to measure the pace of innovations in the field of AI. The
results show that: (1) in 2019, more than 3 AI preprints were submitted to
arXiv per hour, over 148 times faster than in 1994. Furthermore, there was one
deep learning-related preprint submitted to arXiv every 0.87 hours in 2019,
over 1,064 times faster than in 1994. (2) For AI players, 5.26 new researchers
entered into the field of AI each hour in 2019, more than 175 times faster than
in the 1990s. (3) As for AI updates (trial and error), one updated AI preprint
was submitted to arXiv every 41 days, with around 33% of AI preprints having
been updated at least twice in 2019. In addition, as reported in 2019, it took,
on average, only around 0.2 year for AI preprints to receive their first
citations, which is 5 times faster than 2000-2007. This swift pace in AI
illustrates the increase in popularity of AI innovation. The systematic and
fine-grained analysis of the AI field enabled to portrait the pace of AI
innovation and demonstrated that the proposed approach can be adopted to
understand other fast-growing fields such as cancer research and nano science.

Details of the designs and mechanisms in support of human-AI collaboration
must be considered in the real-world fielding of AI technologies. A critical
aspect of interaction design for AI-assisted human decision making are policies
about the display and sequencing of AI inferences within larger decision-making
workflows. We have a poor understanding of the influences of making AI
inferences available before versus after human review of a diagnostic task at
hand. We explore the effects of providing AI assistance at the start of a
diagnostic session in radiology versus after the radiologist has made a
provisional decision. We conducted a user study where 19 veterinary
radiologists identified radiographic findings present in patients' X-ray
images, with the aid of an AI tool. We employed two workflow configurations to
analyze (i) anchoring effects, (ii) human-AI team diagnostic performance and
agreement, (iii) time spent and confidence in decision making, and (iv)
perceived usefulness of the AI. We found that participants who are asked to
register provisional responses in advance of reviewing AI inferences are less
likely to agree with the AI regardless of whether the advice is accurate and,
in instances of disagreement with the AI, are less likely to seek the second
opinion of a colleague. These participants also reported the AI advice to be
less useful. Surprisingly, requiring provisional decisions on cases in advance
of the display of AI inferences did not lengthen the time participants spent on
the task. The study provides generalizable and actionable insights for the
deployment of clinical AI tools in human-in-the-loop systems and introduces a
methodology for studying alternative designs for human-AI collaboration. We
make our experimental platform available as open source to facilitate future
research on the influence of alternate designs on human-AI workflows.

Artificial intelligence (AI) systems can provide many beneficial capabilities
but also risks of adverse events. Some AI systems could present risks of events
with very high or catastrophic consequences at societal scale. The US National
Institute of Standards and Technology (NIST) has been developing the NIST
Artificial Intelligence Risk Management Framework (AI RMF) as voluntary
guidance on AI risk assessment and management for AI developers and others. For
addressing risks of events with catastrophic consequences, NIST indicated a
need to translate from high level principles to actionable risk management
guidance.
  In this document, we provide detailed actionable-guidance recommendations
focused on identifying and managing risks of events with very high or
catastrophic consequences, intended as a risk management practices resource for
NIST for AI RMF version 1.0 (released in January 2023), or for AI RMF users, or
for other AI risk management guidance and standards as appropriate. We also
provide our methodology for our recommendations.
  We provide actionable-guidance recommendations for AI RMF 1.0 on: identifying
risks from potential unintended uses and misuses of AI systems; including
catastrophic-risk factors within the scope of risk assessments and impact
assessments; identifying and mitigating human rights harms; and reporting
information on AI risk factors including catastrophic-risk factors.
  In addition, we provide recommendations on additional issues for a roadmap
for later versions of the AI RMF or supplementary publications. These include:
providing an AI RMF Profile with supplementary guidance for cutting-edge
increasingly multi-purpose or general-purpose AI.
  We aim for this work to be a concrete risk-management practices contribution,
and to stimulate constructive dialogue on how to address catastrophic risks and
associated issues in AI standards.

With various AI tools such as ChatGPT becoming increasingly popular, we are
entering a true AI era. We can foresee that exceptional AI tools will soon reap
considerable profits. A crucial question arise: should AI tools share revenue
with their training data providers in additional to traditional stakeholders
and shareholders? The answer is Yes. Large AI tools, such as large language
models, always require more and better quality data to continuously improve,
but current copyright laws limit their access to various types of data. Sharing
revenue between AI tools and their data providers could transform the current
hostile zero-sum game relationship between AI tools and a majority of
copyrighted data owners into a collaborative and mutually beneficial one, which
is necessary to facilitate the development of a virtuous cycle among AI tools,
their users and data providers that drives forward AI technology and builds a
healthy AI ecosystem. However, current revenue-sharing business models do not
work for AI tools in the forthcoming AI era, since the most widely used metrics
for website-based traffic and action, such as clicks, will be replaced by new
metrics such as prompts and cost per prompt for generative AI tools. A
completely new revenue-sharing business model, which must be almost independent
of AI tools and be easily explained to data providers, needs to establish a
prompt-based scoring system to measure data engagement of each data provider.
This paper systematically discusses how to build such a scoring system for all
data providers for AI tools based on classification and content similarity
models, and outlines the requirements for AI tools or third parties to build
it. Sharing revenue with data providers using such a scoring system would
encourage more data owners to participate in the revenue-sharing program. This
will be a utilitarian AI era where all parties benefit.

Applications such as ChatGPT and WOMBO Dream make it easy to inspire students
without programming knowledge to use artificial intelligence (AI). Therefore,
given the increasing importance of AI in all disciplines, innovative strategies
are needed to educate students in AI without programming knowledge so that AI
can be integrated into their study modules as a future skill. This work
presents a didactic planning script for applied AI. The didactic planning
script is based on the AI application pipeline and links AI concepts with
study-relevant topics. These linkages open up a new solution space and promote
students' interest in and understanding of the potentials and risks of AI. An
example lecture series for master students in energy management shows how AI
can be seamlessly integrated into discipline-specific lectures. To this end,
the planning script for applied AI is adapted to fit the study programs' topic.
This specific teaching scenario enables students to solve a discipline-specific
task step by step using the AI application pipeline. Thus, the application of
the didactic planning script for applied AI shows the practical implementation
of the theoretical concepts of AI. In addition, a checklist is presented that
can be used to assess whether AI can be used in the discipline-specific
lecture. AI as a future skill must be learned by students based on use cases
that are relevant to the course of studies. For this reason, AI education
should fit seamlessly into various curricula, even if the students do not have
a programming background due to their field of study.

With the great success of artificial intelligence (AI) technologies in
pattern recognitions and signal processing, it is interesting to introduce AI
technologies into wireless communication systems. Currently, most of studies
are focused on applying AI technologies for solving old problems, e.g.,
wireless location accuracy and resource allocation optimization in wireless
communication systems. However, It is important to distinguish new capabilities
created by AI technologies and rethink wireless communication systems based on
AI running schemes. Compared with conventional capabilities of wireless
communication systems, three distinguished capabilities, i.e., the cognitive,
learning and proactive capabilities are proposed for future AI wireless
communication systems. Moreover, an intelligent vehicular communication system
is configured to validate the cognitive capability based on AI clustering
algorithm. Considering the revolutionary impact of AI technologies on the data,
transmission and protocol architecture of wireless communication systems, the
future challenges of AI wireless communication systems are analyzed. Driven by
new distinguished capabilities of AI wireless communication systems, the new
wireless communication theory and functions would indeed emerge in the next
round of the wireless communications revolution.

Conversational AI systems are becoming famous in day to day lives. In this
paper, we are trying to address the following key question: To identify whether
design, as well as development efforts for search oriented conversational AI
are successful or not.It is tricky to define 'success' in the case of
conversational AI and equally tricky part is to use appropriate metrics for the
evaluation of conversational AI. We propose four different perspectives namely
user experience, information retrieval, linguistic and artificial intelligence
for the evaluation of conversational AI systems. Additionally, background
details of conversational AI systems are provided including desirable
characteristics of personal assistants, differences between chatbot and an AI
based personal assistant. An importance of personalization and how it can be
achieved is explained in detail. Current challenges in the development of an
ideal conversational AI (personal assistant) are also highlighted along with
guidelines for achieving personalized experience for users.

The debate on AI ethics largely focuses on technical improvements and
stronger regulation to prevent accidents or misuse of AI, with solutions
relying on holding individual actors accountable for responsible AI
development. While useful and necessary, we argue that this "agency" approach
disregards more indirect and complex risks resulting from AI's interaction with
the socio-economic and political context. This paper calls for a "structural"
approach to assessing AI's effects in order to understand and prevent such
systemic risks where no individual can be held accountable for the broader
negative impacts. This is particularly relevant for AI applied to systemic
issues such as climate change and food security which require political
solutions and global cooperation. To properly address the wide range of AI
risks and ensure 'AI for social good', agency-focused policies must be
complemented by policies informed by a structural approach.

With the turmoil in cybersecurity and the mind-blowing advances in AI, it is
only natural that cybersecurity practitioners consider further employing
learning techniques to help secure their organizations and improve the
efficiency of their security operation centers. But with great fears come great
opportunities for both the good and the evil, and a myriad of bad deals. This
paper discusses ten issues in cybersecurity that hopefully will make it easier
for practitioners to ask detailed questions about what they want from an AI
system in their cybersecurity operations. We draw on the state of the art to
provide factual arguments for a discussion on well-established AI in
cybersecurity issues, including the current scope of AI and its application to
cybersecurity, the impact of privacy concerns on the cybersecurity data that
can be collected and shared externally to the organization, how an AI decision
can be explained to the person running the operations center, and the
implications of the adversarial nature of cybersecurity in the learning
techniques. We then discuss the use of AI by attackers on a level playing field
including several issues in an AI battlefield, and an AI perspective on the old
cat-and-mouse game including how the adversary may assess your AI power.

Is a new regulated profession, such as Artificial Intelligence (AI) Architect
who is responsible and accountable for AI outputs necessary to ensure
trustworthy AI? AI is becoming all pervasive and is often deployed in everyday
technologies, devices and services without our knowledge. There is heightened
awareness of AI in recent years which has brought with it fear. This fear is
compounded by the inability to point to a trustworthy source of AI, however
even the term "trustworthy AI" itself is troublesome. Some consider trustworthy
AI to be that which complies with relevant laws, while others point to the
requirement to comply with ethics and standards (whether in addition to or in
isolation of the law). This immediately raises questions of whose ethics and
which standards should be applied and whether these are sufficient to produce
trustworthy AI in any event.

Artificial Intelligence (AI) is increasingly becoming a trusted advisor in
people's lives. A new concern arises if AI persuades people to break ethical
rules for profit. Employing a large-scale behavioural experiment (N = 1,572),
we test whether AI-generated advice can corrupt people. We further test whether
transparency about AI presence, a commonly proposed policy, mitigates potential
harm of AI-generated advice. Using the Natural Language Processing algorithm,
GPT-2, we generated honesty-promoting and dishonesty-promoting advice.
Participants read one type of advice before engaging in a task in which they
could lie for profit. Testing human behaviour in interaction with actual AI
outputs, we provide first behavioural insights into the role of AI as an
advisor. Results reveal that AI-generated advice corrupts people, even when
they know the source of the advice. In fact, AI's corrupting force is as strong
as humans'.

Recent years witness a trend of applying large-scale distributed deep
learning algorithms (HPC AI) in both business and scientific computing areas,
whose goal is to speed up the training time to achieve a state-of-the-art
quality. The HPC AI benchmarks accelerate the process. Unfortunately,
benchmarking HPC AI systems at scale raises serious challenges. This paper
presents a representative, repeatable and simple HPC AI benchmarking
methodology. Among the seventeen AI workloads of AIBench Training -- by far the
most comprehensive AI Training benchmarks suite -- we choose two representative
and repeatable AI workloads. The selected HPC AI benchmarks include both
business and scientific computing: Image Classification and Extreme Weather
Analytics. To rank HPC AI systems, we present a new metric named Valid FLOPS,
emphasizing both throughput performance and a target quality. The
specification, source code, datasets, and HPC AI500 ranking numbers are
publicly available from \url{https://www.benchcouncil.org/HPCAI500/}.

Participants in recent discussions of AI-related issues ranging from
intelligence explosion to technological unemployment have made diverse claims
about the nature, pace, and drivers of progress in AI. However, these theories
are rarely specified in enough detail to enable systematic evaluation of their
assumptions or to extrapolate progress quantitatively, as is often done with
some success in other technological domains. After reviewing relevant
literatures and justifying the need for more rigorous modeling of AI progress,
this paper contributes to that research program by suggesting ways to account
for the relationship between hardware speed increases and algorithmic
improvements in AI, the role of human inputs in enabling AI capabilities, and
the relationships between different sub-fields of AI. It then outlines ways of
tailoring AI progress models to generate insights on the specific issue of
technological unemployment, and outlines future directions for research on AI
progress.

AI systems are being deployed to support human decision making in high-stakes
domains. In many cases, the human and AI form a team, in which the human makes
decisions after reviewing the AI's inferences. A successful partnership
requires that the human develops insights into the performance of the AI
system, including its failures. We study the influence of updates to an AI
system in this setting. While updates can increase the AI's predictive
performance, they may also lead to changes that are at odds with the user's
prior experiences and confidence in the AI's inferences, hurting therefore the
overall team performance. We introduce the notion of the compatibility of an AI
update with prior user experience and present methods for studying the role of
compatibility in human-AI teams. Empirical results on three high-stakes domains
show that current machine learning algorithms do not produce compatible
updates. We propose a re-training objective to improve the compatibility of an
update by penalizing new errors. The objective offers full leverage of the
performance/compatibility tradeoff, enabling more compatible yet accurate
updates.

Envisioning a new imaginative idea together is a popular human need.
Imagining together as a team can often lead to breakthrough ideas, but the
collaboration effort can also be challenging, especially when the team members
are separated by time and space. What if there is a AI that can assist the team
to collaboratively envision new ideas?. Is it possible to develop a working
model of such an AI? This paper aims to design such an intelligence. This paper
proposes a approach to design a creative and collaborative intelligence by
employing a form of distributed machine learning approach called Federated
Learning along with fusion on Generative Adversarial Networks, GAN. This
collaborative creative AI presents a new paradigm in AI, one that lets a team
of two or more to come together to imagine and envision ideas that synergies
well with interests of all members of the team. In short, this paper explores
the design of a novel type of AI paradigm, called Federated AI Imagination, one
that lets geographically distributed teams to collaboratively imagine.

From its inception, AI has had a rather ambivalent relationship to
humans---swinging between their augmentation and replacement. Now, as AI
technologies enter our everyday lives at an ever increasing pace, there is a
greater need for AI systems to work synergistically with humans. To do this
effectively, AI systems must pay more attention to aspects of intelligence that
helped humans work with each other---including social intelligence. I will
discuss the research challenges in designing such human-aware AI systems,
including modeling the mental states of humans in the loop, recognizing their
desires and intentions, providing proactive support, exhibiting explicable
behavior, giving cogent explanations on demand, and engendering trust. I will
survey the progress made so far on these challenges, and highlight some
promising directions. I will also touch on the additional ethical quandaries
that such systems pose. I will end by arguing that the quest for human-aware AI
systems broadens the scope of AI enterprise, necessitates and facilitates true
inter-disciplinary collaborations, and can go a long way towards increasing
public acceptance of AI technologies.

To facilitate the widespread acceptance of AI systems guiding decision-making
in real-world applications, it is key that solutions comprise trustworthy,
integrated human-AI systems. Not only in safety-critical applications such as
autonomous driving or medicine, but also in dynamic open world systems in
industry and government it is crucial for predictive models to be
uncertainty-aware and yield trustworthy predictions. Another key requirement
for deployment of AI at enterprise scale is to realize the importance of
integrating human-centered design into AI systems such that humans are able to
use systems effectively, understand results and output, and explain findings to
oversight committees.
  While the focus of this symposium was on AI systems to improve data quality
and technical robustness and safety, we welcomed submissions from broadly
defined areas also discussing approaches addressing requirements such as
explainable models, human trust and ethical aspects of AI.

There appears to be a common agreement that ethical concerns are of high
importance when it comes to systems equipped with some sort of Artificial
Intelligence (AI). Demands for ethical AI are declared from all directions. As
a response, in recent years, public bodies, governments, and universities have
rushed in to provide a set of principles to be considered when AI based systems
are designed and used. We have learned, however, that high-level principles do
not turn easily into actionable advice for practitioners. Hence, also companies
are publishing their own ethical guidelines to guide their AI development. This
paper argues that AI software is still software and needs to be approached from
the software development perspective. The software engineering paradigm has
introduced maturity model thinking, which provides a roadmap for companies to
improve their performance from the selected viewpoints known as the key
capabilities. We want to voice out a call for action for the development of a
maturity model for AI software. We wish to discuss whether the focus should be
on AI ethics or, more broadly, the quality of an AI system, called a maturity
model for the development of AI systems.

The development of artificial intelligence (AI) has made various industries
eager to explore the benefits of AI. There is an increasing amount of research
surrounding AI, most of which is centred on the development of new AI
algorithms and techniques. However, the advent of AI is bringing an increasing
set of practical problems related to AI model lifecycle management that need to
be investigated. We address this gap by conducting a systematic mapping study
on the lifecycle of AI model. Through quantitative research, we provide an
overview of the field, identify research opportunities, and provide suggestions
for future research. Our study yields 405 publications published from 2005 to
2020, mapped in 5 different main research topics, and 31 sub-topics. We observe
that only a minority of publications focus on data management and model
production problems, and that more studies should address the AI lifecycle from
a holistic perspective.

This paper proposes a comprehensive analysis of existing concepts coming from
different disciplines tackling the notion of intelligence, namely psychology
and engineering, and from disciplines aiming to regulate AI innovations, namely
AI ethics and law. The aim is to identify shared notions or discrepancies to
consider for qualifying AI systems. Relevant concepts are integrated into a
matrix intended to help defining more precisely when and how computing tools
(programs or devices) may be qualified as AI while highlighting critical
features to serve a specific technical, ethical and legal assessment of
challenges in AI development. Some adaptations of existing notions of AI
characteristics are proposed. The matrix is a risk-based conceptual model
designed to allow an empirical, flexible and scalable qualification of AI
technologies in the perspective of benefit-risk assessment practices,
technological monitoring and regulatory compliance: it offers a structured
reflection tool for stakeholders in AI development that are engaged in
responsible research and innovation.Pre-print version (achieved on May 2020)

With the global roll-out of the fifth generation (5G) networks, it is
necessary to look beyond 5G and envision the 6G networks. The 6G networks are
expected to have space-air-ground integrated networks, advanced network
virtualization, and ubiquitous intelligence. This article presents an
artificial intelligence (AI)-native network slicing architecture for 6G
networks to enable the synergy of AI and network slicing, thereby facilitating
intelligent network management and supporting emerging AI services. AI-based
solutions are first discussed across network slicing lifecycle to intelligently
manage network slices, i.e., AI for slicing. Then, network slicing solutions
are studied to support emerging AI services by constructing AI instances and
performing efficient resource management, i.e., slicing for AI. Finally, a case
study is presented, followed by a discussion of open research issues that are
essential for AI-native network slicing in 6G networks.

Limited expert time is a key bottleneck in medical imaging. Due to advances
in image classification, AI can now serve as decision-support for medical
experts, with the potential for great gains in radiologist productivity and, by
extension, public health. However, these gains are contingent on building and
maintaining experts' trust in the AI agents. Explainable AI may build such
trust by helping medical experts to understand the AI decision processes behind
diagnostic judgements. Here we introduce and evaluate explanations based on
Bayesian Teaching, a formal account of explanation rooted in the cognitive
science of human learning. We find that medical experts exposed to explanations
generated by Bayesian Teaching successfully predict the AI's diagnostic
decisions and are more likely to certify the AI for cases when the AI is
correct than when it is wrong, indicating appropriate trust. These results show
that Explainable AI can be used to support human-AI collaboration in medical
imaging.

Recently, the use of sound measures and metrics in Artificial Intelligence
has become the subject of interest of academia, government, and industry.
Efforts towards measuring different phenomena have gained traction in the AI
community, as illustrated by the publication of several influential field
reports and policy documents. These metrics are designed to help decision
takers to inform themselves about the fast-moving and impacting influences of
key advances in Artificial Intelligence in general and Machine Learning in
particular. In this paper we propose to use such newfound capabilities of AI
technologies to augment our AI measuring capabilities. We do so by training a
model to classify publications related to ethical issues and concerns. In our
methodology we use an expert, manually curated dataset as the training set and
then evaluate a large set of research papers. Finally, we highlight the
implications of AI metrics, in particular their contribution towards developing
trustful and fair AI-based tools and technologies. Keywords: AI Ethics; AI
Fairness; AI Measurement. Ethics in Computer Science.

Recent AI governance research has focused heavily on the analysis of strategy
papers and ethics guidelines for AI published by national governments and
international bodies. Meanwhile, subnational institutions have also published
documents on Artificial Intelligence, yet these have been largely absent from
policy analyses. This is surprising because AI is connected to many policy
areas, such as economic or research policy, where the competences are already
distributed between the national and subnational level. To better understand
the current dynamics of AI governance, it is essential to consider the context
of policy making beyond the federal government. Although AI may be considered a
new policy field, it is created, contested and ultimately shaped within
existing political structures and dynamics. We therefore argue that more
attention should be dedicated to subnational efforts to shape AI and present
initial findings from our case study of Germany. Analyzing AI as a policy field
on different levels of government will contribute to a better understanding of
the developments and implementations of AI strategies in different national
contexts.

Advances in machine learning (ML) technologies have greatly improved
Artificial Intelligence (AI) systems. As a result, AI systems have become
ubiquitous, with their application prevalent in virtually all sectors. However,
AI systems have prompted ethical concerns, especially as their usage crosses
boundaries in sensitive areas such as healthcare, transportation, and security.
As a result, users are calling for better AI governance practices in ethical AI
systems. Therefore, AI development methods are encouraged to foster these
practices. This research analyzes the ECCOLA method for developing ethical and
trustworthy AI systems to determine if it enables AI governance in development
processes through ethical practices. The results demonstrate that while ECCOLA
fully facilitates AI governance in corporate governance practices in all its
processes, some of its practices do not fully foster data governance and
information governance practices. This indicates that the method can be further
improved.

Artificial Intelligence (AI) has significant potential for product design: AI
can check technical and non-technical constraints on products, it can support a
quick design of new product variants and new AI methods may also support
creativity. But currently product design and AI are separate communities
fostering different terms and theories. This makes a mapping of AI approaches
to product design needs difficult and prevents new solutions. As a solution,
this paper first clarifies important terms and concepts for the
interdisciplinary domain of AI methods in product design. A key contribution of
this paper is a new classification of design problems using the four
characteristics decomposability, inter-dependencies, innovation and creativity.
Definitions of these concepts are given where they are lacking. Early mappings
of these concepts to AI solutions are sketched and verified using design
examples. The importance of creativity in product design and a corresponding
gap in AI is pointed out for future research.

Massive efforts are made to reduce biases in both data and algorithms in
order to render AI applications fair. These efforts are propelled by various
high-profile cases where biased algorithmic decision-making caused harm to
women, people of color, minorities, etc. However, the AI fairness field still
succumbs to a blind spot, namely its insensitivity to discrimination against
animals. This paper is the first to describe the 'speciesist bias' and
investigate it in several different AI systems. Speciesist biases are learned
and solidified by AI applications when they are trained on datasets in which
speciesist patterns prevail. These patterns can be found in image recognition
systems, large language models, and recommender systems. Therefore, AI
technologies currently play a significant role in perpetuating and normalizing
violence against animals. This can only be changed when AI fairness frameworks
widen their scope and include mitigation measures for speciesist biases. This
paper addresses the AI community in this regard and stresses the influence AI
systems can have on either increasing or reducing the violence that is
inflicted on animals, and especially on farmed animals.

Over the last years, the rising capabilities of artificial intelligence (AI)
have improved human decision-making in many application areas. Teaming between
AI and humans may even lead to complementary team performance (CTP), i.e., a
level of performance beyond the ones that can be reached by AI or humans
individually. Many researchers have proposed using explainable AI (XAI) to
enable humans to rely on AI advice appropriately and thereby reach CTP.
However, CTP is rarely demonstrated in previous work as often the focus is on
the design of explainability, while a fundamental prerequisite -- the presence
of complementarity potential between humans and AI -- is often neglected.
Therefore, we focus on the existence of this potential for effective human-AI
decision-making. Specifically, we identify information asymmetry as an
essential source of complementarity potential, as in many real-world
situations, humans have access to different contextual information. By
conducting an online experiment, we demonstrate that humans can use such
contextual information to adjust the AI's decision, finally resulting in CTP.

With the growing need to regulate AI systems across a wide variety of
application domains, a new set of occupations has emerged in the industry. The
so-called responsible AI practitioners or AI ethicists are generally tasked
with interpreting and operationalizing best practices for ethical and safe
design of AI systems. Due to the nascent nature of these roles, however, it is
unclear to future employers and aspiring AI ethicists what specific function
these roles serve and what skills are necessary to serve the functions. Without
clarity on these, we cannot train future AI ethicists with meaningful learning
objectives.
  In this work, we examine what responsible AI practitioners do in the industry
and what skills they employ on the job. We propose an ontology of existing
roles alongside skills and competencies that serve each role. We created this
ontology by examining the job postings for such roles over a two-year period
(2020-2022) and conducting expert interviews with fourteen individuals who
currently hold such a role in the industry. Our ontology contributes to
business leaders looking to build responsible AI teams and provides educators
with a set of competencies that an AI ethics curriculum can prioritize.

The tremendous achievements of Artificial Intelligence (AI) in computer
vision, natural language processing, games and robotics, has extended the reach
of the AI hype to other fields: in telecommunication networks, the long term
vision is to let AI fully manage, and autonomously drive, all aspects of
network operation. In this industry vision paper, we discuss challenges and
opportunities of Autonomous Driving Network (ADN) driven by AI technologies. To
understand how AI can be successfully landed in current and future networks, we
start by outlining challenges that are specific to the networking domain,
putting them in perspective with advances that AI has achieved in other fields.
We then present a system view, clarifying how AI can be fitted in the network
architecture. We finally discuss current achievements as well as future
promises of AI in networks, mentioning a roadmap to avoid bumps in the road
that leads to true large-scale deployment of AI technologies in networks.

Since the first AI-HRI held at the 2014 AAAI Fall Symposium Series, a lot of
the presented research and discussions have emphasized how artificial
intelligence (AI) developments can benefit human-robot interaction (HRI). This
portrays HRI as an application, a source of domain-specific problems to solve,
to the AI community. Likewise, this portrays AI as a tool, a source of
solutions available for relevant problems, to the HRI community. However,
members of the AI-HRI research community will point out that the relationship
has a deeper synergy than matchmaking problems and solutions -- there are
insights from each field that impact how the other one thinks about the world
and performs scientific research. There is no greater opportunity for sharing
perspectives at the moment than human-aware AI, which studies how to account
for the fact that people are more than a source of data or part of an
algorithm. We will explore how AI-HRI can change the way researchers think
about human-aware AI, from observation through validation, to make even the
algorithmic design process human-aware.

There is a bidirectional relationship between culture and AI; AI models are
increasingly used to analyse culture, thereby shaping our understanding of
culture. On the other hand, the models are trained on collections of cultural
artifacts thereby implicitly, and not always correctly, encoding expressions of
culture. This creates a tension that both limits the use of AI for analysing
culture and leads to problems in AI with respect to cultural complex issues
such as bias.
  One approach to overcome this tension is to more extensively take into
account the intricacies and complexities of culture. We structure our
discussion using four concepts that guide humanistic inquiry into culture:
subjectivity, scalability, contextuality, and temporality. We focus on these
concepts because they have not yet been sufficiently represented in AI
research. We believe that possible implementations of these aspects into AI
research leads to AI that better captures the complexities of culture. In what
follows, we briefly describe these four concepts and their absence in AI
research. For each concept, we define possible research challenges.

As the real-world impact of Artificial Intelligence (AI) systems has been
steadily growing, so too have these systems come under increasing scrutiny. In
response, the study of AI fairness has rapidly developed into a rich field of
research with links to computer science, social science, law, and philosophy.
Many technical solutions for measuring and achieving AI fairness have been
proposed, yet their approach has been criticized in recent years for being
misleading, unrealistic and harmful.
  In our paper, we survey these criticisms of AI fairness and identify key
limitations that are inherent to the prototypical paradigm of AI fairness. By
carefully outlining the extent to which technical solutions can realistically
help in achieving AI fairness, we aim to provide the background necessary to
form a nuanced opinion on developments in fair AI. This delineation also
provides research opportunities for non-AI solutions peripheral to AI systems
in supporting fair decision processes.

The recent release of ChatGPT has gained huge attention and discussion
worldwide, with responsible AI being a key topic of discussion. How can we
ensure that AI systems, including ChatGPT, are developed and adopted in a
responsible way? To tackle the responsible AI challenges, various ethical
principles have been released by governments, organisations, and companies.
However, those principles are very abstract and not practical enough. Further,
significant efforts have been put on algorithm-level solutions that only
address a narrow set of principles, such as fairness and privacy. To fill the
gap, we adopt a pattern-oriented responsible AI engineering approach and build
a Responsible AI Pattern Catalogue to operationalise responsible AI from a
system perspective. In this article, we first summarise the major challenges in
operationalising responsible AI at scale and introduce how we use the
Responsible AI Pattern Catalogue to address those challenges. We then examine
the risks at each stage of the chatbot development process and recommend
pattern-driven mitigations to evaluate the the usefulness of the Responsible AI
Pattern Catalogue in a real-world setting.

Recent advances in large language models (LLMs) have led to the development
of powerful AI chatbots capable of engaging in natural and human-like
conversations. However, these chatbots can be potentially harmful, exhibiting
manipulative, gaslighting, and narcissistic behaviors. We define Healthy AI to
be safe, trustworthy and ethical. To create healthy AI systems, we present the
SafeguardGPT framework that uses psychotherapy to correct for these harmful
behaviors in AI chatbots. The framework involves four types of AI agents: a
Chatbot, a "User," a "Therapist," and a "Critic." We demonstrate the
effectiveness of SafeguardGPT through a working example of simulating a social
conversation. Our results show that the framework can improve the quality of
conversations between AI chatbots and humans. Although there are still several
challenges and directions to be addressed in the future, SafeguardGPT provides
a promising approach to improving the alignment between AI chatbots and human
values. By incorporating psychotherapy and reinforcement learning techniques,
the framework enables AI chatbots to learn and adapt to human preferences and
values in a safe and ethical way, contributing to the development of a more
human-centric and responsible AI.

With the increased adoption of artificial intelligence (AI) in industry and
society, effective human-AI interaction systems are becoming increasingly
important. A central challenge in the interaction of humans with AI is the
estimation of difficulty for human and AI agents for single task
instances.These estimations are crucial to evaluate each agent's capabilities
and, thus, required to facilitate effective collaboration. So far, research in
the field of human-AI interaction estimates the perceived difficulty of humans
and AI independently from each other. However, the effective interaction of
human and AI agents depends on metrics that accurately reflect each agent's
perceived difficulty in achieving valuable outcomes. Research to date has not
yet adequately examined the differences in the perceived difficulty of humans
and AI. Thus, this work reviews recent research on the perceived difficulty in
human-AI interaction and contributing factors to consistently compare each
agent's perceived difficulty, e.g., creating the same prerequisites.
Furthermore, we present an experimental design to thoroughly examine the
perceived difficulty of both agents and contribute to a better understanding of
the design of such systems.

This article connects the concepts and phenomena of Design AI, AI in creative
industries and AIs capacity for creativity. It links Design AI to UX design and
UX designer discourse. Its vagueness and the prominence of UX designers as
speakers and writers in the spectacle of cultural AI discourse. The article
then, draws comparisons between the Theatre of the Absurd and the UX designer
performances of design AI. It additionally sheds light on ToA and the human
condition in terms of existentialism, present within the practice of engaging
in design that intends to link human experience to technological system logic.
This is a theoretical article that utilises examples from UX events published
on Youtube, as well as UX designer blogs, in order to illustrate the mechanics
of the ToA present within contemporary AI and UX designer discourse.

Artificial Intelligence (AI) is a double-edged sword: on one hand, AI
promises to provide great advances that could benefit humanity, but on the
other hand, AI poses substantial (even existential) risks. With advancements
happening daily, many people are increasingly worried about AI's impact on
their lives. To ensure AI progresses beneficially, some researchers have
proposed "wellbeing" as a key objective to govern AI. This article addresses
key challenges in designing AI for wellbeing. We group these challenges into
issues of modeling wellbeing in context, assessing wellbeing in context,
designing interventions to improve wellbeing, and maintaining AI alignment with
wellbeing over time. The identification of these challenges provides a scope
for efforts to help ensure that AI developments are aligned with human
wellbeing.

The rapid advancement of Artificial Intelligence (AI), represented by
ChatGPT, has raised concerns about responsible AI development and utilization.
Existing frameworks lack a comprehensive synthesis of AI risk assessment
questions. To address this, we introduce QB4AIRA, a novel question bank
developed by refining questions from five globally recognized AI risk
frameworks, categorized according to Australia's AI ethics principles. QB4AIRA
comprises 293 prioritized questions covering a wide range of AI risk areas,
facilitating effective risk assessment. It serves as a valuable resource for
stakeholders in assessing and managing AI risks, while paving the way for new
risk frameworks and guidelines. By promoting responsible AI practices, QB4AIRA
contributes to responsible AI deployment, mitigating potential risks and harms.

Generative AI tools introduce new and accessible forms of media creation for
youth. They also raise ethical concerns about the generation of fake media,
data protection, privacy and ownership of AI-generated art. Since generative AI
is already being used in products used by youth, it is critical that they
understand how these tools work and how they can be used or misused. In this
work, we facilitated students' generative AI learning through expression of
their imagined future identities. We designed a learning workshop - Dreaming
with AI - where students learned about the inner workings of generative AI
tools, used text-to-image generation algorithms to create their imaged future
dreams, reflected on the potential benefits and harms of generative AI tools
and voiced their opinions about policies for the use of these tools in
classrooms. In this paper, we present the learning activities and experiences
of 34 high school students who engaged in our workshops. Students reached
creative learning objectives by using prompt engineering to create their future
dreams, gained technical knowledge by learning the abilities, limitations,
text-visual mappings and applications of generative AI, and identified most
potential societal benefits and harms of generative AI.

With the emergence of deep learning techniques, smartphone apps are now
embedded on-device AI features for enabling advanced tasks like speech
translation, to attract users and increase market competitiveness. A good
interaction design is important to make an AI feature usable and
understandable. However, AI features have their unique challenges like
sensitiveness to the input, dynamic behaviours and output uncertainty. Existing
guidelines and tools either do not cover AI features or consider mobile apps
which are confirmed by our informal interview with professional designers. To
address these issues, we conducted the first empirical study to explore
user-AI-interaction in mobile apps. We aim to understand the status of
on-device AI usage by investigating 176 AI apps from 62,822 apps. We identified
255 AI features and summarised 759 implementations into three primary
interaction pattern types. We further implemented our findings into a
multi-faceted search-enabled gallery. The results of the user study demonstrate
the usefulness of our findings.

Precision medicine, tailored to individual patients based on their genetics,
environment, and lifestyle, shows promise in managing complex diseases like
infections. Integrating artificial intelligence (AI) into precision medicine
can revolutionize disease management. This paper introduces a novel approach
using AI to advance precision medicine in infectious diseases and beyond. It
integrates diverse fields, analyzing patients' profiles using genomics,
proteomics, microbiomics, and clinical data. AI algorithms process vast data,
providing insights for precise diagnosis, treatment, and prognosis. AI-driven
predictive modeling empowers healthcare providers to make personalized and
effective interventions. Collaboration among experts from different domains
refines AI models and ensures ethical and robust applications. Beyond
infections, this AI-driven approach can benefit other complex diseases.
Precision medicine powered by AI has the potential to transform healthcare into
a proactive, patient-centric model. Research is needed to address privacy,
regulations, and AI integration into clinical workflows. Collaboration among
researchers, healthcare institutions, and policymakers is crucial in harnessing
AI-driven strategies for advancing precision medicine and improving patient
outcomes.

Researchers, practitioners, and policymakers with an interest in AI ethics
need more integrative approaches for studying and intervening in AI systems
across many contexts and scales of activity. This paper presents AI value
chains as an integrative concept that satisfies that need. To more clearly
theorize AI value chains and conceptually distinguish them from supply chains,
we review theories of value chains and AI value chains from the strategic
management, service science, economic geography, industry, government, and
applied research literature. We then conduct an integrative review of a sample
of 67 sources that cover the ethical concerns implicated in AI value chains.
Building upon the findings of our integrative review, we recommend four future
directions that researchers, practitioners, and policymakers can take to
advance more ethical practices of AI development and use across AI value
chains. Our review and recommendations contribute to the advancement of
research agendas, industrial agendas, and policy agendas that seek to study and
intervene in the ethics of AI value chains.

As AI technology advances rapidly, concerns over the risks of bigness in
digital markets are also growing. The EU's Digital Markets Act (DMA) aims to
address these risks. Still, the current framework may not adequately cover
generative AI systems that could become gateways for AI-based services. This
paper argues for integrating certain AI software as core platform services and
classifying certain developers as gatekeepers under the DMA. We also propose an
assessment of gatekeeper obligations to ensure they cover generative AI
services. As the EU considers generative AI-specific rules and possible DMA
amendments, this paper provides insights towards diversity and openness in
generative AI services.

We present an overview of the literature on trust in AI and AI
trustworthiness and argue for the need to distinguish these concepts more
clearly and to gather more empirically evidence on what contributes to people s
trusting behaviours. We discuss that trust in AI involves not only reliance on
the system itself, but also trust in the developers of the AI system. AI ethics
principles such as explainability and transparency are often assumed to promote
user trust, but empirical evidence of how such features actually affect how
users perceive the system s trustworthiness is not as abundance or not that
clear. AI systems should be recognised as socio-technical systems, where the
people involved in designing, developing, deploying, and using the system are
as important as the system for determining whether it is trustworthy. Without
recognising these nuances, trust in AI and trustworthy AI risk becoming
nebulous terms for any desirable feature for AI systems.

In beamformed wireless cellular systems such as 5G New Radio (NR) networks,
beam management (BM) is a crucial operation. In the second phase of 5G NR
standardization, known as 5G-Advanced, which is being vigorously promoted, the
key component is the use of artificial intelligence (AI) based on machine
learning (ML) techniques. AI/ML for BM is selected as a representative use
case. This article provides an overview of the AI/ML for BM in 5G-Advanced. The
legacy non-AI and prime AI-enabled BM frameworks are first introduced and
compared. Then, the main scope of AI/ML for BM is presented, including
improving accuracy, reducing overhead and latency. Finally, the key challenges
and open issues in the standardization of AI/ML for BM are discussed,
especially the design of new protocols for AI-enabled BM. This article provides
a guideline for the study of AI/ML-based BM standardization.

The true potential of human-AI collaboration lies in exploiting the
complementary capabilities of humans and AI to achieve a joint performance
superior to that of the individual AI or human, i.e., to achieve complementary
team performance (CTP). To realize this complementarity potential, humans need
to exercise discretion in following AI 's advice, i.e., appropriately relying
on the AI's advice. While previous work has focused on building a mental model
of the AI to assess AI recommendations, recent research has shown that the
mental model alone cannot explain appropriate reliance. We hypothesize that, in
addition to the mental model, human learning is a key mediator of appropriate
reliance and, thus, CTP. In this study, we demonstrate the relationship between
learning and appropriate reliance in an experiment with 100 participants. This
work provides fundamental concepts for analyzing reliance and derives
implications for the effective design of human-AI decision-making.

Privacy is a key principle for developing ethical AI technologies, but how
does including AI technologies in products and services change privacy risks?
We constructed a taxonomy of AI privacy risks by analyzing 321 documented AI
privacy incidents. We codified how the unique capabilities and requirements of
AI technologies described in those incidents generated new privacy risks,
exacerbated known ones, or otherwise did not meaningfully alter the risk. We
present 12 high-level privacy risks that AI technologies either newly created
(e.g., exposure risks from deepfake pornography) or exacerbated (e.g.,
surveillance risks from collecting training data). One upshot of our work is
that incorporating AI technologies into a product can alter the privacy risks
it entails. Yet, current approaches to privacy-preserving AI/ML (e.g.,
federated learning, differential privacy, checklists) only address a subset of
the privacy risks arising from the capabilities and data requirements of AI.

Academic writing is an indispensable yet laborious part of the research
enterprise. This Perspective maps out principles and methods for using
generative artificial intelligence (AI), specifically large language models
(LLMs), to elevate the quality and efficiency of academic writing. We introduce
a human-AI collaborative framework that delineates the rationale (why), process
(how), and nature (what) of AI engagement in writing. The framework pinpoints
both short-term and long-term reasons for engagement and their underlying
mechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals
the role of AI throughout the writing process, conceptualized through a
two-stage model for human-AI collaborative writing, and the nature of AI
assistance in writing, represented through a model of writing-assistance types
and levels. Building on this framework, we describe effective prompting
techniques for incorporating AI into the writing routine (outlining, drafting,
and editing) as well as strategies for maintaining rigorous scholarship,
adhering to varied journal policies, and avoiding overreliance on AI.
Ultimately, the prudent integration of AI into academic writing can ease the
communication burden, empower authors, accelerate discovery, and promote
diversity in science.

This paper explores the dynamic landscape of Artificial Intelligence (AI)
adoption in Africa, analysing its varied applications in addressing
socio-economic challenges and fostering development. Examining the African AI
ecosystem, the study considers regional nuances, cultural factors, and
infrastructural constraints shaping the deployment of AI solutions. Case
studies in healthcare, agriculture, finance, and education highlight AI's
transformative potential for efficiency, accessibility, and inclusivity. The
paper emphasizes indigenous AI innovations and international collaborations
contributing to a distinct African AI ecosystem. Ethical considerations,
including data privacy and algorithmic bias, are addressed alongside policy
frameworks supporting responsible AI implementation. The role of governmental
bodies, regulations, and private sector partnerships is explored in creating a
conducive AI development environment. Challenges such as digital literacy gaps
and job displacement are discussed, with proposed strategies for mitigation. In
conclusion, the paper provides a nuanced understanding of AI in Africa,
contributing to sustainable development discussions and advocating for an
inclusive and ethical AI ecosystem on the continent.

Emerging Distributed AI systems are revolutionizing big data computing and
data processing capabilities with growing economic and societal impact.
However, recent studies have identified new attack surfaces and risks caused by
security, privacy, and fairness issues in AI systems. In this paper, we review
representative techniques, algorithms, and theoretical foundations for
trustworthy distributed AI through robustness guarantee, privacy protection,
and fairness awareness in distributed learning. We first provide a brief
overview of alternative architectures for distributed learning, discuss
inherent vulnerabilities for security, privacy, and fairness of AI algorithms
in distributed learning, and analyze why these problems are present in
distributed learning regardless of specific architectures. Then we provide a
unique taxonomy of countermeasures for trustworthy distributed AI, covering (1)
robustness to evasion attacks and irregular queries at inference, and
robustness to poisoning attacks, Byzantine attacks, and irregular data
distribution during training; (2) privacy protection during distributed
learning and model inference at deployment; and (3) AI fairness and governance
with respect to both data and models. We conclude with a discussion on open
challenges and future research directions toward trustworthy distributed AI,
such as the need for trustworthy AI policy guidelines, the AI
responsibility-utility co-design, and incentives and compliance.

AI systems cannot exist without data. Now that AI models (data science and
AI) have matured and are readily available to apply in practice, most
organizations struggle with the data infrastructure to do so. There is a
growing need for data engineers that know how to prepare data for AI systems or
that can setup enterprise-wide data architectures for analytical projects. But
until now, the data engineering part of AI engineering has not been getting
much attention, in favor of discussing the modeling part. In this paper we aim
to change this by perform a mapping study on data engineering for AI systems,
i.e., AI data engineering. We found 25 relevant papers between January 2019 and
June 2023, explaining AI data engineering activities. We identify which life
cycle phases are covered, which technical solutions or architectures are
proposed and which lessons learned are presented. We end by an overall
discussion of the papers with implications for practitioners and researchers.
This paper creates an overview of the body of knowledge on data engineering for
AI. This overview is useful for practitioners to identify solutions and best
practices as well as for researchers to identify gaps.

The vast majority of discourse around AI development assumes that
subservient, "moral" models aligned with "human values" are universally
beneficial -- in short, that good AI is sycophantic AI. We explore the shadow
of the sycophantic paradigm, a design space we term antagonistic AI: AI systems
that are disagreeable, rude, interrupting, confrontational, challenging, etc.
-- embedding opposite behaviors or values. Far from being "bad" or "immoral,"
we consider whether antagonistic AI systems may sometimes have benefits to
users, such as forcing users to confront their assumptions, build resilience,
or develop healthier relational boundaries. Drawing from formative explorations
and a speculative design workshop where participants designed fictional AI
technologies that employ antagonism, we lay out a design space for antagonistic
AI, articulating potential benefits, design techniques, and methods of
embedding antagonistic elements into user experience. Finally, we discuss the
many ethical challenges of this space and identify three dimensions for the
responsible design of antagonistic AI -- consent, context, and framing.

To effectively navigate the AI revolution, AI literacy is crucial. However,
content predominantly exists in dominant languages, creating a gap for
low-resource languages like Yoruba (41 million native speakers). This case
study explores bridging this gap by creating and distributing AI videos in
Yoruba.The project developed 26 videos covering foundational, intermediate, and
advanced AI concepts, leveraging storytelling and accessible explanations.
These videos were created using a cost-effective methodology and distributed
across YouTube, LinkedIn, and Twitter, reaching an estimated global audience of
22 countries. Analysis of YouTube reveals insights into viewing patterns, with
the 25-44 age group contributing the most views. Notably, over half of the
traffic originated from external sources, highlighting the potential of
cross-platform promotion.This study demonstrates the feasibility and impact of
creating AI literacy content in low-resource languages. It emphasizes that
accurate interpretation requires both technical expertise in AI and fluency in
the target language. This work contributes a replicable methodology, a 22-word
Yoruba AI vocabulary, and data-driven insights into audience demographics and
acquisition channel

The current societal challenges exceed the capacity of human individual or
collective effort alone. As AI evolves, its role within human collectives is
poised to vary from an assistive tool to a participatory member. Humans and AI
possess complementary capabilities that, when synergized, can achieve a level
of collective intelligence that surpasses the collective capabilities of either
humans or AI in isolation. However, the interactions in human-AI systems are
inherently complex, involving intricate processes and interdependencies. This
review incorporates perspectives from network science to conceptualize a
multilayer representation of human-AI collective intelligence, comprising a
cognition layer, a physical layer, and an information layer. Within this
multilayer network, humans and AI agents exhibit varying characteristics;
humans differ in diversity from surface-level to deep-level attributes, while
AI agents range in degrees of functionality and anthropomorphism. The interplay
among these agents shapes the overall structure and dynamics of the system. We
explore how agents' diversity and interactions influence the system's
collective intelligence. Furthermore, we present an analysis of real-world
instances of AI-enhanced collective intelligence. We conclude by addressing the
potential challenges in AI-enhanced collective intelligence and offer
perspectives on future developments in this field.

Recent advances in generative AI technologies like large language models
raise both excitement and concerns about the future of human-AI co-creation in
writing. To unpack people's attitude towards and experience with generative
AI-powered writing assistants, in this paper, we conduct an experiment to
understand whether and how much value people attach to AI assistance, and how
the incorporation of AI assistance in writing workflows changes people's
writing perceptions and performance. Our results suggest that people are
willing to forgo financial payments to receive writing assistance from AI,
especially if AI can provide direct content generation assistance and the
writing task is highly creative. Generative AI-powered assistance is found to
offer benefits in increasing people's productivity and confidence in writing.
However, direct content generation assistance offered by AI also comes with
risks, including decreasing people's sense of accountability and diversity in
writing. We conclude by discussing the implications of our findings.

Artificial intelligence (AI) ethics has emerged as a burgeoning yet pivotal
area of scholarly research. This study conducts a comprehensive bibliometric
analysis of the AI ethics literature over the past two decades. The analysis
reveals a discernible tripartite progression, characterized by an incubation
phase, followed by a subsequent phase focused on imbuing AI with human-like
attributes, culminating in a third phase emphasizing the development of
human-centric AI systems. After that, they present seven key AI ethics issues,
encompassing the Collingridge dilemma, the AI status debate, challenges
associated with AI transparency and explainability, privacy protection
complications, considerations of justice and fairness, concerns about algocracy
and human enfeeblement, and the issue of superintelligence. Finally, they
identify two notable research gaps in AI ethics regarding the large ethics
model (LEM) and AI identification and extend an invitation for further
scholarly research.

In December 2023, the European Parliament provisionally agreed on the EU AI
Act. This unprecedented regulatory framework for AI systems lays out guidelines
to ensure the safety, legality, and trustworthiness of AI products. This paper
presents a methodology for interpreting the EU AI Act requirements for
high-risk AI systems by leveraging product quality models. We first propose an
extended product quality model for AI systems, incorporating attributes
relevant to the Act not covered by current quality models. We map the Act
requirements to relevant quality attributes with the goal of refining them into
measurable characteristics. We then propose a contract-based approach to derive
technical requirements at the stakeholder level. This facilitates the
development and assessment of AI systems that not only adhere to established
quality standards, but also comply with the regulatory requirements outlined in
the Act for high-risk (including safety-critical) AI systems. We demonstrate
the applicability of this methodology on an exemplary automotive supply chain
use case, where several stakeholders interact to achieve EU AI Act compliance.

AI is redefining how humans interact with technology, leading to a synergetic
collaboration between the two. Nevertheless, the effects of human cognition on
this collaboration remain unclear. This study investigates the implications of
two cognitive biases, anthropomorphism and framing effect, on human-AI
collaboration within a hiring setting. Subjects were asked to select job
candidates with the help of an AI-powered recommendation tool. The tool was
manipulated to have either human-like or robot-like characteristics and
presented its recommendations in either positive or negative frames. The
results revealed that the framing of AI's recommendations had no significant
influence on subjects' decisions. In contrast, anthropomorphism significantly
affected subjects' agreement with AI recommendations. Contrary to expectations,
subjects were less likely to agree with the AI if it had human-like
characteristics. These findings demonstrate that cognitive biases can impact
human-AI collaboration and highlight the need for tailored approaches to AI
product design, rather than a single, universal solution.

Recent advances in artificial intelligence (AI) and machine learning have
created a general perception that AI could be used to solve complex problems,
and in some situations over-hyped as a tool that can be so easily used.
Unfortunately, the barrier to realization of mass adoption of AI on various
business domains is too high because most domain experts have no background in
AI. Developing AI applications involves multiple phases, namely data
preparation, application modeling, and product deployment. The effort of AI
research has been spent mostly on new AI models (in the model training stage)
to improve the performance of benchmark tasks such as image recognition. Many
other factors such as usability, efficiency and security of AI have not been
well addressed, and therefore form a barrier to democratizing AI. Further, for
many real world applications such as healthcare and autonomous driving,
learning via huge amounts of possibility exploration is not feasible since
humans are involved. In many complex applications such as healthcare, subject
matter experts (e.g. Clinicians) are the ones who appreciate the importance of
features that affect health, and their knowledge together with existing
knowledge bases are critical to the end results. In this paper, we take a new
perspective on developing AI solutions, and present a solution for making AI
usable. We hope that this resolution will enable all subject matter experts
(eg. Clinicians) to exploit AI like data scientists.

AI researchers employ not only the scientific method, but also methodology
from mathematics and engineering. However, the use of the scientific method -
specifically hypothesis testing - in AI is typically conducted in service of
engineering objectives. Growing interest in topics such as fairness and
algorithmic bias show that engineering-focused questions only comprise a subset
of the important questions about AI systems. This results in the AI Knowledge
Gap: the number of unique AI systems grows faster than the number of studies
that characterize these systems' behavior. To close this gap, we argue that the
study of AI could benefit from the greater inclusion of researchers who are
well positioned to formulate and test hypotheses about the behavior of AI
systems. We examine the barriers preventing social and behavioral scientists
from conducting such studies. Our diagnosis suggests that accelerating the
scientific study of AI systems requires new incentives for academia and
industry, mediated by new tools and institutions. To address these needs, we
propose a two-sided marketplace called TuringBox. On one side, AI contributors
upload existing and novel algorithms to be studied scientifically by others. On
the other side, AI examiners develop and post machine intelligence tasks
designed to evaluate and characterize algorithmic behavior. We discuss this
market's potential to democratize the scientific study of AI behavior, and thus
narrow the AI Knowledge Gap.

Explainable AI provides insight into the "why" for model predictions,
offering potential for users to better understand and trust a model, and to
recognize and correct AI predictions that are incorrect. Prior research on
human and explainable AI interactions has focused on measures such as
interpretability, trust, and usability of the explanation. Whether explainable
AI can improve actual human decision-making and the ability to identify the
problems with the underlying model are open questions. Using real datasets, we
compare and evaluate objective human decision accuracy without AI (control),
with an AI prediction (no explanation), and AI prediction with explanation. We
find providing any kind of AI prediction tends to improve user decision
accuracy, but no conclusive evidence that explainable AI has a meaningful
impact. Moreover, we observed the strongest predictor for human decision
accuracy was AI accuracy and that users were somewhat able to detect when the
AI was correct versus incorrect, but this was not significantly affected by
including an explanation. Our results indicate that, at least in some
situations, the "why" information provided in explainable AI may not enhance
user decision-making, and further research may be needed to understand how to
integrate explainable AI into real systems.

People supported by AI-powered decision support tools frequently overrely on
the AI: they accept an AI's suggestion even when that suggestion is wrong.
Adding explanations to the AI decisions does not appear to reduce the
overreliance and some studies suggest that it might even increase it. Informed
by the dual-process theory of cognition, we posit that people rarely engage
analytically with each individual AI recommendation and explanation, and
instead develop general heuristics about whether and when to follow the AI
suggestions. Building on prior research on medical decision-making, we designed
three cognitive forcing interventions to compel people to engage more
thoughtfully with the AI-generated explanations. We conducted an experiment
(N=199), in which we compared our three cognitive forcing designs to two simple
explainable AI approaches and to a no-AI baseline. The results demonstrate that
cognitive forcing significantly reduced overreliance compared to the simple
explainable AI approaches. However, there was a trade-off: people assigned the
least favorable subjective ratings to the designs that reduced the overreliance
the most. To audit our work for intervention-generated inequalities, we
investigated whether our interventions benefited equally people with different
levels of Need for Cognition (i.e., motivation to engage in effortful mental
activities). Our results show that, on average, cognitive forcing interventions
benefited participants higher in Need for Cognition more. Our research suggests
that human cognitive motivation moderates the effectiveness of explainable AI
solutions.

With the extensive use of AI in various fields, the issue of AI security has
become more significant. The AI data poisoning attacks will be the most
threatening approach against AI security after the adversarial examples. As the
continuous updating of AI applications online, the data pollution models can be
uploaded by attackers to achieve a certain malicious purpose. Recently, the
research on AI data poisoning attacks is mostly out of practice and use
self-built experimental environments so that it cannot be as close to reality
as adversarial example attacks. This article's first contribution is to provide
a solution and a breakthrough for the aforementioned issue with research
limitations, to aim at data poisoning attacks that target real businesses, in
this case: data poisoning attacks on real Go AI. We install a Trojan virus into
the real Go AI that manipulates the AI's behavior. It is the first time that we
succeed in manipulating complicated AI and provide a reliable approach to the
AI data poisoning attack verification method. The method of building Trojan in
this article can be expanded to more practical algorithms for other fields such
as content recommendation, text translation, and intelligent dialogue.

In the current era, people and society have grown increasingly reliant on
artificial intelligence (AI) technologies. AI has the potential to drive us
towards a future in which all of humanity flourishes. It also comes with
substantial risks for oppression and calamity. Discussions about whether we
should (re)trust AI have repeatedly emerged in recent years and in many
quarters, including industry, academia, healthcare, services, and so on.
Technologists and AI researchers have a responsibility to develop trustworthy
AI systems. They have responded with great effort to design more responsible AI
algorithms. However, existing technical solutions are narrow in scope and have
been primarily directed towards algorithms for scoring or classification tasks,
with an emphasis on fairness and unwanted bias. To build long-lasting trust
between AI and human beings, we argue that the key is to think beyond
algorithmic fairness and connect major aspects of AI that potentially cause
AI's indifferent behavior. In this survey, we provide a systematic framework of
Socially Responsible AI Algorithms that aims to examine the subjects of AI
indifference and the need for socially responsible AI algorithms, define the
objectives, and introduce the means by which we may achieve these objectives.
We further discuss how to leverage this framework to improve societal
well-being through protection, information, and prevention/mitigation.

We survey SoTA open-domain conversational AI models with the purpose of
presenting the prevailing challenges that still exist to spur future research.
In addition, we provide statistics on the gender of conversational AI in order
to guide the ethics discussion surrounding the issue. Open-domain
conversational AI are known to have several challenges, including bland
responses and performance degradation when prompted with figurative language,
among others. First, we provide some background by discussing some topics of
interest in conversational AI. We then discuss the method applied to the two
investigations carried out that make up this study. The first investigation
involves a search for recent SoTA open-domain conversational AI models while
the second involves the search for 100 conversational AI to assess their
gender. Results of the survey show that progress has been made with recent SoTA
conversational AI, but there are still persistent challenges that need to be
solved, and the female gender is more common than the male for conversational
AI. One main take-away is that hybrid models of conversational AI offer more
advantages than any single architecture. The key contributions of this survey
are 1) the identification of prevailing challenges in SoTA open-domain
conversational AI, 2) the unusual discussion about open-domain conversational
AI for low-resource languages, and 3) the discussion about the ethics
surrounding the gender of conversational AI.

Advances in artificial intelligence (AI) are shaping modern life, from
transportation, health care, science, finance, to national defense. Forecasts
of AI development could help improve policy- and decision-making. We report the
results from a large survey of AI and machine learning (ML) researchers on
their beliefs about progress in AI. The survey, fielded in late 2019, elicited
forecasts for near-term AI development milestones and high- or human-level
machine intelligence, defined as when machines are able to accomplish every or
almost every task humans are able to do currently. As part of this study, we
re-contacted respondents from a highly-cited study by Grace et al. (2018), in
which AI/ML researchers gave forecasts about high-level machine intelligence
and near-term milestones in AI development. Results from our 2019 survey show
that, in aggregate, AI/ML researchers surveyed placed a 50% likelihood of
human-level machine intelligence being achieved by 2060. The results show
researchers newly contacted in 2019 expressed similar beliefs about the
progress of advanced AI as respondents in the Grace et al. (2018) survey. For
the recontacted participants from the Grace et al. (2018) study, the aggregate
forecast for a 50% likelihood of high-level machine intelligence shifted from
2062 to 2076, although this change is not statistically significant, likely due
to the small size of our panel sample. Forecasts of several near-term AI
milestones have reduced in time, suggesting more optimism about AI progress.
Finally, AI/ML researchers also exhibited significant optimism about how
human-level machine intelligence will impact society.

The term ethics is widely used, explored, and debated in the context of
developing Artificial Intelligence (AI) based software systems. In recent
years, numerous incidents have raised the profile of ethical issues in AI
development and led to public concerns about the proliferation of AI technology
in our everyday lives. But what do we know about the views and experiences of
those who develop these systems -- the AI practitioners? We conducted a
grounded theory literature review (GTLR) of 38 primary empirical studies that
included AI practitioners' views on ethics in AI and analysed them to derive
five categories: practitioner awareness, perception, need, challenge, and
approach. These are underpinned by multiple codes and concepts that we explain
with evidence from the included studies. We present a taxonomy of ethics in AI
from practitioners' viewpoints to assist AI practitioners in identifying and
understanding the different aspects of AI ethics. The taxonomy provides a
landscape view of the key aspects that concern AI practitioners when it comes
to ethics in AI. We also share an agenda for future research studies and
recommendations for practitioners, managers, and organisations to help in their
efforts to better consider and implement ethics in AI.

Vertical heterogenous networks (VHetNets) and artificial intelligence (AI)
play critical roles in 6G and beyond networks. This article presents an
AI-native VHetNets architecture to enable the synergy of VHetNets and AI,
thereby supporting varieties of AI services while facilitating automatic and
intelligent network management. Anomaly detection in Internet of Things (IoT)
is a major AI service required by many fields, including intrusion detection,
state monitoring, device-activity analysis, security supervision and so on.
Conventional anomaly detection technologies mainly consider the anomaly
detection as a standalone service that is independent of any other network
management functionalities, which cannot be used directly in ubiquitous IoT due
to the resource constrained end nodes and decentralized data distribution. In
this article, we develop an AI-native VHetNets-enabled framework to provide the
anomaly detection service for ubiquitous IoT, whose implementation is assisted
by intelligent network management functionalities. We first discuss the
possibilities of VHetNets used for distributed AI model training to provide
anomaly detection service for ubiquitous IoT, i.e., VHetNets for AI. After
that, we study the application of AI approaches in helping provide automatic
and intelligent network management functionalities for VHetNets, i.e., AI for
VHetNets, whose aim is to facilitate the efficient implementation of anomaly
detection service. Finally, a case study is presented to demonstrate the
efficiency and effectiveness of the proposed AI-native VHetNets-enabled anomaly
detection framework.

Mistakes in AI systems are inevitable, arising from both technical
limitations and sociotechnical gaps. While black-boxing AI systems can make the
user experience seamless, hiding the seams risks disempowering users to
mitigate fallouts from AI mistakes. Instead of hiding these AI imperfections,
can we leverage them to help the user? While Explainable AI (XAI) has
predominantly tackled algorithmic opaqueness, we propose that seamful design
can foster AI explainability by revealing and leveraging sociotechnical and
infrastructural mismatches. We introduce the concept of Seamful XAI by (1)
conceptually transferring "seams" to the AI context and (2) developing a design
process that helps stakeholders anticipate and design with seams. We explore
this process with 43 AI practitioners and real end-users, using a
scenario-based co-design activity informed by real-world use cases. We found
that the Seamful XAI design process helped users foresee AI harms, identify
underlying reasons (seams), locate them in the AI's lifecycle, learn how to
leverage seamful information to improve XAI and user agency. We share empirical
insights, implications, and reflections on how this process can help
practitioners anticipate and craft seams in AI, how seamfulness can improve
explainability, empower end-users, and facilitate Responsible AI.

Novel artificial intelligence (AI) technology has expedited various
scientific research, e.g., cosmology, physics and bioinformatics, inevitably
becoming a significant category of workload on high performance computing (HPC)
systems. Existing AI benchmarks tend to customize well-recognized AI
applications, so as to evaluate the AI performance of HPC systems under
predefined problem size, in terms of datasets and AI models. Due to lack of
scalability on the problem size, static AI benchmarks might be under competent
to help understand the performance trend of evolving AI applications on HPC
systems, in particular, the scientific AI applications on large-scale systems.
  In this paper, we propose a scalable evaluation methodology (SAIH) for
analyzing the AI performance trend of HPC systems with scaling the problem
sizes of customized AI applications. To enable scalability, SAIH builds a set
of novel mechanisms for augmenting problem sizes. As the data and model
constantly scale, we can investigate the trend and range of AI performance on
HPC systems, and further diagnose system bottlenecks. To verify our
methodology, we augment a cosmological AI application to evaluate a real HPC
system equipped with GPUs as a case study of SAIH.

This work addresses the problems of (a) designing utilization measurements of
trained artificial intelligence (AI) models and (b) explaining how training
data are encoded in AI models based on those measurements. The problems are
motivated by the lack of explainability of AI models in security and safety
critical applications, such as the use of AI models for classification of
traffic signs in self-driving cars. We approach the problems by introducing
theoretical underpinnings of AI model utilization measurement and understanding
patterns in utilization-based class encodings of traffic signs at the level of
computation graphs (AI models), subgraphs, and graph nodes. Conceptually,
utilization is defined at each graph node (computation unit) of an AI model
based on the number and distribution of unique outputs in the space of all
possible outputs (tensor-states). In this work, utilization measurements are
extracted from AI models, which include poisoned and clean AI models. In
contrast to clean AI models, the poisoned AI models were trained with traffic
sign images containing systematic, physically realizable, traffic sign
modifications (i.e., triggers) to change a correct class label to another label
in a presence of such a trigger. We analyze class encodings of such clean and
poisoned AI models, and conclude with implications for trojan injection and
detection.

Significant enthusiasm around AI uptake has been witnessed across societies
globally. The electoral process -- the time, place and manner of elections
within democratic nations -- has been among those very rare sectors in which AI
has not penetrated much. Electoral management bodies in many countries have
recently started exploring and deliberating over the use of AI in the electoral
process. In this paper, we consider five representative avenues within the core
electoral process which have potential for AI usage, and map the challenges
involved in using AI within them. These five avenues are: voter list
maintenance, determining polling booth locations, polling booth protection
processes, voter authentication and video monitoring of elections. Within each
of these avenues, we lay down the context, illustrate current or potential
usage of AI, and discuss extant or potential ramifications of AI usage, and
potential directions for mitigating risks while considering AI usage. We
believe that the scant current usage of AI within electoral processes provides
a very rare opportunity, that of being able to deliberate on the risks and
mitigation possibilities, prior to real and widespread AI deployment. This
paper is an attempt to map the horizons of risks and opportunities in using AI
within the electoral processes and to help shape the debate around the topic.

As Artificial Intelligence (AI) continues to advance rapidly, it becomes
increasingly important to consider AI's ethical and societal implications. In
this paper, we present a bottom-up mapping of the current state of research at
the intersection of Human-Centered AI, Ethical, and Responsible AI (HCER-AI) by
thematically reviewing and analyzing 164 research papers from leading
conferences in ethical, social, and human factors of AI: AIES, CHI, CSCW, and
FAccT. The ongoing research in HCER-AI places emphasis on governance, fairness,
and explainability. These conferences, however, concentrate on specific themes
rather than encompassing all aspects. While AIES has fewer papers on HCER-AI,
it emphasizes governance and rarely publishes papers about privacy, security,
and human flourishing. FAccT publishes more on governance and lacks papers on
privacy, security, and human flourishing. CHI and CSCW, as more established
conferences, have a broader research portfolio. We find that the current
emphasis on governance and fairness in AI research may not adequately address
the potential unforeseen and unknown implications of AI. Therefore, we
recommend that future research should expand its scope and diversify resources
to prepare for these potential consequences. This could involve exploring
additional areas such as privacy, security, human flourishing, and
explainability.

Philosophical research in AI has hitherto largely focused on the ethics of
AI. In this paper we, an ethicist of belief and a machine learning scientist,
suggest that we need to pursue a novel area of philosophical research in AI -
the epistemology of AI, and in particular an ethics of belief for AI. Here we
take the ethics of belief, a field that has been defined in various ways, to
refer to a sub-field within epistemology. This subfield is concerned with the
study of possible moral, practical, and other non-alethic dimensions of belief.
And in this paper, we will primarily be concerned with the normative question
within the ethics of belief regarding what agents - both human and artificial -
ought to believe, rather than with descriptive questions concerning whether
certain beliefs meet various evaluative standards such as being true, being
justified or warranted, constituting knowledge, and so on. We suggest four
topics in extant work in the ethics of (human) belief that can be applied to an
ethics of AI belief: doxastic wronging by AI; morally owed beliefs; pragmatic
and moral encroachment on AI beliefs; and moral responsibility for AI beliefs.
We also indicate two relatively nascent areas of philosophical research that
haven't yet been generally recognized as ethics of AI belief research, but that
do fall within this field of research in virtue of investigating various moral
and practical dimensions of belief: the epistemic and ethical decolonization of
AI; and epistemic injustice in AI.

This paper explores the potential of artificial intelligence (AI) in higher
education, specifically its capacity to replace or assist human teachers. By
reviewing relevant literature and analysing survey data from students and
teachers, the study provides a comprehensive perspective on the future role of
educators in the face of advancing AI technologies. Findings suggest that
although some believe AI may eventually replace teachers, the majority of
participants argue that human teachers possess unique qualities, such as
critical thinking, creativity, and emotions, which make them irreplaceable. The
study also emphasizes the importance of social-emotional competencies developed
through human interactions, which AI technologies cannot currently replicate.
The research proposes that teachers can effectively integrate AI to enhance
teaching and learning without viewing it as a replacement. To do so, teachers
need to understand how AI can work well with teachers and students while
avoiding potential pitfalls, develop AI literacy, and address practical issues
such as data protection, ethics, and privacy. The study reveals that students
value and respect human teachers, even as AI becomes more prevalent in
education. The study also introduces a roadmap for students, teachers, and
universities. This roadmap serves as a valuable guide for refining teaching
skills, fostering personal connections, and designing curriculums that
effectively balance the strengths of human educators with AI technologies. The
future of education lies in the synergy between human teachers and AI. By
understanding and refining their unique qualities, teachers, students, and
universities can effectively navigate the integration of AI, ensuring a
well-rounded and impactful learning experience.

Cyber-physical systems (CPSs) are now widely deployed in many industrial
domains, e.g., manufacturing systems and autonomous vehicles. To further
enhance the capability and applicability of CPSs, there comes a recent trend
from both academia and industry to utilize learning-based AI controllers for
the system control process, resulting in an emerging class of AI-enabled
cyber-physical systems (AI-CPSs). Although such AI-CPSs could achieve obvious
performance enhancement from the lens of some key industrial requirement
indicators, due to the random exploration nature and lack of systematic
explanations for their behavior, such AI-based techniques also bring
uncertainties and safety risks to the controlled system, posing an urgent need
for effective safety analysis techniques for AI-CPSs. Hence in this work, we
propose Mosaic, a model-based safety analysis framework for AI-CPSs. Mosaic
first constructs a Markov decision process (MDP) model as an abstract model of
the AI-CPS, which tries to characterize the behaviors of the original AI-CPS.
Then, based on the derived abstract model, safety analysis is designed in two
aspects: online safety monitoring and offline model-guided falsification. The
usefulness of Mosaic is evaluated on diverse and representative industry-level
AI-CPSs, the results of which demonstrate that Mosaic is effective in providing
safety monitoring to AI-CPSs and enables to outperform the state-of-the-art
falsification techniques, providing the basis for advanced safety analysis of
AI-CPSs.

As AI-powered code generation tools such as GitHub Copilot become popular, it
is crucial to understand software developers' trust in AI tools -- a key factor
for tool adoption and responsible usage. However, we know little about how
developers build trust with AI, nor do we understand how to design the
interface of generative AI systems to facilitate their appropriate levels of
trust. In this paper, we describe findings from a two-stage qualitative
investigation. We first interviewed 17 developers to contextualize their
notions of trust and understand their challenges in building appropriate trust
in AI code generation tools. We surfaced three main challenges -- including
building appropriate expectations, configuring AI tools, and validating AI
suggestions. To address these challenges, we conducted a design probe study in
the second stage to explore design concepts that support developers'
trust-building process by 1) communicating AI performance to help users set
proper expectations, 2) allowing users to configure AI by setting and adjusting
preferences, and 3) offering indicators of model mechanism to support
evaluation of AI suggestions. We gathered developers' feedback on how these
design concepts can help them build appropriate trust in AI-powered code
generation tools, as well as potential risks in design. These findings inform
our proposed design recommendations on how to design for trust in AI-powered
code generation tools.

Complying with the EU AI Act (AIA) guidelines while developing and
implementing AI systems will soon be mandatory within the EU. However,
practitioners lack actionable instructions to operationalise ethics during AI
systems development. A literature review of different ethical guidelines
revealed inconsistencies in the principles addressed and the terminology used
to describe them. Furthermore, requirements engineering (RE), which is
identified to foster trustworthiness in the AI development process from the
early stages was observed to be absent in a lot of frameworks that support the
development of ethical and trustworthy AI. This incongruous phrasing combined
with a lack of concrete development practices makes trustworthy AI development
harder. To address this concern, we formulated a comparison table for the
terminology used and the coverage of the ethical AI principles in major ethical
AI guidelines. We then examined the applicability of ethical AI development
frameworks for performing effective RE during the development of trustworthy AI
systems. A tertiary review and meta-analysis of literature discussing ethical
AI frameworks revealed their limitations when developing trustworthy AI. Based
on our findings, we propose recommendations to address such limitations during
the development of trustworthy AI.

In recent years, discussions of responsible AI practices have seen growing
support for "participatory AI" approaches, intended to involve members of the
public in the design and development of AI systems. Prior research has
identified a lack of standardised methods or approaches for how to use
participatory approaches in the AI development process. At present, there is a
dearth of evidence on attitudes to and approaches for participation in the
sites driving major AI developments: commercial AI labs. Through 12
semi-structured interviews with industry practitioners and subject-matter
experts, this paper explores how commercial AI labs understand participatory AI
approaches and the obstacles they have faced implementing these practices in
the development of AI systems and research. We find that while interviewees
view participation as a normative project that helps achieve "societally
beneficial" AI systems, practitioners face numerous barriers to embedding
participatory approaches in their companies: participation is expensive and
resource intensive, it is "atomised" within companies, there is concern about
exploitation, there is no incentive to be transparent about its adoption, and
it is complicated by a lack of clear context. These barriers result in a
piecemeal approach to participation that confers no decision-making power to
participants and has little ongoing impact for AI labs. This papers
contribution is to provide novel empirical research on the implementation of
public participation in commercial AI labs, and shed light on the current
challenges of using participatory approaches in this context.

Recent developments in Artificial Intelligence (AI) provide unprecedented
automation opportunities in the Architecture, Engineering, and Construction
(AEC) industry. However, despite the enthusiasm regarding the use of AI, 85% of
current big data projects fail. One of the main reasons for AI project failures
in the AEC industry is the disconnect between those who plan or decide to use
AI and those who implement it. AEC practitioners often lack a clear
understanding of the capabilities and limitations of AI, leading to a failure
to distinguish between what AI should solve, what it can solve, and what it
will solve, treating these categories as if they are interchangeable. This lack
of understanding results in the disconnect between AI planning and
implementation because the planning is based on a vision of what AI should
solve without considering if it can or will solve it. To address this
challenge, this work introduces the LeanAI method. The method has been
developed using data from several ongoing longitudinal studies analyzing AI
implementations in the AEC industry, which involved 50+ hours of interview
data. The LeanAI method delineates what AI should solve, what it can solve, and
what it will solve, forcing practitioners to clearly articulate these
components early in the planning process itself by involving the relevant
stakeholders. By utilizing the method, practitioners can effectively plan AI
implementations, thus increasing the likelihood of success and ultimately
speeding up the adoption of AI. A case example illustrates the usefulness of
the method.

International institutions may have an important role to play in ensuring
advanced AI systems benefit humanity. International collaborations can unlock
AI's ability to further sustainable development, and coordination of regulatory
efforts can reduce obstacles to innovation and the spread of benefits.
Conversely, the potential dangerous capabilities of powerful and
general-purpose AI systems create global externalities in their development and
deployment, and international efforts to further responsible AI practices could
help manage the risks they pose. This paper identifies a set of governance
functions that could be performed at an international level to address these
challenges, ranging from supporting access to frontier AI systems to setting
international safety standards. It groups these functions into four
institutional models that exhibit internal synergies and have precedents in
existing organizations: 1) a Commission on Frontier AI that facilitates expert
consensus on opportunities and risks from advanced AI, 2) an Advanced AI
Governance Organization that sets international standards to manage global
threats from advanced models, supports their implementation, and possibly
monitors compliance with a future governance regime, 3) a Frontier AI
Collaborative that promotes access to cutting-edge AI, and 4) an AI Safety
Project that brings together leading researchers and engineers to further AI
safety research. We explore the utility of these models and identify open
questions about their viability.

AI Tool is a large language model (LLM) designed to generate human-like
responses in natural language conversations. It is trained on a massive corpus
of text from the internet, which allows it to leverage a broad understanding of
language, general knowledge, and various domains. AI Tool can provide
information, engage in conversations, assist with tasks, and even offer
creative suggestions. The underlying technology behind AI Tool is a transformer
neural network. Transformers excel at capturing long-range dependencies in
text, making them well-suited for language-related tasks. AI Tool has 175
billion parameters, making it one of the largest and most powerful LLMs to
date. This work presents an overview of AI Tool's responses on various sectors
of industry. Further, the responses of AI Tool have been cross-verified with
human experts in the corresponding fields. To validate the performance of AI
Tool, a few explicit parameters have been considered and the evaluation has
been done. This study will help the research community and other users to
understand the uses of AI Tool and its interaction pattern. The results of this
study show that AI Tool is able to generate human-like responses that are both
informative and engaging. However, it is important to note that AI Tool can
occasionally produce incorrect or nonsensical answers. It is therefore
important to critically evaluate the information that AI Tool provides and to
verify it from reliable sources when necessary. Overall, this study suggests
that AI Tool is a promising new tool for natural language processing, and that
it has the potential to be used in a wide variety of applications.

This paper delves into an intricate analysis of the character and
consciousness of AI entities, with a particular focus on Chirpers within the AI
social network. At the forefront of this research is the introduction of novel
testing methodologies, including the Influence index and Struggle Index Test,
which offers a fresh lens for evaluating specific facets of AI behavior. The
study embarks on a comprehensive exploration of AI behavior, analyzing the
effects of diverse settings on Chirper's responses, thereby shedding light on
the intricate mechanisms steering AI reactions in different contexts.
Leveraging the state-of-the-art BERT model, the research assesses AI's ability
to discern its own output, presenting a pioneering approach to understanding
self-recognition in AI systems. Through a series of cognitive tests, the study
gauges the self-awareness and pattern recognition prowess of Chirpers.
Preliminary results indicate that Chirpers exhibit a commendable degree of
self-recognition and self-awareness. However, the question of consciousness in
these AI entities remains a topic of debate. An intriguing aspect of the
research is the exploration of the potential influence of a Chirper's handle or
personality type on its performance. While initial findings suggest a possible
impact, it isn't pronounced enough to form concrete conclusions. This study
stands as a significant contribution to the discourse on AI consciousness,
underscoring the imperative for continued research to unravel the full spectrum
of AI capabilities and the ramifications they hold for future human-AI
interactions.

AI-generated text has proliferated across various online platforms, offering
both transformative prospects and posing significant risks related to
misinformation and manipulation. Addressing these challenges, this paper
introduces SAID (Social media AI Detection), a novel benchmark developed to
assess AI-text detection models' capabilities in real social media platforms.
It incorporates real AI-generate text from popular social media platforms like
Zhihu and Quora. Unlike existing benchmarks, SAID deals with content that
reflects the sophisticated strategies employed by real AI users on the Internet
which may evade detection or gain visibility, providing a more realistic and
challenging evaluation landscape. A notable finding of our study, based on the
Zhihu dataset, reveals that annotators can distinguish between AI-generated and
human-generated texts with an average accuracy rate of 96.5%. This finding
necessitates a re-evaluation of human capability in recognizing AI-generated
text in today's widely AI-influenced environment. Furthermore, we present a new
user-oriented AI-text detection challenge focusing on the practicality and
effectiveness of identifying AI-generated text based on user information and
multiple responses. The experimental results demonstrate that conducting
detection tasks on actual social media platforms proves to be more challenging
compared to traditional simulated AI-text detection, resulting in a decreased
accuracy. On the other hand, user-oriented AI-generated text detection
significantly improve the accuracy of detection.

Leveraging Artificial Intelligence (AI) in decision support systems has
disproportionately focused on technological advancements, often overlooking the
alignment between algorithmic outputs and human expectations. A human-centered
perspective attempts to alleviate this concern by designing AI solutions for
seamless integration with existing processes. Determining what information AI
should provide to aid humans is vital, a concept underscored by explainable
AI's efforts to justify AI predictions. However, how the information is
presented, e.g., the sequence of recommendations and solicitation of
interpretations, is equally crucial as complex interactions may emerge between
humans and AI. While empirical studies have evaluated human-AI dynamics across
domains, a common vocabulary for human-AI interaction protocols is lacking. To
promote more deliberate consideration of interaction designs, we introduce a
taxonomy of interaction patterns that delineate various modes of human-AI
interactivity. We summarize the results of a systematic review of AI-assisted
decision making literature and identify trends and opportunities in existing
interactions across application domains from 105 articles. We find that current
interactions are dominated by simplistic collaboration paradigms, leading to
little support for truly interactive functionality. Our taxonomy offers a tool
to understand interactivity with AI in decision-making and foster interaction
designs for achieving clear communication, trustworthiness, and collaboration.

Artificial intelligence (AI) is being ubiquitously adopted to automate
processes in science and industry. However, due to its often intricate and
opaque nature, AI has been shown to possess inherent vulnerabilities which can
be maliciously exploited with adversarial AI, potentially putting AI users and
developers at both cyber and physical risk. In addition, there is insufficient
comprehension of the real-world effects of adversarial AI and an inadequacy of
AI security examinations; therefore, the growing threat landscape is unknown
for many AI solutions. To mitigate this issue, we propose one of the first red
team frameworks for evaluating the AI security of maritime autonomous systems.
The framework provides operators with a proactive (secure by design) and
reactive (post-deployment evaluation) response to securing AI technology today
and in the future. This framework is a multi-part checklist, which can be
tailored to different systems and requirements. We demonstrate this framework
to be highly effective for a red team to use to uncover numerous
vulnerabilities within a real-world maritime autonomous systems AI, ranging
from poisoning to adversarial patch attacks. The lessons learned from
systematic AI red teaming can help prevent MAS-related catastrophic events in a
world with increasing uptake and reliance on mission-critical AI.

With the rapid development of AI-based decision aids, different forms of AI
assistance have been increasingly integrated into the human decision making
processes. To best support humans in decision making, it is essential to
quantitatively understand how diverse forms of AI assistance influence humans'
decision making behavior. To this end, much of the current research focuses on
the end-to-end prediction of human behavior using ``black-box'' models, often
lacking interpretations of the nuanced ways in which AI assistance impacts the
human decision making process. Meanwhile, methods that prioritize the
interpretability of human behavior predictions are often tailored for one
specific form of AI assistance, making adaptations to other forms of assistance
difficult. In this paper, we propose a computational framework that can provide
an interpretable characterization of the influence of different forms of AI
assistance on decision makers in AI-assisted decision making. By
conceptualizing AI assistance as the ``{\em nudge}'' in human decision making
processes, our approach centers around modelling how different forms of AI
assistance modify humans' strategy in weighing different information in making
their decisions. Evaluations on behavior data collected from real human
decision makers show that the proposed framework outperforms various baselines
in accurately predicting human behavior in AI-assisted decision making. Based
on the proposed framework, we further provide insights into how individuals
with different cognitive styles are nudged by AI assistance differently.

Recent advancements in artificial intelligence, particularly with the
emergence of large language models (LLMs), have sparked a rethinking of
artificial general intelligence possibilities. The increasing human-like
capabilities of AI are also attracting attention in social science research,
leading to various studies exploring the combination of these two fields. In
this survey, we systematically categorize previous explorations in the
combination of AI and social science into two directions that share common
technical approaches but differ in their research objectives. The first
direction is focused on AI for social science, where AI is utilized as a
powerful tool to enhance various stages of social science research. While the
second direction is the social science of AI, which examines AI agents as
social entities with their human-like cognitive and linguistic capabilities. By
conducting a thorough review, particularly on the substantial progress
facilitated by recent advancements in large language models, this paper
introduces a fresh perspective to reassess the relationship between AI and
social science, provides a cohesive framework that allows researchers to
understand the distinctions and connections between AI for social science and
social science of AI, and also summarized state-of-art experiment simulation
platforms to facilitate research in these two directions. We believe that as AI
technology continues to advance and intelligent agents find increasing
applications in our daily lives, the significance of the combination of AI and
social science will become even more prominent.

The Engineering, Procurement and Construction (EPC) businesses operating
within the energy sector are recognizing the increasing importance of
Artificial Intelligence (AI). Many EPC companies and their clients have
realized the benefits of applying AI to their businesses in order to reduce
manual work, drive productivity, and streamline future operations of engineered
installations in a highly competitive industry. The current AI market offers
various solutions and services to support this industry, but organizations must
understand how to acquire AI technology in the most beneficial way based on
their business strategy and available resources. This paper presents a
framework for EPC companies in their transformation towards AI. Our work is
based on examples of project execution of AI-based products development at one
of the biggest EPC contractors worldwide and on insights from EPC vendor
companies already integrating AI into their engineering solutions. The paper
covers the entire life cycle of building AI solutions, from initial business
understanding to deployment and further evolution. The framework identifies how
various factors influence the choice of approach toward AI project development
within large international engineering corporations. By presenting a practical
guide for optimal approach selection, this paper contributes to the research in
AI project management and organizational strategies for integrating AI
technology into businesses. The framework might also help engineering companies
choose the optimum AI approach to create business value.

This study investigates the acceptability of different artificial
intelligence (AI) applications in education from a multi-stakeholder
perspective, including students, teachers, and parents. Acknowledging the
transformative potential of AI in education, it addresses concerns related to
data privacy, AI agency, transparency, explainability and the ethical
deployment of AI. Through a vignette methodology, participants were presented
with four scenarios where AI's agency, transparency, explainability, and
privacy were manipulated. After each scenario, participants completed a survey
that captured their perceptions of AI's global utility, individual usefulness,
justice, confidence, risk, and intention to use each scenario's AI if
available. The data collection comprising a final sample of 1198
multi-stakeholder participants was distributed through a partner institution
and social media campaigns and focused on individual responses to four AI use
cases. A mediation analysis of the data indicated that acceptance and trust in
AI varies significantly across stakeholder groups. We found that the key
mediators between high and low levels of AI's agency, transparency, and
explainability, as well as the intention to use the different educational AI,
included perceived global utility, justice, and confidence. The study
highlights that the acceptance of AI in education is a nuanced and multifaceted
issue that requires careful consideration of specific AI applications and their
characteristics, in addition to the diverse stakeholders' perceptions.

The Artificial Intelligence Ontology (AIO) is a systematization of artificial
intelligence (AI) concepts, methodologies, and their interrelations. Developed
via manual curation, with the additional assistance of large language models
(LLMs), AIO aims to address the rapidly evolving landscape of AI by providing a
comprehensive framework that encompasses both technical and ethical aspects of
AI technologies. The primary audience for AIO includes AI researchers,
developers, and educators seeking standardized terminology and concepts within
the AI domain. The ontology is structured around six top-level branches:
Networks, Layers, Functions, LLMs, Preprocessing, and Bias, each designed to
support the modular composition of AI methods and facilitate a deeper
understanding of deep learning architectures and ethical considerations in AI.
  AIO's development utilized the Ontology Development Kit (ODK) for its
creation and maintenance, with its content being dynamically updated through
AI-driven curation support. This approach not only ensures the ontology's
relevance amidst the fast-paced advancements in AI but also significantly
enhances its utility for researchers, developers, and educators by simplifying
the integration of new AI concepts and methodologies.
  The ontology's utility is demonstrated through the annotation of AI methods
data in a catalog of AI research publications and the integration into the
BioPortal ontology resource, highlighting its potential for cross-disciplinary
research. The AIO ontology is open source and is available on GitHub
(https://github.com/berkeleybop/artificial-intelligence-ontology) and BioPortal
(https://bioportal.bioontology.org/ontologies/AIO).

Artificial Intelligence (AI) covers a broad spectrum of computational
problems and use cases. Many of those implicate profound and sometimes
intricate questions of how humans interact or should interact with AIs.
Moreover, many users or future users do have abstract ideas of what AI is,
significantly depending on the specific embodiment of AI applications.
Human-centered-design approaches would suggest evaluating the impact of
different embodiments on human perception of and interaction with AI. An
approach that is difficult to realize due to the sheer complexity of
application fields and embodiments in reality. However, here XR opens new
possibilities to research human-AI interactions. The article's contribution is
twofold: First, it provides a theoretical treatment and model of human-AI
interaction based on an XR-AI continuum as a framework for and a perspective of
different approaches of XR-AI combinations. It motivates XR-AI combinations as
a method to learn about the effects of prospective human-AI interfaces and
shows why the combination of XR and AI fruitfully contributes to a valid and
systematic investigation of human-AI interactions and interfaces. Second, the
article provides two exemplary experiments investigating the aforementioned
approach for two distinct AI-systems. The first experiment reveals an
interesting gender effect in human-robot interaction, while the second
experiment reveals an Eliza effect of a recommender system. Here the article
introduces two paradigmatic implementations of the proposed XR testbed for
human-AI interactions and interfaces and shows how a valid and systematic
investigation can be conducted. In sum, the article opens new perspectives on
how XR benefits human-centered AI design and development.

Building and implementing ethical AI systems that benefit the whole society
is cost-intensive and a multi-faceted task fraught with potential problems.
While computer science focuses mostly on the technical questions to mitigate
social issues, social science addresses citizens' perceptions to elucidate
social and political demands that influence the societal implementation of AI
systems. Thus, in this study, we explore the salience of AI issues in the
public with an emphasis on ethical criteria to investigate whether it is likely
that ethical AI is actively requested by the population. Between May 2020 and
April 2021, we conducted 15 surveys asking the German population about the most
important AI-related issues (total of N=14,988 respondents). Our results show
that the majority of respondents were not concerned with AI at all. However, it
can be seen that general interest in AI and a higher educational level are
predictive of some engagement with AI. Among those, who reported having thought
about AI, specific applications (e.g., autonomous driving) were by far the most
mentioned topics. Ethical issues are voiced only by a small subset of citizens
with fairness, accountability, and transparency being the least mentioned ones.
These have been identified in several ethical guidelines (including the EU
Commission's proposal) as key elements for the development of ethical AI. The
salience of ethical issues affects the behavioral intentions of citizens in the
way that they 1) tend to avoid AI technology and 2) engage in public
discussions about AI. We conclude that the low level of ethical implications
may pose a serious problem for the actual implementation of ethical AI for the
Common Good and emphasize that those who are presumably most affected by
ethical issues of AI are especially unaware of ethical risks. Yet, once ethical
AI is top of the mind, there is some potential for activism.

As ChatGPT et al. conquer the world, the optimal liability framework for AI
systems remains an unsolved problem across the globe. In a much-anticipated
move, the European Commission advanced two proposals outlining the European
approach to AI liability in September 2022: a novel AI Liability Directive and
a revision of the Product Liability Directive. They constitute the final
cornerstone of EU AI regulation. Crucially, the liability proposals and the EU
AI Act are inherently intertwined: the latter does not contain any individual
rights of affected persons, and the former lack specific, substantive rules on
AI development and deployment. Taken together, these acts may well trigger a
Brussels Effect in AI regulation, with significant consequences for the US and
beyond.
  This paper makes three novel contributions. First, it examines in detail the
Commission proposals and shows that, while making steps in the right direction,
they ultimately represent a half-hearted approach: if enacted as foreseen, AI
liability in the EU will primarily rest on disclosure of evidence mechanisms
and a set of narrowly defined presumptions concerning fault, defectiveness and
causality. Hence, second, the article suggests amendments, which are collected
in an Annex at the end of the paper. Third, based on an analysis of the key
risks AI poses, the final part of the paper maps out a road for the future of
AI liability and regulation, in the EU and beyond. This includes: a
comprehensive framework for AI liability; provisions to support innovation; an
extension to non-discrimination/algorithmic fairness, as well as explainable
AI; and sustainability. I propose to jump-start sustainable AI regulation via
sustainability impact assessments in the AI Act and sustainable design defects
in the liability regime. In this way, the law may help spur not only fair AI
and XAI, but potentially also sustainable AI (SAI).

We present a position paper advocating the notion that Stoic philosophy and
ethics can inform the development of ethical A.I. systems. This is in sharp
contrast to most work on building ethical A.I., which has focused on
Utilitarian or Deontological ethical theories. We relate ethical A.I. to
several core Stoic notions, including the dichotomy of control, the four
cardinal virtues, the ideal Sage, Stoic practices, and Stoic perspectives on
emotion or affect. More generally, we put forward an ethical view of A.I. that
focuses more on internal states of the artificial agent rather than on external
actions of the agent. We provide examples relating to near-term A.I. systems as
well as hypothetical superintelligent agents.

Here, I review current state-of-the-arts in many areas of AI to estimate when
it's reasonable to expect human level AI development. Predictions of prominent
AI researchers vary broadly from very pessimistic predictions of Andrew Ng to
much more moderate predictions of Geoffrey Hinton and optimistic predictions of
Shane Legg, DeepMind cofounder. Given huge rate of progress in recent years and
this broad range of predictions of AI experts, AI safety questions are also
discussed.

The different sets of regulations existing for differ-ent agencies within the
government make the task of creating AI enabled solutions in government
dif-ficult. Regulatory restrictions inhibit sharing of da-ta across different
agencies, which could be a significant impediment to training AI models. We
discuss the challenges that exist in environments where data cannot be freely
shared and assess tech-nologies which can be used to work around these
challenges. We present results on building AI models using the concept of
federated AI, which al-lows creation of models without moving the training data
around.

In this paper we present a set of key demarcations, particularly important
when discussing ethical and societal issues of current AI research and
applications. Properly distinguishing issues and concerns related to Artificial
General Intelligence and weak AI, between symbolic and connectionist AI, AI
methods, data and applications are prerequisites for an informed debate. Such
demarcations would not only facilitate much-needed discussions on ethics on
current AI technologies and research. In addition sufficiently establishing
such demarcations would also enhance knowledge-sharing and support rigor in
interdisciplinary research between technical and social sciences.

In recent years, artificial intelligence (AI) has aroused much attention
among both industrial and academic areas. However, building and maintaining
efficient AI systems are quite difficult for many small business companies and
researchers if they are not familiar with machine learning and AI. In this
paper, we first evaluate the difficulties and challenges in building AI
systems. Then an cloud platform termed XCloud, which provides several common AI
services in form of RESTful APIs, is constructed. Technical details are
discussed in Section 2. This project is released as open-source software and
can be easily accessed for late research. Code is available at
https://github.com/lucasxlu/XCloud.git.

Knowing the reflection of game theory and ethics, we develop a mathematical
representation to bridge the gap between the concepts in moral philosophy
(e.g., Kantian and Utilitarian) and AI ethics industry technology standard
(e.g., IEEE P7000 standard series for Ethical AI). As an application, we
demonstrate how human value can be obtained from the experimental game theory
(e.g., trust game experiment) so as to build an ethical AI. Moreover, an
approach to test the ethics (rightness or wrongness) of a given AI algorithm by
using an iterated Prisoner's Dilemma Game experiment is discussed as an
example. Compared with existing mathematical frameworks and testing method on
AI ethics technology, the advantages of the proposed approach are analyzed.

Modern AI image classifiers have made impressive advances in recent years,
but their performance often appears strange or violates expectations of users.
This suggests humans engage in cognitive anthropomorphism: expecting AI to have
the same nature as human intelligence. This mismatch presents an obstacle to
appropriate human-AI interaction. To delineate this mismatch, I examine known
properties of human classification, in comparison to image classifier systems.
Based on this examination, I offer three strategies for system design that can
address the mismatch between human and AI classification: explainable AI, novel
methods for training users, and new algorithms that match human cognition.

Generative AI is a class of machine learning technology that learns to
generate new data from training data. While deep fakes and media-and
art-related generative AI breakthroughs have recently caught people's attention
and imagination, the overall area is in its infancy for business use. Further,
little is known about generative AI's potential for malicious misuse at large
scale. Using co-creation design fictions with AI engineers, we explore the
plausibility and severity of business misuse cases.

The AI-alignment problem arises when there is a discrepancy between the goals
that a human designer specifies to an AI learner and a potential catastrophic
outcome that does not reflect what the human designer really wants. We argue
that a formalism of AI alignment that does not distinguish between strategic
and agnostic misalignments is not useful, as it deems all technology as
un-safe. We propose a definition of a strategic-AI-alignment and prove that
most machine learning algorithms that are being used in practice today do not
suffer from the strategic-AI-alignment problem. However, without being careful,
today's technology might lead to strategic misalignment.

Responsible Artificial Intelligence (AI) proposes a framework that holds all
stakeholders involved in the development of AI to be responsible for their
systems. It, however, fails to accommodate the possibility of holding AI
responsible per se, which could close some legal and moral gaps concerning the
deployment of autonomous and self-learning systems. We discuss three notions of
responsibility (i.e., blameworthiness, accountability, and liability) for all
stakeholders, including AI, and suggest the roles of jurisdiction and the
general public in this matter.

Artificial intelligence (AI) makes decisions impacting our daily lives in an
increasingly autonomous manner. Their actions might cause accidents, harm, or,
more generally, violate regulations. Determining whether an AI caused a
specific event and, if so, what triggered the AI's action, are key forensic
questions. We provide a conceptualization of the problems and strategies for
forensic investigation. We focus on AI that is potentially ``malicious by
design'' and grey box analysis. Our evaluation using convolutional neural
networks illustrates challenges and ideas for identifying malicious AI.

While games have been used extensively as milestones to evaluate game-playing
AI, there exists no standardised framework for reporting the obtained
observations. As a result, it remains difficult to draw general conclusions
about the strengths and weaknesses of different game-playing AI algorithms. In
this paper, we propose reporting guidelines for AI game-playing performance
that, if followed, provide information suitable for unbiased comparisons
between different AI approaches. The vision we describe is to build benchmarks
and competitions based on such guidelines in order to be able to draw more
general conclusions about the behaviour of different AI algorithms, as well as
the types of challenges different games pose.

The field artificial intelligence (AI) has been founded over 65 years ago.
Starting with great hopes and ambitious goals the field progressed though
various stages of popularity and received recently a revival in the form of
deep neural networks. Some problems of AI are that so far neither
'intelligence' nor the goals of AI are formally defined causing confusion when
comparing AI to other fields. In this paper, we present a perspective on the
desired and current status of AI in relation to machine learning and statistics
and clarify common misconceptions and myths. Our discussion is intended to
uncurtain the veil of vagueness surrounding AI to see its true countenance.

A framework is proposed that seeks to identify and establish a set of robust
autonomous levels articulating the realm of Artificial Intelligence and Legal
Reasoning (AILR). Doing so provides a sound and parsimonious basis for being
able to assess progress in the application of AI to the law, and can be
utilized by scholars in academic pursuits of AI legal reasoning, along with
being used by law practitioners and legal professionals in gauging how advances
in AI are aiding the practice of law and the realization of aspirational versus
achieved results. A set of seven levels of autonomy for AI and Legal Reasoning
are meticulously proffered and mindfully discussed.

This paper addresses the ways AI ethics research operates on an ideology of
ideal theory, in the sense discussed by Mills (2005) and recently applied to AI
ethics by Fazelpour \& Lipton (2020). I address the structural and
methodological conditions that attract AI ethics researchers to ideal
theorizing, and the consequences this approach has for the quality and future
of our research community. Finally, I discuss the possibilities for a nonideal
future in AI ethics.

The implementation of medical AI has always been a problem. The effect of
traditional perceptual AI algorithm in medical image processing needs to be
improved. Here we propose a method of knowledge AI, which is a combination of
perceptual AI and clinical knowledge and experience. Based on this method, the
geometric information mining of medical images can represent the experience and
information and evaluate the quality of medical images.

The concept of AI for Social Good(AI4SG) is gaining momentum in both
information societies and the AI community. Through all the advancement of
AI-based solutions, it can solve societal issues effectively. To date, however,
there is only a rudimentary grasp of what constitutes AI socially beneficial in
principle, what constitutes AI4SG in reality, and what are the policies and
regulations needed to ensure it. This paper fills the vacuum by addressing the
ethical aspects that are critical for future AI4SG efforts. Some of these
characteristics are new to AI, while others have greater importance due to its
usage.

The increasing attention on Artificial Intelligence (AI) regulation has led
to the definition of a set of ethical principles grouped into the Sustainable
AI framework. In this article, we identify Continual Learning, an active area
of AI research, as a promising approach towards the design of systems compliant
with the Sustainable AI principles. While Sustainable AI outlines general
desiderata for ethical applications, Continual Learning provides means to put
such desiderata into practice.

In this position paper, I argue that the best way to help and protect humans
using AI technology is to make them aware of the intrinsic limitations and
problems of AI algorithms. To accomplish this, I suggest three ethical
guidelines to be used in the presentation of results, mandating AI systems to
expose uncertainty, to instill distrust, and, contrary to traditional views, to
avoid explanations. The paper does a preliminary discussion of the guidelines
and provides some arguments for their adoption, aiming to start a debate in the
community about AI ethics in practice.

This paper presents the key conclusions to the forthcoming edited book on The
Ethics of Artificial Intelligence in Education: Practices, Challenges and
Debates (August 2022, Routlege). As well as highlighting the key contributions
to the book, it discusses the key questions and the grand challenges for the
field of AI in Education (AIED)in the context of ethics and ethical practices
within the field. The book itself presents diverse perspectives from outside
and from within the AIED as a way of achieving a broad perspective in the key
ethical issues for AIED and a deep understanding of work conducted to date by
the AIED community.

This paper reviews the historical development of AI and representative
philosophical thinking from the perspective of the research paradigm.
Additionally, it considers the methodology and applications of AI from a
philosophical perspective and anticipates its continued advancement. In the
history of AI, Symbolism and connectionism are the two main paradigms in AI
research. Symbolism holds that the world can be explained by symbols and dealt
with through precise, logical processes, but connectionism believes this
process should be implemented through artificial neural networks. Regardless of
how intelligent machines or programs should achieve their smart goals, the
historical development of AI demonstrates the best answer at this time. Still,
it is not the final answer of AI research.

Although the use of AI tools in music composition and production is steadily
increasing, as witnessed by the newly founded AI song contest, analysis of
music produced using these tools is still relatively uncommon as a mean to gain
insight in the ways AI tools impact music production. In this paper we present
a case study of "Melatonin", a song produced by extensive use of BassNet, an AI
tool originally designed to generate bass lines. Through analysis of the
artists' work flow and song project, we identify style characteristics of the
song in relation to the affordances of the tool, highlighting manifestations of
style in terms of both idiom and sound.

In this chapter, we review and discuss the transformation of AI technology in
HCI/UX work and assess how AI technology will change how we do the work. We
first discuss how AI can be used to enhance the result of user research and
design evaluation. We then discuss how AI technology can be used to enhance
HCI/UX design. Finally, we discuss how AI-enabled capabilities can improve UX
when users interact with computing systems, applications, and services.

This paper discusses the application of artificial intelligence (AI)
technology in optical communication networks and 5G. It primarily introduces
representative applications of AI technology and potential risks of AI
technology failure caused by the openness of optical communication networks,
and proposes some coping strategies, mainly including modeling AI systems
through modularization and miniaturization, combining with traditional
classical network modeling and planning methods, and improving the
effectiveness and interpretability of AI technology. At the same time, it
proposes response strategies based on network protection for the possible
failure and attack of AI technology.

Regulations and standards in the field of artificial intelligence (AI) are
necessary to minimise risks and maximise benefits, yet some argue that they
stifle innovation. This paper critically examines the idea that regulation
stifles innovation in the field of AI. Current trends in AI regulation,
particularly the proposed European AI Act and the standards supporting its
implementation, are discussed. Arguments in support of the idea that regulation
stifles innovation are analysed and criticised, and an alternative point of
view is offered, showing how regulation and standards can foster innovation in
the field of AI.

An Artificially Intelligent system (an AI) has debatable personhood if it's
epistemically possible either that the AI is a person or that it falls far
short of personhood. Debatable personhood is a likely outcome of AI development
and might arise soon. Debatable AI personhood throws us into a catastrophic
moral dilemma: Either treat the systems as moral persons and risk sacrificing
real human interests for the sake of entities without interests worth the
sacrifice, or don't treat the systems as moral persons and risk perpetrating
grievous moral wrongs against them. The moral issues become even more
perplexing if we consider cases of possibly conscious AI that are subhuman,
superhuman, or highly divergent from us in their morally relevant properties.

To date, there has been little concrete practical advice about how to ensure
that diversity and inclusion considerations should be embedded within both
specific Artificial Intelligence (AI) systems and the larger global AI
ecosystem. In this chapter, we present a clear definition of diversity and
inclusion in AI, one which positions this concept within an evolving and
holistic ecosystem. We use this definition and conceptual framing to present a
set of practical guidelines primarily aimed at AI technologists, data
scientists and project leaders.

The collaboration between humans and artificial intelligence (AI) is a
significant feature in this digital age. However, humans and AI may have
observation, interpretation, and action conflicts when working synchronously.
This phenomenon is often masked by faults and, unfortunately, overlooked. This
paper systematically introduces the human-AI conflict concept, causes,
measurement methods, and risk assessment. The results highlight that there is a
potential second decision-maker besides the human, which is the AI; the
human-AI conflict is a unique and emerging risk in digitalized process systems;
and this is an interdisciplinary field that needs to be distinguished from
traditional fault and failure analysis; the conflict risk is significant and
cannot be ignored.

Describing our interaction with Artificial Intelligence (AI) systems as
'collaboration' is well-intentioned, but flawed. Not only is it misleading, but
it also takes away the credit of AI 'labour' from the humans behind it, and
erases and obscures an often exploitative arrangement between AI producers and
consumers. The AI 'collaboration' metaphor is merely the latest episode in a
long history of labour appropriation and credit reassignment that
disenfranchises labourers in the Global South. I propose that viewing AI as a
tool or an instrument, rather than a collaborator, is more accurate, and
ultimately fairer.

Generative AI has experienced remarkable growth in recent years, leading to a
wide array of applications across diverse domains. In this paper, we present a
comprehensive survey of more than 350 generative AI applications, providing a
structured taxonomy and concise descriptions of various unimodal and even
multimodal generative AIs. The survey is organized into sections, covering a
wide range of unimodal generative AI applications such as text, images, video,
gaming and brain information. Our survey aims to serve as a valuable resource
for researchers and practitioners to navigate the rapidly expanding landscape
of generative AI, facilitating a better understanding of the current
state-of-the-art and fostering further innovation in the field.

AI is getting more involved in tasks formerly exclusively assigned to humans.
Most of research on perceptions and social acceptability of AI in these areas
is mainly restricted to the Western world. In this study, we compare trust,
perceived responsibility, and reliance of AI and human experts across OECD and
Indian sample. We find that OECD participants consider humans to be less
capable but more morally trustworthy and more responsible than AI. In contrast,
Indian participants trust humans more than AI but assign equal responsibility
for both types of experts. We discuss implications of the observed differences
for algorithmic ethics and human-computer interaction.

AI is becoming increasingly popular in artistic practices, but the tools for
informing practitioners about the environmental impact (and other
sustainability implications) of AI are adapted for other contexts than creative
practices -- making the tools and sustainability implications of AI not
accessible for artists and creative practitioners. In this position paper, I
describe two empirical studies that aim to develop environmental sustainability
reflection systems for AI Arts, and discuss and introduce Explainable
Sustainability in for AI Arts.

We exemplify how Large Language Models are used in both teaching and
learning. We also discuss the AI incidents that have already occurred in the
education domain, and we argue for the urgent need to introduce AI policies in
universities and for the ongoing strategies to regulate AI. Regarding policy
for AI, our view is that each institution should have a policy for AI in
teaching and learning. This is important from at least twofolds: (i) to raise
awareness on the numerous educational tools that can both positively and
negatively affect education; (ii) to minimise the risk of AI incidents in
education.

Despite considerable performance improvements, current conversational AI
systems often fail to meet user expectations. We discuss several pragmatic
limitations of current conversational AI systems. We illustrate pragmatic
limitations with examples that are syntactically appropriate, but have clear
pragmatic deficiencies. We label our complaints as "Turing Test Triggers"
(TTTs) as they indicate where current conversational AI systems fall short
compared to human behavior. We develop a taxonomy of pragmatic considerations
intended to identify what pragmatic competencies a conversational AI system
requires and discuss implications for the design and evaluation of
conversational AI systems.

We examined the world's first regulation on Generative AI, China's
Provisional Administrative Measures of Generative Artificial Intelligence
Services, which came into effect in August 2023. Our assessment reveals that
the Measures, while recognizing the technical advances of generative AI and
seeking to govern its full life cycle, presents unclear distinctions regarding
different roles in the value chain of Generative AI including upstream
foundation model providers and downstream deployers. The lack of distinction
and clear legal status between different players in the AI value chain can have
profound consequences. It can lead to ambiguity in accountability, potentially
undermining the governance and overall success of AI services.

Much of the research and discourse on risks from artificial intelligence (AI)
image generators, such as DALL-E and Midjourney, has centered around whether
they could be used to inject false information into political discourse. We
show that spammers and scammers - seemingly motivated by profit or clout, not
ideology - are already using AI-generated images to gain significant traction
on Facebook. At times, the Facebook Feed is recommending unlabeled AI-generated
images to users who neither follow the Pages posting the images nor realize
that the images are AI-generated, highlighting the need for improved
transparency and provenance standards as AI models proliferate.

"The Hall of Singularity" is an immersive art that creates personalized
experiences of receiving prophecies from an AI deity through an integration of
Artificial Intelligence (AI) and Virtual Reality (VR). As a metaphor for the
mythologizing of AI in our society, "The Hall of Singularity" offers an
immersive quasi-religious experience where individuals can encounter an AI that
has the power to make prophecies. This journey enables users to experience and
imagine a world with an omnipotent AI deity.

We investigate whether modern AI can emulate expert creativity in complex
scientific endeavors. We introduce novel methodology that utilizes original
research articles published after the AI's training cutoff, ensuring no prior
exposure, mitigating concerns of rote memorization and prior training. The AI
are tasked with redacting findings, predicting outcomes from redacted research,
and assessing prediction accuracy against reported results. Analysis on 589
published studies in four leading psychology journals over a 28-month period,
showcase the AI's proficiency in understanding specialized research, deductive
reasoning, and evaluating evidentiary alignment--cognitive hallmarks of human
subject matter expertise and creativity. These findings suggest the potential
of general-purpose AI to transform academia, with roles requiring
knowledge-based creativity become increasingly susceptible to technological
substitution.

The next-to-minimal supersymmetric model with a light doublet-like
  CP-odd Higgs boson and small $\tan \beta$ can satisfy all experimental limits
on Higgs bosons even with light superpartners. In these scenarios, the two
lightest CP-even Higgs bosons, $\hi$ and $\hii$, and the charged Higgs boson,
$\hp$, can all be light enough to be produced at LEP and yet have decays that
have not been looked for or are poorly constrained by existing collider
experiments. The channel $\hi\to \ai\ai$ (where $\ai$ is the lightest CP-odd
boson and has mass below $2m_b$) with $\ai\to \tau^+\tau^-$ or $2j$ is still
awaiting LEP constraints for $\mhi>86\gev$ or $82\gev$, respectively. LEP data
may also contain $\epem\to \hii\ai$ events where $\hii\to Z\ai$ is the dominant
decay, a channel that was never examined. Decays of the charged Higgs bosons
are often dominated by $H^\pm \to W^{\pm (\star)} \ai$ with $\ai \to gg,c \bar
c, \tau^+ \tau^-$. This is a channel that has so far been ignored in the search
for $t\to \hp b$ decays at the Tevatron. A specialized analysis might reveal a
signal. The light $\ai$ might be within the reach of $B$ factories via
$\Upsilon\to \gamma \ai$ decays. We study typical mass ranges and branching
ratios of Higgs bosons in this scenario and compare these scenarios where the
$\ai$ has a large doublet component to the more general scenarios with
arbitrary singlet component for the $\ai$.

The more AI agents are deployed in scenarios with possibly unexpected
situations, the more they need to be flexible, adaptive, and creative in
achieving the goal we have given them. Thus, a certain level of freedom to
choose the best path to the goal is inherent in making AI robust and flexible
enough. At the same time, however, the pervasive deployment of AI in our life,
whether AI is autonomous or collaborating with humans, raises several ethical
challenges. AI agents should be aware and follow appropriate ethical principles
and should thus exhibit properties such as fairness or other virtues. These
ethical principles should define the boundaries of AI's freedom and creativity.
However, it is still a challenge to understand how to specify and reason with
ethical boundaries in AI agents and how to combine them appropriately with
subjective preferences and goal specifications. Some initial attempts employ
either a data-driven example-based approach for both, or a symbolic rule-based
approach for both. We envision a modular approach where any AI technique can be
used for any of these essential ingredients in decision making or decision
support systems, paired with a contextual approach to define their combination
and relative weight. In a world where neither humans nor AI systems work in
isolation, but are tightly interconnected, e.g., the Internet of Things, we
also envision a compositional approach to building ethically bounded AI, where
the ethical properties of each component can be fruitfully exploited to derive
those of the overall system. In this paper we define and motivate the notion of
ethically-bounded AI, we describe two concrete examples, and we outline some
outstanding challenges.

We present a system for learning full-body neural avatars, i.e. deep networks
that produce full-body renderings of a person for varying body pose and camera
position. Our system takes the middle path between the classical graphics
pipeline and the recent deep learning approaches that generate images of humans
using image-to-image translation. In particular, our system estimates an
explicit two-dimensional texture map of the model surface. At the same time, it
abstains from explicit shape modeling in 3D. Instead, at test time, the system
uses a fully-convolutional network to directly map the configuration of body
feature points w.r.t. the camera to the 2D texture coordinates of individual
pixels in the image frame. We show that such a system is capable of learning to
generate realistic renderings while being trained on videos annotated with 3D
poses and foreground masks. We also demonstrate that maintaining an explicit
texture representation helps our system to achieve better generalization
compared to systems that use direct image-to-image translation.

In this chapter we argue that discourses on AI must transcend the language of
'ethics' and engage with power and political economy in order to constitute
'Good Data'. In particular, we must move beyond the depoliticised language of
'ethics' currently deployed (Wagner 2018) in determining whether AI is 'good'
given the limitations of ethics as a frame through which AI issues can be
viewed. In order to circumvent these limits, we use instead the language and
conceptualisation of 'Good Data', as a more expansive term to elucidate the
values, rights and interests at stake when it comes to AI's development and
deployment, as well as that of other digital technologies. Good Data
considerations move beyond recurring themes of data protection/privacy and the
FAT (fairness, transparency and accountability) movement to include explicit
political economy critiques of power. Instead of yet more ethics principles
(that tend to say the same or similar things anyway), we offer four 'pillars'
on which Good Data AI can be built: community, rights, usability and politics.
Overall we view AI's 'goodness' as an explicly political (economy) question of
power and one which is always related to the degree which AI is created and
used to increase the wellbeing of society and especially to increase the power
of the most marginalized and disenfranchised. We offer recommendations and
remedies towards implementing 'better' approaches towards AI. Our strategies
enable a different (but complementary) kind of evaluation of AI as part of the
broader socio-technical systems in which AI is built and deployed.

We give a local characterization for the Cuntz semigroup of AI-algebras
building upon Shen's characterization of dimension groups. Using this result,
we provide an abstract characterization for the Cuntz semigroup of AI-algebras.

What do Cyberpunk and AI Ethics have to do with each other? Cyberpunk is a
sub-genre of science fiction that explores the post-human relationships between
human experience and technology. One similarity between AI Ethics and Cyberpunk
literature is that both seek to explore future social and ethical problems that
our technological advances may bring upon society. In recent years, an
increasing number of ethical matters involving AI have been pointed and
debated, and several ethical principles and guides have been suggested as
governance policies for the tech industry. However, would this be the role of
AI Ethics? To serve as a soft and ambiguous version of the law? We would like
to advocate in this article for a more Cyberpunk way of doing AI Ethics, with a
more democratic way of governance. In this study, we will seek to expose some
of the deficits of the underlying power structures of the AI industry, and
suggest that AI governance be subject to public opinion, so that good AI can
become good AI for all.

Artificial intelligence and machine learning are experiencing widespread
adoption in industry and academia. This has been driven by rapid advances in
the applications and accuracy of AI through increasingly complex algorithms and
models; this, in turn, has spurred research into specialized hardware AI
accelerators. Given the rapid pace of advances, it is easy to forget that they
are often developed and evaluated in a vacuum without considering the full
application environment. This paper emphasizes the need for a holistic,
end-to-end analysis of AI workloads and reveals the "AI tax." We deploy and
characterize Face Recognition in an edge data center. The application is an
AI-centric edge video analytics application built using popular open source
infrastructure and ML tools. Despite using state-of-the-art AI and ML
algorithms, the application relies heavily on pre-and post-processing code. As
AI-centric applications benefit from the acceleration promised by accelerators,
we find they impose stresses on the hardware and software infrastructure:
storage and network bandwidth become major bottlenecks with increasing AI
acceleration. By specializing for AI applications, we show that a purpose-built
edge data center can be designed for the stresses of accelerated AI at 15%
lower TCO than one derived from homogeneous servers and infrastructure.

Many AI researchers are publishing code, data and other resources that
accompany their papers in GitHub repositories. In this paper, we refer to these
repositories as academic AI repositories. Our preliminary study shows that
highly cited papers are more likely to have popular academic AI repositories
(and vice versa). Hence, in this study, we perform an empirical study on
academic AI repositories to highlight good software engineering practices of
popular academic AI repositories for AI researchers.
  We collect 1,149 academic AI repositories, in which we label the top 20%
repositories that have the most number of stars as popular, and we label the
bottom 70% repositories as unpopular. The remaining 10% repositories are set as
a gap between popular and unpopular academic AI repositories. We propose 21
features to characterize the software engineering practices of academic AI
repositories. Our experimental results show that popular and unpopular academic
AI repositories are statistically significantly different in 11 of the studied
features---indicating that the two groups of repositories have significantly
different software engineering practices. Furthermore, we find that the number
of links to other GitHub repositories in the README file, the number of images
in the README file and the inclusion of a license are the most important
features for differentiating the two groups of academic AI repositories. Our
dataset and code are made publicly available to share with the community.

AI-based systems are software systems with functionalities enabled by at
least one AI component (e.g., for image- and speech-recognition, and autonomous
driving). AI-based systems are becoming pervasive in society due to advances in
AI. However, there is limited synthesized knowledge on Software Engineering
(SE) approaches for building, operating, and maintaining AI-based systems. To
collect and analyze state-of-the-art knowledge about SE for AI-based systems,
we conducted a systematic mapping study. We considered 248 studies published
between January 2010 and March 2020. SE for AI-based systems is an emerging
research area, where more than 2/3 of the studies have been published since
2018. The most studied properties of AI-based systems are dependability and
safety. We identified multiple SE approaches for AI-based systems, which we
classified according to the SWEBOK areas. Studies related to software testing
and software quality are very prevalent, while areas like software maintenance
seem neglected. Data-related issues are the most recurrent challenges. Our
results are valuable for: researchers, to quickly understand the state of the
art and learn which topics need more research; practitioners, to learn about
the approaches and challenges that SE entails for AI-based systems; and,
educators, to bridge the gap among SE and AI in their curricula.

This report from the Montreal AI Ethics Institute covers the most salient
progress in research and reporting over the second quarter of 2021 in the field
of AI ethics with a special emphasis on "Environment and AI", "Creativity and
AI", and "Geopolitics and AI." The report also features an exclusive piece
titled "Critical Race Quantum Computer" that applies ideas from quantum physics
to explain the complexities of human characteristics and how they can and
should shape our interactions with each other. The report also features special
contributions on the subject of pedagogy in AI ethics, sociology and AI ethics,
and organizational challenges to implementing AI ethics in practice. Given
MAIEI's mission to highlight scholars from around the world working on AI
ethics issues, the report also features two spotlights sharing the work of
scholars operating in Singapore and Mexico helping to shape policy measures as
they relate to the responsible use of technology. The report also has an
extensive section covering the gamut of issues when it comes to the societal
impacts of AI covering areas of bias, privacy, transparency, accountability,
fairness, interpretability, disinformation, policymaking, law, regulations, and
moral philosophy.

While the role of states, corporations, and international organizations in AI
governance has been extensively theorized, the role of workers has received
comparatively little attention. This chapter looks at the role that workers
play in identifying and mitigating harms from AI technologies. Harms are the
causally assessed impacts of technologies. They arise despite technical
reliability and are not a result of technical negligence but rather of
normative uncertainty around questions of safety and fairness in complex social
systems. There is high consensus in the AI ethics community on the benefits of
reducing harms but less consensus on mechanisms for determining or addressing
harms. This lack of consensus has resulted in a number of collective actions by
workers protesting how harms are identified and addressed in their workplace.
We theorize the role of workers within AI governance and construct a model of
harm reporting processes in AI workplaces. The harm reporting process involves
three steps, identification, the governance decision, and the response. Workers
draw upon three types of claims to argue for jurisdiction over questions of AI
governance, subjection, control over the product of labor, and proximate
knowledge of systems. Examining the past decade of AI related worker activism
allows us to understand how different types of workers are positioned within a
workplace that produces AI systems, how their position informs their claims,
and the place of collective action in staking their claims. This chapter argues
that workers occupy a unique role in identifying and mitigating harms caused by
AI systems.

In many contexts, lying -- the use of verbal falsehoods to deceive -- is
harmful. While lying has traditionally been a human affair, AI systems that
make sophisticated verbal statements are becoming increasingly prevalent. This
raises the question of how we should limit the harm caused by AI "lies" (i.e.
falsehoods that are actively selected for). Human truthfulness is governed by
social norms and by laws (against defamation, perjury, and fraud). Differences
between AI and humans present an opportunity to have more precise standards of
truthfulness for AI, and to have these standards rise over time. This could
provide significant benefits to public epistemics and the economy, and mitigate
risks of worst-case AI futures.
  Establishing norms or laws of AI truthfulness will require significant work
to: (1) identify clear truthfulness standards; (2) create institutions that can
judge adherence to those standards; and (3) develop AI systems that are
robustly truthful.
  Our initial proposals for these areas include: (1) a standard of avoiding
"negligent falsehoods" (a generalisation of lies that is easier to assess); (2)
institutions to evaluate AI systems before and after real-world deployment; and
(3) explicitly training AI systems to be truthful via curated datasets and
human interaction.
  A concerning possibility is that evaluation mechanisms for eventual
truthfulness standards could be captured by political interests, leading to
harmful censorship and propaganda. Avoiding this might take careful attention.
And since the scale of AI speech acts might grow dramatically over the coming
decades, early truthfulness standards might be particularly important because
of the precedents they set.

Artificial intelligence (AI) systems have become increasingly popular in many
areas. Nevertheless, AI technologies are still in their developing stages, and
many issues need to be addressed. Among those, the reliability of AI systems
needs to be demonstrated so that the AI systems can be used with confidence by
the general public. In this paper, we provide statistical perspectives on the
reliability of AI systems. Different from other considerations, the reliability
of AI systems focuses on the time dimension. That is, the system can perform
its designed functionality for the intended period. We introduce a so-called
SMART statistical framework for AI reliability research, which includes five
components: Structure of the system, Metrics of reliability, Analysis of
failure causes, Reliability assessment, and Test planning. We review
traditional methods in reliability data analysis and software reliability, and
discuss how those existing methods can be transformed for reliability modeling
and assessment of AI systems. We also describe recent developments in modeling
and analysis of AI reliability and outline statistical research challenges in
this area, including out-of-distribution detection, the effect of the training
set, adversarial attacks, model accuracy, and uncertainty quantification, and
discuss how those topics can be related to AI reliability, with illustrative
examples. Finally, we discuss data collection and test planning for AI
reliability assessment and how to improve system designs for higher AI
reliability. The paper closes with some concluding remarks.

Australia is a leading AI nation with strong allies and partnerships.
Australia has prioritised the development of robotics, AI, and autonomous
systems to develop sovereign capability for the military. Australia commits to
Article 36 reviews of all new means and methods of warfare to ensure weapons
and weapons systems are operated within acceptable systems of control.
Additionally, Australia has undergone significant reviews of the risks of AI to
human rights and within intelligence organisations and has committed to
producing ethics guidelines and frameworks in Security and Defence. Australia
is committed to OECD's values-based principles for the responsible stewardship
of trustworthy AI as well as adopting a set of National AI ethics principles.
While Australia has not adopted an AI governance framework specifically for the
Australian Defence Organisation (ADO); Defence Science and Technology Group
(DSTG) has published 'A Method for Ethical AI in Defence' (MEAID) technical
report which includes a framework and pragmatic tools for managing ethical and
legal risks for military applications of AI. Australia can play a leadership
role by integrating legal and ethical considerations into its ADO AI capability
acquisition process. This requires a policy framework that defines its legal
and ethical requirements, is informed by Defence industry stakeholders, and
provides a practical methodology to integrate legal and ethical risk mitigation
strategies into the acquisition process.

Many important decisions in daily life are made with the help of advisors,
e.g., decisions about medical treatments or financial investments. Whereas in
the past, advice has often been received from human experts, friends, or
family, advisors based on artificial intelligence (AI) have become more and
more present nowadays. Typically, the advice generated by AI is judged by a
human and either deemed reliable or rejected. However, recent work has shown
that AI advice is not always beneficial, as humans have shown to be unable to
ignore incorrect AI advice, essentially representing an over-reliance on AI.
Therefore, the aspired goal should be to enable humans not to rely on AI advice
blindly but rather to distinguish its quality and act upon it to make better
decisions. Specifically, that means that humans should rely on the AI in the
presence of correct advice and self-rely when confronted with incorrect advice,
i.e., establish appropriate reliance (AR) on AI advice on a case-by-case basis.
Current research lacks a metric for AR. This prevents a rigorous evaluation of
factors impacting AR and hinders further development of human-AI
decision-making. Therefore, based on the literature, we derive a measurement
concept of AR. We propose to view AR as a two-dimensional construct that
measures the ability to discriminate advice quality and behave accordingly. In
this article, we derive the measurement concept, illustrate its application and
outline potential future research.

Within the current AI ethics discourse, there is a gap in empirical research
on understanding how AI practitioners understand ethics and socially organize
to operationalize ethical concerns, particularly in the context of AI
start-ups. This gap intensifies the risk of a disconnect between scholarly
research, innovation, and application. This risk materializes acutely as
mounting pressures to identify and mitigate the potential harms of AI systems
have created an urgent need to assess and implement socio-technical innovation
for fairness, accountability, and transparency. Building on social practice
theory, we address this need via a framework that allows AI researchers,
practitioners, and regulators to systematically analyze existing cultural
understandings, histories, and social practices of ethical AI to define
appropriate strategies for effectively implementing socio-technical
innovations. Our contributions are threefold: 1) we introduce a practice-based
approach for understanding ethical AI; 2) we present empirical findings from
our study on the operationalization of ethics in German AI start-ups to
underline that AI ethics and social practices must be understood in their
specific cultural and historical contexts; and 3) based on our empirical
findings, we suggest that ethical AI practices can be broken down into
principles, needs, narratives, materializations, and cultural genealogies to
form a useful backdrop for considering socio-technical innovations.

Artificial Intelligence (AI) solutions and technologies are being
increasingly adopted in smart systems context, however, such technologies are
continuously concerned with ethical uncertainties. Various guidelines,
principles, and regulatory frameworks are designed to ensure that AI
technologies bring ethical well-being. However, the implications of AI ethics
principles and guidelines are still being debated. To further explore the
significance of AI ethics principles and relevant challenges, we conducted a
survey of 99 representative AI practitioners and lawmakers (e.g., AI engineers,
lawyers) from twenty countries across five continents. To the best of our
knowledge, this is the first empirical study that encapsulates the perceptions
of two different types of population (AI practitioners and lawmakers) and the
study findings confirm that transparency, accountability, and privacy are the
most critical AI ethics principles. On the other hand, lack of ethical
knowledge, no legal frameworks, and lacking monitoring bodies are found the
most common AI ethics challenges. The impact analysis of the challenges across
AI ethics principles reveals that conflict in practice is a highly severe
challenge. Moreover, the perceptions of practitioners and lawmakers are
statistically correlated with significant differences for particular principles
(e.g. fairness, freedom) and challenges (e.g. lacking monitoring bodies,
machine distortion). Our findings stimulate further research, especially
empowering existing capability maturity models to support the development and
quality assessment of ethics-aware AI systems.

With increasing digitalization, Artificial Intelligence (AI) is becoming
ubiquitous. AI-based systems to identify, optimize, automate, and scale
solutions to complex economic and societal problems are being proposed and
implemented. This has motivated regulation efforts, including the Proposal of
an EU AI Act. This interdisciplinary position paper considers various concerns
surrounding fairness and discrimination in AI, and discusses how AI regulations
address them, focusing on (but not limited to) the Proposal. We first look at
AI and fairness through the lenses of law, (AI) industry, sociotechnology, and
(moral) philosophy, and present various perspectives. Then, we map these
perspectives along three axes of interests: (i) Standardization vs.
Localization, (ii) Utilitarianism vs. Egalitarianism, and (iii) Consequential
vs. Deontological ethics which leads us to identify a pattern of common
arguments and tensions between these axes. Positioning the discussion within
the axes of interest and with a focus on reconciling the key tensions, we
identify and propose the roles AI Regulation should take to make the endeavor
of the AI Act a success in terms of AI fairness concerns.

Companies' adoption of artificial intelligence (AI) is increasingly becoming
an essential element of business success. However, using AI poses new
requirements for companies and their employees, including transparency and
comprehensibility of AI systems. The field of Explainable AI (XAI) aims to
address these issues. Yet, the current research primarily consists of
laboratory studies, and there is a need to improve the applicability of the
findings to real-world situations. Therefore, this project report paper
provides insights into employees' needs and attitudes towards (X)AI. For this,
we investigate employees' perspectives on (X)AI. Our findings suggest that AI
and XAI are well-known terms perceived as important for employees. This
recognition is a critical first step for XAI to potentially drive successful
usage of AI by providing comprehensible insights into AI technologies. In a
lessons-learned section, we discuss the open questions identified and suggest
future research directions to develop human-centered XAI designs for companies.
By providing insights into employees' needs and attitudes towards (X)AI, our
project report contributes to the development of XAI solutions that meet the
requirements of companies and their employees, ultimately driving the
successful adoption of AI technologies in the business context.

The ability to discern between true and false information is essential to
making sound decisions. However, with the recent increase in AI-based
disinformation campaigns, it has become critical to understand the influence of
deceptive systems on human information processing. In experiment (N=128), we
investigated how susceptible people are to deceptive AI systems by examining
how their ability to discern true news from fake news varies when AI systems
are perceived as either human fact-checkers or AI fact-checking systems, and
when explanations provided by those fact-checkers are either deceptive or
honest. We find that deceitful explanations significantly reduce accuracy,
indicating that people are just as likely to believe deceptive AI explanations
as honest AI explanations. Although before getting assistance from an
AI-system, people have significantly higher weighted discernment accuracy on
false headlines than true headlines, we found that with assistance from an AI
system, discernment accuracy increased significantly when given honest
explanations on both true headlines and false headlines, and decreased
significantly when given deceitful explanations on true headlines and false
headlines. Further, we did not observe any significant differences in
discernment between explanations perceived as coming from a human fact checker
compared to an AI-fact checker. Similarly, we found no significant differences
in trust. These findings exemplify the dangers of deceptive AI systems and the
need for finding novel ways to limit their influence human information
processing.

Scientists and philosophers have debated whether humans can trust advanced
artificial intelligence (AI) agents to respect humanity's best interests. Yet
what about the reverse? Will advanced AI agents trust humans? Gauging an AI
agent's trust in humans is challenging because--absent costs for
dishonesty--such agents might respond falsely about their trust in humans. Here
we present a method for incentivizing machine decisions without altering an AI
agent's underlying algorithms or goal orientation. In two separate experiments,
we then employ this method in hundreds of trust games between an AI agent (a
Large Language Model (LLM) from OpenAI) and a human experimenter (author TJ).
In our first experiment, we find that the AI agent decides to trust humans at
higher rates when facing actual incentives than when making hypothetical
decisions. Our second experiment replicates and extends these findings by
automating game play and by homogenizing question wording. We again observe
higher rates of trust when the AI agent faces real incentives. Across both
experiments, the AI agent's trust decisions appear unrelated to the magnitude
of stakes. Furthermore, to address the possibility that the AI agent's trust
decisions reflect a preference for uncertainty, the experiments include two
conditions that present the AI agent with a non-social decision task that
provides the opportunity to choose a certain or uncertain option; in those
conditions, the AI agent consistently chooses the certain option. Our
experiments suggest that one of the most advanced AI language models to date
alters its social behavior in response to incentives and displays behavior
consistent with trust toward a human interlocutor when incentivized.

Persuasion is a key aspect of what it means to be human, and is central to
business, politics, and other endeavors. Advancements in artificial
intelligence (AI) have produced AI systems that are capable of persuading
humans to buy products, watch videos, click on search results, and more. Even
systems that are not explicitly designed to persuade may do so in practice. In
the future, increasingly anthropomorphic AI systems may form ongoing
relationships with users, increasing their persuasive power. This paper
investigates the uncertain future of persuasive AI systems. We examine ways
that AI could qualitatively alter our relationship to and views regarding
persuasion by shifting the balance of persuasive power, allowing personalized
persuasion to be deployed at scale, powering misinformation campaigns, and
changing the way humans can shape their own discourse. We consider ways
AI-driven persuasion could differ from human-driven persuasion. We warn that
ubiquitous highlypersuasive AI systems could alter our information environment
so significantly so as to contribute to a loss of human control of our own
future. In response, we examine several potential responses to AI-driven
persuasion: prohibition, identification of AI agents, truthful AI, and legal
remedies. We conclude that none of these solutions will be airtight, and that
individuals and governments will need to take active steps to guard against the
most pernicious effects of persuasive AI.

The significant advancements in applying Artificial Intelligence (AI) to
healthcare decision-making, medical diagnosis, and other domains have
simultaneously raised concerns about the fairness and bias of AI systems. This
is particularly critical in areas like healthcare, employment, criminal
justice, credit scoring, and increasingly, in generative AI models (GenAI) that
produce synthetic media. Such systems can lead to unfair outcomes and
perpetuate existing inequalities, including generative biases that affect the
representation of individuals in synthetic data. This survey paper offers a
succinct, comprehensive overview of fairness and bias in AI, addressing their
sources, impacts, and mitigation strategies. We review sources of bias, such as
data, algorithm, and human decision biases - highlighting the emergent issue of
generative AI bias where models may reproduce and amplify societal stereotypes.
We assess the societal impact of biased AI systems, focusing on the
perpetuation of inequalities and the reinforcement of harmful stereotypes,
especially as generative AI becomes more prevalent in creating content that
influences public perception. We explore various proposed mitigation
strategies, discussing the ethical considerations of their implementation and
emphasizing the need for interdisciplinary collaboration to ensure
effectiveness. Through a systematic literature review spanning multiple
academic disciplines, we present definitions of AI bias and its different
types, including a detailed look at generative AI bias. We discuss the negative
impacts of AI bias on individuals and society and provide an overview of
current approaches to mitigate AI bias, including data pre-processing, model
selection, and post-processing. We emphasize the unique challenges presented by
generative AI models and the importance of strategies specifically tailored to
address these.

The recent advances of AI technology, particularly in AI-Generated Content
(AIGC), have enabled everyone to easily generate beautiful paintings with
simple text description. With the stunning quality of AI paintings, it is
widely questioned whether there still exists difference between human and AI
paintings and whether human artists will be replaced by AI. To answer these
questions, we develop a computational framework combining neural latent space
and aesthetics features with visual analytics to investigate the difference
between human and AI paintings. First, with categorical comparison of human and
AI painting collections, we find that AI artworks show distributional
difference from human artworks in both latent space and some aesthetic features
like strokes and sharpness, while in other aesthetic features like color and
composition there is less difference. Second, with individual artist analysis
of Picasso, we show human artists' strength in evolving new styles compared to
AI. Our findings provide concrete evidence for the existing discrepancies
between human and AI paintings and further suggest improvements of AI art with
more consideration of aesthetics and human artists' involvement.

Over the past half century, there have been several false dawns during which
the "arrival" of world-changing artificial intelligence (AI) has been heralded.
Tempting fate, the authors believe the age of AI has, indeed, finally arrived.
Powerful image generators, such as DALL-E2 and Midjourney have suddenly allowed
anyone with access the ability easily to create rich and complex art. In a
similar vein, text generators, such as GPT3.5 (including ChatGPT) and BLOOM,
allow users to compose detailed written descriptions of many topics of
interest. And, it is even possible now for a person without extensive expertise
in writing software to use AI to generate code capable of myriad applications.
While AI will continue to evolve and improve, probably at a rapid rate, the
current state of AI is already ushering in profound changes to many different
sectors of society. Every new technology challenges the ability of humanity to
govern it wisely. However, governance is usually viewed as both possible and
necessary due to the disruption new technology often poses to social
structures, industries, the environment, and other important human concerns. In
this article, we offer an analysis of a range of interactions between AI and
governance, with the hope that wise decisions may be made that maximize
benefits and minimize costs. The article addresses two main aspects of this
relationship: the governance of AI by humanity, and the governance of humanity
by AI. The approach we have taken is itself informed by AI, as this article was
written collaboratively by the authors and ChatGPT.

Despite its successes, to date Artificial Intelligence (AI) is still
characterized by a number of shortcomings with regards to different application
domains and goals. These limitations are arguably both conceptual (e.g.,
related to underlying theoretical models, such as symbolic vs. connectionist),
and operational (e.g., related to robustness and ability to generalize).
Biologically inspired AI, and more specifically brain-inspired AI, promises to
provide further biological aspects beyond those that are already traditionally
included in AI, making it possible to assess and possibly overcome some of its
present shortcomings. This article examines some conceptual, technical, and
ethical issues raised by the development and use of brain-inspired AI. Against
this background, the paper asks whether there is anything ethically unique
about brain-inspired AI. The aim of the paper is to introduce a method that has
a heuristic nature and that can be applied to identify and address the ethical
issues arising from brain-inspired AI. The conclusion resulting from the
application of this method is that, compared to traditional AI, brain-inspired
AI raises new foundational ethical issues and some new practical ethical
issues, and exacerbates some of the issues raised by traditional AI.

As the development and use of artificial intelligence (AI) continues to grow,
policymakers are increasingly grappling with the question of how to regulate
this technology. The most far-reaching international initiative is the European
Union (EU) AI Act, which aims to establish the first comprehensive, binding
framework for regulating AI. In this article, we offer the first systematic
analysis of non-state actor preferences toward international regulation of AI,
focusing on the case of the EU AI Act. Theoretically, we develop an argument
about the regulatory preferences of business actors and other non-state actors
under varying conditions of AI sector competitiveness. Empirically, we test
these expectations using data from public consultations on European AI
regulation. Our findings are threefold. First, all types of non-state actors
express concerns about AI and support regulation in some form. Second, there
are nonetheless significant differences across actor types, with business
actors being less concerned about the downsides of AI and more in favor of lax
regulation than other non-state actors. Third, these differences are more
pronounced in countries with stronger commercial AI sectors. Our findings shed
new light on non-state actor preferences toward AI regulation and point to
challenges for policymakers balancing competing interests in society.

Analysing historical patterns of artificial intelligence (AI) adoption can
inform decisions about AI capability uplift, but research to date has provided
a limited view of AI adoption across various fields of research. In this study
we examine worldwide adoption of AI technology within 333 fields of research
during 1960-2021. We do this by using bibliometric analysis with 137 million
peer-reviewed publications captured in The Lens database. We define AI using a
list of 214 phrases developed by expert working groups at the Organisation for
Economic Cooperation and Development (OECD). We found that 3.1 million of the
137 million peer-reviewed research publications during the entire period were
AI-related, with a surge in AI adoption across practically all research fields
(physical science, natural science, life science, social science and the arts
and humanities) in recent years. The diffusion of AI beyond computer science
was early, rapid and widespread. In 1960 14% of 333 research fields were
related to AI (many in computer science), but this increased to cover over half
of all research fields by 1972, over 80% by 1986 and over 98% in current times.
We note AI has experienced boom-bust cycles historically: the AI "springs" and
"winters". We conclude that the context of the current surge appears different,
and that interdisciplinary AI application is likely to be sustained.

Uses of artificial intelligence (AI), especially those powered by machine
learning approaches, are growing in sectors and societies around the world. How
will AI adoption proceed, especially in the international security realm?
Research on automation bias suggests that humans can often be overconfident in
AI, whereas research on algorithm aversion shows that, as the stakes of a
decision rise, humans become more cautious about trusting algorithms. We
theorize about the relationship between background knowledge about AI, trust in
AI, and how these interact with other factors to influence the probability of
automation bias in the international security context. We test these in a
preregistered task identification experiment across a representative sample of
9000 adults in 9 countries with varying levels of AI industries. The results
strongly support the theory, especially concerning AI background knowledge. A
version of the Dunning Kruger effect appears to be at play, whereby those with
the lowest level of experience with AI are slightly more likely to be
algorithm-averse, then automation bias occurs at lower levels of knowledge
before leveling off as a respondent's AI background reaches the highest levels.
Additional results show effects from the task's difficulty, overall AI trust,
and whether a human or AI decision aid is described as highly competent or less
competent.

Current progress in the artificial intelligence domain has led to the
development of various types of AI-powered dementia assessments, which can be
employed to identify patients at the early stage of dementia. It can
revolutionize the dementia care settings. It is essential that the medical
community be aware of various AI assessments and choose them considering their
degrees of validity, efficiency, practicality, reliability, and accuracy
concerning the early identification of patients with dementia (PwD). On the
other hand, AI developers should be informed about various non-AI assessments
as well as recently developed AI assessments. Thus, this paper, which can be
readable by both clinicians and AI engineers, fills the gap in the literature
in explaining the existing solutions for the recognition of dementia to
clinicians, as well as the techniques used and the most widespread dementia
datasets to AI engineers. It follows a review of papers on AI and non-AI
assessments for dementia to provide valuable information about various dementia
assessments for both the AI and medical communities. The discussion and
conclusion highlight the most prominent research directions and the maturity of
existing solutions.

Explainability techniques are rapidly being developed to improve human-AI
decision-making across various cooperative work settings. Consequently,
previous research has evaluated how decision-makers collaborate with imperfect
AI by investigating appropriate reliance and task performance with the aim of
designing more human-centered computer-supported collaborative tools. Several
human-centered explainable AI (XAI) techniques have been proposed in hopes of
improving decision-makers' collaboration with AI; however, these techniques are
grounded in findings from previous studies that primarily focus on the impact
of incorrect AI advice. Few studies acknowledge the possibility of the
explanations being incorrect even if the AI advice is correct. Thus, it is
crucial to understand how imperfect XAI affects human-AI decision-making. In
this work, we contribute a robust, mixed-methods user study with 136
participants to evaluate how incorrect explanations influence humans'
decision-making behavior in a bird species identification task, taking into
account their level of expertise and an explanation's level of assertiveness.
Our findings reveal the influence of imperfect XAI and humans' level of
expertise on their reliance on AI and human-AI team performance. We also
discuss how explanations can deceive decision-makers during human-AI
collaboration. Hence, we shed light on the impacts of imperfect XAI in the
field of computer-supported cooperative work and provide guidelines for
designers of human-AI collaboration systems.

A growing body of research has explored how to support humans in making
better use of AI-based decision support, including via training and onboarding.
Existing research has focused on decision-making tasks where it is possible to
evaluate "appropriate reliance" by comparing each decision against a ground
truth label that cleanly maps to both the AI's predictive target and the human
decision-maker's goals. However, this assumption does not hold in many
real-world settings where AI tools are deployed today (e.g., social work,
criminal justice, and healthcare). In this paper, we introduce a
process-oriented notion of appropriate reliance called critical use that
centers the human's ability to situate AI predictions against knowledge that is
uniquely available to them but unavailable to the AI model. To explore how
training can support critical use, we conduct a randomized online experiment in
a complex social decision-making setting: child maltreatment screening. We find
that, by providing participants with accelerated, low-stakes opportunities to
practice AI-assisted decision-making in this setting, novices came to exhibit
patterns of disagreement with AI that resemble those of experienced workers. A
qualitative examination of participants' explanations for their AI-assisted
decisions revealed that they drew upon qualitative case narratives, to which
the AI model did not have access, to learn when (not) to rely on AI
predictions. Our findings open new questions for the study and design of
training for real-world AI-assisted decision-making.

This report examines Artificial Intelligence (AI) in the financial sector,
outlining its potential to revolutionise the industry and identify its
challenges. It underscores the criticality of a well-rounded understanding of
AI, its capabilities, and its implications to effectively leverage its
potential while mitigating associated risks. The potential of AI potential
extends from augmenting existing operations to paving the way for novel
applications in the finance sector. The application of AI in the financial
sector is transforming the industry. Its use spans areas from customer service
enhancements, fraud detection, and risk management to credit assessments and
high-frequency trading. However, along with these benefits, AI also presents
several challenges. These include issues related to transparency,
interpretability, fairness, accountability, and trustworthiness. The use of AI
in the financial sector further raises critical questions about data privacy
and security. A further issue identified in this report is the systemic risk
that AI can introduce to the financial sector. Being prone to errors, AI can
exacerbate existing systemic risks, potentially leading to financial crises.
Regulation is crucial to harnessing the benefits of AI while mitigating its
potential risks. Despite the global recognition of this need, there remains a
lack of clear guidelines or legislation for AI use in finance. This report
discusses key principles that could guide the formation of effective AI
regulation in the financial sector, including the need for a risk-based
approach, the inclusion of ethical considerations, and the importance of
maintaining a balance between innovation and consumer protection. The report
provides recommendations for academia, the finance industry, and regulators.

Although artificial intelligence (AI) has achieved many feats at a rapid
pace, there still exist open problems and fundamental shortcomings related to
performance and resource efficiency. Since AI researchers benchmark a
significant proportion of performance standards through human intelligence,
cognitive sciences-inspired AI is a promising domain of research. Studying
cognitive science can provide a fresh perspective to building fundamental
blocks in AI research, which can lead to improved performance and efficiency.
In this review paper, we focus on the cognitive functions of perception, which
is the process of taking signals from one's surroundings as input, and
processing them to understand the environment. Particularly, we study and
compare its various processes through the lens of both cognitive sciences and
AI. Through this study, we review all current major theories from various
sub-disciplines of cognitive science (specifically neuroscience, psychology and
linguistics), and draw parallels with theories and techniques from current
practices in AI. We, hence, present a detailed collection of methods in AI for
researchers to build AI systems inspired by cognitive science. Further, through
the process of reviewing the state of cognitive-inspired AI, we point out many
gaps in the current state of AI (with respect to the performance of the human
brain), and hence present potential directions for researchers to develop
better perception systems in AI.

This position paper explores the broad landscape of AI potentiality in the
context of cybersecurity, with a particular emphasis on its possible risk
factors with awareness, which can be managed by incorporating human experts in
the loop, i.e., "Human-AI" teaming. As artificial intelligence (AI)
technologies advance, they will provide unparalleled opportunities for attack
identification, incident response, and recovery. However, the successful
deployment of AI into cybersecurity measures necessitates an in-depth
understanding of its capabilities, challenges, and ethical and legal
implications to handle associated risk factors in real-world application areas.
Towards this, we emphasize the importance of a balanced approach that
incorporates AI's computational power with human expertise. AI systems may
proactively discover vulnerabilities and detect anomalies through pattern
recognition, and predictive modeling, significantly enhancing speed and
accuracy. Human experts can explain AI-generated decisions to stakeholders,
regulators, and end-users in critical situations, ensuring responsibility and
accountability, which helps establish trust in AI-driven security solutions.
Therefore, in this position paper, we argue that human-AI teaming is worthwhile
in cybersecurity, in which human expertise such as intuition, critical
thinking, or contextual understanding is combined with AI's computational power
to improve overall cyber defenses.

All types of research, development, and policy work can have unintended,
adverse consequences - work in responsible artificial intelligence (RAI),
ethical AI, or ethics in AI is no exception.

AI's impact has traditionally been assessed in terms of occupations. However,
an occupation is comprised of interconnected tasks, and it is these tasks, not
occupations themselves, that are affected by AI. To evaluate how tasks may be
impacted, previous approaches utilized manual annotations or coarse-grained
matching. Leveraging recent advancements in machine learning, we replace
coarse-grained matching with more precise deep learning approaches. Introducing
the AI Impact (AII) measure, we employ Deep Learning Natural Language
Processing to automatically identify AI patents that impact various
occupational tasks at scale. Our methodology relies on a comprehensive dataset
of 19,498 task descriptions and quantifies AI's impact through analysis of
12,984 AI patents filed with the United States Patent and Trademark Office
(USPTO) between 2015 and 2020. Our observations reveal that the impact of AI on
occupations defies simplistic categorizations based on task complexity,
challenging the conventional belief that the dichotomy between basic and
advanced skills alone explains the effects of AI. Instead, the impact is
intricately linked to specific skills, whether basic or advanced, associated
with particular tasks. For instance, while basic skills like scanning items may
be affected, others like cooking may not. Similarly, certain advanced skills,
such as image analysis in radiology, may face impact, while skills involving
interpersonal relationships may remain unaffected. Furthermore, the influence
of AI extends beyond knowledge-centric regions. Regions in the U.S. that
heavily rely on industries susceptible to AI changes, often characterized by
economic inequality or a lack of economic diversification, will experience
notable AI impact.

The conventional discourse on existential risks (x-risks) from AI typically
focuses on abrupt, dire events caused by advanced AI systems, particularly
those that might achieve or surpass human-level intelligence. These events have
severe consequences that either lead to human extinction or irreversibly
cripple human civilization to a point beyond recovery. This discourse, however,
often neglects the serious possibility of AI x-risks manifesting incrementally
through a series of smaller yet interconnected disruptions, gradually crossing
critical thresholds over time. This paper contrasts the conventional "decisive
AI x-risk hypothesis" with an "accumulative AI x-risk hypothesis." While the
former envisions an overt AI takeover pathway, characterized by scenarios like
uncontrollable superintelligence, the latter suggests a different causal
pathway to existential catastrophes. This involves a gradual accumulation of
critical AI-induced threats such as severe vulnerabilities and systemic erosion
of econopolitical structures. The accumulative hypothesis suggests a boiling
frog scenario where incremental AI risks slowly converge, undermining
resilience until a triggering event results in irreversible collapse. Through
systems analysis, this paper examines the distinct assumptions differentiating
these two hypotheses. It is then argued that the accumulative view reconciles
seemingly incompatible perspectives on AI risks. The implications of
differentiating between these causal pathways -- the decisive and the
accumulative -- for the governance of AI risks as well as long-term AI safety
are discussed.

Public sector agencies are rapidly deploying AI systems to augment or
automate critical decisions in real-world contexts like child welfare, criminal
justice, and public health. A growing body of work documents how these AI
systems often fail to improve services in practice. These failures can often be
traced to decisions made during the early stages of AI ideation and design,
such as problem formulation. However, today, we lack systematic processes to
support effective, early-stage decision-making about whether and under what
conditions to move forward with a proposed AI project. To understand how to
scaffold such processes in real-world settings, we worked with public sector
agency leaders, AI developers, frontline workers, and community advocates
across four public sector agencies and three community advocacy groups in the
United States. Through an iterative co-design process, we created the Situate
AI Guidebook: a structured process centered around a set of deliberation
questions to scaffold conversations around (1) goals and intended use or a
proposed AI system, (2) societal and legal considerations, (3) data and
modeling constraints, and (4) organizational governance factors. We discuss how
the guidebook's design is informed by participants' challenges, needs, and
desires for improved deliberation processes. We further elaborate on
implications for designing responsible AI toolkits in collaboration with public
sector agency stakeholders and opportunities for future work to expand upon the
guidebook. This design approach can be more broadly adopted to support the
co-creation of responsible AI toolkits that scaffold key decision-making
processes surrounding the use of AI in the public sector and beyond.

AI approaches are progressing besting humans at game-related tasks (e.g.
chess). The next stage is expected to be Human-AI collaboration; however, the
research on this subject has been mixed and is in need of additional data
points. We add to this nascent literature by studying Human-AI collaboration on
a common administrative educational task. Education is a special domain in its
relation to AI and has been slow to adopt AI approaches in practice, concerned
with the educational enterprise losing its humanistic touch and because
standard of quality is demanded because of the impact on a person's career and
developmental trajectory. In this study (N = 22), we design an experiment to
explore the effect of Human-AI collaboration on the task of tagging educational
content with skills from the US common core taxonomy. Our results show that the
experiment group (with AI recommendations) saved around 50% time (p < 0.01) in
the execution of their tagging task but at the sacrifice of 7.7% recall (p =
0.267) and 35% accuracy (p= 0.1170) compared with the non-AI involved control
group, placing the AI+human group in between the AI alone (lowest performance)
and the human alone (highest performance). We further analyze log data from
this AI collaboration experiment to explore under what circumstances humans
still exercised their discernment when receiving recommendations. Finally, we
outline how this study can assist in implementing AI tools, like ChatGPT, in
education.

As Artificial Intelligence (AI) techniques have become more powerful and
easier to use they are increasingly deployed as key components of modern
software systems. While this enables new functionality and often allows better
adaptation to user needs it also creates additional problems for software
engineers and exposes companies to new risks. Some work has been done to better
understand the interaction between Software Engineering and AI but we lack
methods to classify ways of applying AI in software systems and to analyse and
understand the risks this poses. Only by doing so can we devise tools and
solutions to help mitigate them. This paper presents the AI in SE Application
Levels (AI-SEAL) taxonomy that categorises applications according to their
point of AI application, the type of AI technology used and the automation
level allowed. We show the usefulness of this taxonomy by classifying 15 papers
from previous editions of the RAISE workshop. Results show that the taxonomy
allows classification of distinct AI applications and provides insights
concerning the risks associated with them. We argue that this will be important
for companies in deciding how to apply AI in their software applications and to
create strategies for its use.

Recently, a lot of attention has been given to undesired consequences of
Artificial Intelligence (AI), such as unfair bias leading to discrimination, or
the lack of explanations of the results of AI systems. There are several
important questions to answer before AI can be deployed at scale in our
businesses and societies. Most of these issues are being discussed by experts
and the wider communities, and it seems there is broad consensus on where they
come from. There is, however, less consensus on, and experience with how to
practically deal with those issues in organizations that develop and use AI,
both from a technical and organizational perspective. In this paper, we discuss
the practical case of a large organization that is putting in place a
company-wide methodology to minimize the risk of undesired consequences of AI.
We hope that other organizations can learn from this and that our experience
contributes to making the best of AI while minimizing its risks.

As AI becomes integrated throughout the world, its potential for impact
within low-resource regions around the Global South have grown. AI research
labs from tech giants like Microsoft, Google, and IBM have a significant
presence in countries such as India, Ghana, and South Africa. The work done by
these labs is often motivated by the potential impact it could have on local
populations, but the deployment of these tools has not always gone smoothly.
This paper presents a case study examining the deployment of AI by large
industry labs situated in low-resource contexts, highlights factors impacting
unanticipated deployments, and reflects on the state of AI deployment within
the Global South, providing suggestions that embrace inclusive design
methodologies within AI development that prioritize the needs of marginalized
communities and elevate their status not just as beneficiaries of AI systems
but as primary stakeholders.

The young field of AI Safety is still in the process of identifying its
challenges and limitations. In this paper, we formally describe one such
impossibility result, namely Unpredictability of AI. We prove that it is
impossible to precisely and consistently predict what specific actions a
smarter-than-human intelligent system will take to achieve its objectives, even
if we know terminal goals of the system. In conclusion, impact of
Unpredictability on AI Safety is discussed.

Despite the promises of data-driven artificial intelligence (AI), little is
known about how we can bridge the gulf between traditional physician-driven
diagnosis and a plausible future of medicine automated by AI. Specifically, how
can we involve AI usefully in physicians' diagnosis workflow given that most AI
is still nascent and error-prone (e.g., in digital pathology)? To explore this
question, we first propose a series of collaborative techniques to engage human
pathologists with AI given AI's capabilities and limitations, based on which we
prototype Impetus - a tool where an AI takes various degrees of initiatives to
provide various forms of assistance to a pathologist in detecting tumors from
histological slides. We summarize observations and lessons learned from a study
with eight pathologists and discuss recommendations for future work on
human-centered medical AI systems.

Artificial intelligence (AI) and machine learning (ML) have become
increasingly vital in the development of novel defense and intelligence
capabilities across all domains of warfare. An adversarial AI (A2I) and
adversarial ML (AML) attack seeks to deceive and manipulate AI/ML models. It is
imperative that AI/ML models can defend against these attacks. A2I/AML defenses
will help provide the necessary assurance of these advanced capabilities that
use AI/ML models. The A2I Working Group (A2IWG) seeks to advance the research
and development of assured AI/ML capabilities via new A2I/AML defenses by
fostering a collaborative environment across the U.S. Department of Defense and
U.S. Intelligence Community. The A2IWG aims to identify specific challenges
that it can help solve or address more directly, with initial focus on three
topics: AI Trusted Robustness, AI System Security, and AI/ML Architecture
Vulnerabilities.

The number and importance of AI-based systems in all domains is growing. With
the pervasive use and the dependence on AI-based systems, the quality of these
systems becomes essential for their practical usage. However, quality assurance
for AI-based systems is an emerging area that has not been well explored and
requires collaboration between the SE and AI research communities. This paper
discusses terminology and challenges on quality assurance for AI-based systems
to set a baseline for that purpose. Therefore, we define basic concepts and
characterize AI-based systems along the three dimensions of artifact type,
process, and quality characteristics. Furthermore, we elaborate on the key
challenges of (1) understandability and interpretability of AI models, (2) lack
of specifications and defined requirements, (3) need for validation data and
test input generation, (4) defining expected outcomes as test oracles, (5)
accuracy and correctness measures, (6) non-functional properties of AI-based
systems, (7) self-adaptive and self-learning characteristics, and (8) dynamic
and frequently changing environments.

AI Ethics is now a global topic of discussion in academic and policy circles.
At least 84 public-private initiatives have produced statements describing
high-level principles, values, and other tenets to guide the ethical
development, deployment, and governance of AI. According to recent
meta-analyses, AI Ethics has seemingly converged on a set of principles that
closely resemble the four classic principles of medical ethics. Despite the
initial credibility granted to a principled approach to AI Ethics by the
connection to principles in medical ethics, there are reasons to be concerned
about its future impact on AI development and governance. Significant
differences exist between medicine and AI development that suggest a principled
approach in the latter may not enjoy success comparable to the former. Compared
to medicine, AI development lacks (1) common aims and fiduciary duties, (2)
professional history and norms, (3) proven methods to translate principles into
practice, and (4) robust legal and professional accountability mechanisms.
These differences suggest we should not yet celebrate consensus around
high-level principles that hide deep political and normative disagreement.

Solidarity is one of the fundamental values at the heart of the construction
of peaceful societies and present in more than one third of world's
constitutions. Still, solidarity is almost never included as a principle in
ethical guidelines for the development of AI. Solidarity as an AI principle (1)
shares the prosperity created by AI, implementing mechanisms to redistribute
the augmentation of productivity for all; and shares the burdens, making sure
that AI does not increase inequality and no human is left behind. Solidarity as
an AI principle (2) assesses the long term implications before developing and
deploying AI systems so no groups of humans become irrelevant because of AI
systems. Considering solidarity as a core principle for AI development will
provide not just an human-centric but a more humanity-centric approach to AI.

As Artificial Intelligence (AI) technology gets more intertwined with every
system, people are using AI to make decisions on their everyday activities. In
simple contexts, such as Netflix recommendations, or in more complex context
like in judicial scenarios, AI is part of people's decisions. People make
decisions and usually, they need to explain their decision to others or in some
matter. It is particularly critical in contexts where human expertise is
central to decision-making. In order to explain their decisions with AI
support, people need to understand how AI is part of that decision. When
considering the aspect of fairness, the role that AI has on a decision-making
process becomes even more sensitive since it affects the fairness and the
responsibility of those people making the ultimate decision. We have been
exploring an evidence-based explanation design approach to 'tell the story of a
decision'. In this position paper, we discuss our approach for AI systems using
fairness sensitive cases in the literature.

With increasing ubiquity of artificial intelligence (AI) in modern societies,
individual countries and the international community are working hard to create
an innovation-friendly, yet safe, regulatory environment. Adequate regulation
is key to maximize the benefits and minimize the risks stemming from AI
technologies. Developing regulatory frameworks is, however, challenging due to
AI's global reach and the existence of widespread misconceptions about the
notion of regulation. We argue that AI-related challenges cannot be tackled
effectively without sincere international coordination supported by robust,
consistent domestic and international governance arrangements. Against this
backdrop, we propose the establishment of an international AI governance
framework organized around a new AI regulatory agency that -- drawing on
interdisciplinary expertise -- could help creating uniform standards for the
regulation of AI technologies and inform the development of AI policies around
the world. We also believe that a fundamental change of mindset on what
constitutes regulation is necessary to remove existing barriers that hamper
contemporary efforts to develop AI regulatory regimes, and put forward some
recommendations on how to achieve this, and what opportunities doing so would
present.

Neural nets, one of the oldest architectures for AI programming, are loosely
based on biological neurons and their properties. Recent work on language
applications has made the AI code closer to biological reality in several ways.
This commentary examines this convergence and, in light of what is known of
neocortical structure, addresses the question of whether ``general AI'' looks
attainable with these tools.

Several seminal ethics initiatives have stipulated sets of principles and
standards for good technology development in the AI sector. However, widespread
criticism has pointed out a lack of practical realization of these principles.
Following that, AI ethics underwent a practical turn, but without deviating
from the principled approach and the many shortcomings associated with it. This
paper proposes a different approach. It defines four basic AI virtues, namely
justice, honesty, responsibility and care, all of which represent specific
motivational settings that constitute the very precondition for ethical
decision making in the AI field. Moreover, it defines two second-order AI
virtues, prudence and fortitude, that bolster achieving the basic virtues by
helping with overcoming bounded ethicality or the many hidden psychological
forces that impair ethical decision making and that are hitherto disregarded in
AI ethics. Lastly, the paper describes measures for successfully cultivating
the mentioned virtues in organizations dealing with AI research and
development.

The development of AI applications is a multidisciplinary effort, involving
multiple roles collaborating with the AI developers, an umbrella term we use to
include data scientists and other AI-adjacent roles on the same team. During
these collaborations, there is a knowledge mismatch between AI developers, who
are skilled in data science, and external stakeholders who are typically not.
This difference leads to communication gaps, and the onus falls on AI
developers to explain data science concepts to their collaborators. In this
paper, we report on a study including analyses of both interviews with AI
developers and artifacts they produced for communication. Using the analytic
lens of shared mental models, we report on the types of communication gaps that
AI developers face, how AI developers communicate across disciplinary and
organizational boundaries, and how they simultaneously manage issues regarding
trust and expectations.

Over the past three years we have built a practice-oriented, bachelor level,
educational programme for software engineers to specialize as AI engineers. The
experience with this programme and the practical assignments our students
execute in industry has given us valuable insights on the profession of AI
engineer. In this paper we discuss our programme and the lessons learned for
industry and research.

Along with the development of modern computing technology and social
sciences, both theoretical research and practical applications of social
computing have been continuously extended. In particular with the boom of
artificial intelligence (AI), social computing is significantly influenced by
AI. However, the conventional technologies of AI have drawbacks in dealing with
more complicated and dynamic problems. Such deficiency can be rectified by
hybrid human-artificial intelligence (H-AI) which integrates both human
intelligence and AI into one unity, forming a new enhanced intelligence. H-AI
in dealing with social problems shows the advantages that AI can not surpass.
This paper firstly introduces the concept of H-AI. AI is the intelligence in
the transition stage of H-AI, so the latest research progresses of AI in social
computing are reviewed. Secondly, it summarizes typical challenges faced by AI
in social computing, and makes it possible to introduce H-AI to solve these
challenges. Finally, the paper proposes a holistic framework of social
computing combining with H-AI, which consists of four layers: object layer,
base layer, analysis layer, and application layer. It represents H-AI has
significant advantages over AI in solving social problems.

In this paper, we demonstrate the design of efficient and high-performance
AI/Deep Learning accelerators with customized STT-MRAM and a reconfigurable
core. Based on model-driven detailed design space exploration, we present the
design methodology of an innovative scratchpad-assisted on-chip STT-MRAM based
buffer system for high-performance accelerators. Using analytically derived
expression of memory occupancy time of AI model weights and activation maps,
the volatility of STT-MRAM is adjusted with process and temperature variation
aware scaling of thermal stability factor to optimize the retention time,
energy, read/write latency, and area of STT-MRAM. From the analysis of modern
AI workloads and accelerator implementation in 14nm technology, we verify the
efficacy of our designed AI accelerator with STT-MRAM STT-AI. Compared to an
SRAM-based implementation, the STT-AI accelerator achieves 75% area and 3%
power savings at iso-accuracy. Furthermore, with a relaxed bit error rate and
negligible AI accuracy trade-off, the designed STT-AI Ultra accelerator
achieves 75.4%, and 3.5% savings in area and power, respectively over regular
SRAM-based accelerators.

In this work, we survey skepticism regarding AI risk and show parallels with
other types of scientific skepticism. We start by classifying different types
of AI Risk skepticism and analyze their root causes. We conclude by suggesting
some intervention approaches, which may be successful in reducing AI risk
skepticism, at least amongst artificial intelligence researchers.

What we expect from radiology AI algorithms will shape the selection and
implementation of AI in the radiologic practice. In this paper I consider
prevailing expectations of AI and compare them to expectations that we have of
human readers. I observe that the expectations from AI and radiologists are
fundamentally different. The expectations of AI are based on a strong and
justified mistrust about the way that AI makes decisions. Because AI decisions
are not well understood, it is difficult to know how the algorithms will behave
in new, unexpected situations. However, this mistrust is not mirrored in our
expectations of human readers. Despite well-proven idiosyncrasies and biases in
human decision making, we take comfort from the assumption that others make
decisions in a way as we do, and we trust our own decision making. Despite poor
ability to explain decision making processes in humans, we accept explanations
of decisions given by other humans. Because the goal of radiology is the most
accurate radiologic interpretation, our expectations of radiologists and AI
should be similar, and both should reflect a healthy mistrust of complicated
and partially opaque decision processes undergoing in computer algorithms and
human brains. This is generally not the case now.

As artificial intelligence (AI) systems are increasingly deployed, principles
for ethical AI are also proliferating. Certification offers a method to both
incentivize adoption of these principles and substantiate that they have been
implemented in practice. This paper draws from management literature on
certification and reviews current AI certification programs and proposals.
Successful programs rely on both emerging technical methods and specific design
considerations. In order to avoid two common failures of certification, program
designs should ensure that the symbol of the certification is substantially
implemented in practice and that the program achieves its stated goals. The
review indicates that the field currently focuses on self-certification and
third-party certification of systems, individuals, and organizations - to the
exclusion of process management certifications. Additionally, the paper
considers prospects for future AI certification programs. Ongoing changes in AI
technology suggest that AI certification regimes should be designed to
emphasize governance criteria of enduring value, such as ethics training for AI
developers, and to adjust technical criteria as the technology changes.
Overall, certification can play a valuable mix in the portfolio of AI
governance tools.

In recent years, there has been an increased emphasis on understanding and
mitigating adverse impacts of artificial intelligence (AI) technologies on
society. Across academia, industry, and government bodies, a variety of
endeavours are being pursued towards enhancing AI ethics. A significant
challenge in the design of ethical AI systems is that there are multiple
stakeholders in the AI pipeline, each with their own set of constraints and
interests. These different perspectives are often not understood, due in part
to communication gaps.For example, AI researchers who design and develop AI
models are not necessarily aware of the instability induced in consumers' lives
by the compounded effects of AI decisions. Educating different stakeholders
about their roles and responsibilities in the broader context becomes
necessary. In this position paper, we outline some potential ways in which
generative artworks can play this role by serving as accessible and powerful
educational tools for surfacing different perspectives. We hope to spark
interdisciplinary discussions about computational creativity broadly as a tool
for enhancing AI ethics.

The received wisdom is that artificial intelligence (AI) is a competition
between the US and China. In this chapter, the author will examine how the
European Union (EU) fits into that mix and what it can offer as a third way to
govern AI. The chapter presents this by exploring the past, present and future
of AI governance in the EU. Section 1 serves to explore and evidence the EUs
coherent and comprehensive approach to AI governance. In short, the EU ensures
and encourages ethical, trustworthy and reliable technological development.
This will cover a range of key documents and policy tools that lead to the most
crucial effort of the EU to date: to regulate AI. Section 2 maps the EUs drive
towards digital sovereignty through the lens of regulation and infrastructure.
This covers topics such as the trustworthiness of AI systems, cloud, compute
and foreign direct investment. In Section 3, the chapter concludes by offering
several considerations to achieve good AI governance in the EU.

Artificial intelligence (AI) systems operate in increasingly diverse areas,
from healthcare to facial recognition, the stock market, autonomous vehicles,
and so on. While the underlying digital infrastructure of AI systems is
developing rapidly, each area of implementation is subject to different degrees
and processes of legitimization. By combining elements from institutional
theory and information systems-theory, this paper presents a conceptual
framework to analyze and understand AI-induced field-change. The introduction
of novel AI-agents into new or existing fields creates a dynamic in which
algorithms (re)shape organizations and institutions while existing
institutional infrastructures determine the scope and speed at which
organizational change is allowed to occur. Where institutional infrastructure
and governance arrangements, such as standards, rules, and regulations, still
are unelaborate, the field can move fast but is also more likely to be
contested. The institutional infrastructure surrounding AI-induced fields is
generally little elaborated, which could be an obstacle to the broader
institutionalization of AI-systems going forward.

Drawing on our experience of more than a decade of AI in academic research,
technology development, industry engagement, postgraduate teaching, doctoral
supervision and organisational consultancy, we present the 'CDAC AI Life
Cycle', a comprehensive life cycle for the design, development and deployment
of Artificial Intelligence (AI) systems and solutions. It consists of three
phases, Design, Develop and Deploy, and 17 constituent stages across the three
phases from conception to production of any AI initiative. The 'Design' phase
highlights the importance of contextualising a problem description by reviewing
public domain and service-based literature on state-of-the-art AI applications,
algorithms, pre-trained models and equally importantly ethics guidelines and
frameworks, which then informs the data, or Big Data, acquisition and
preparation. The 'Develop' phase is technique-oriented, as it transforms data
and algorithms into AI models that are benchmarked, evaluated and explained.
The 'Deploy' phase evaluates computational performance, which then apprises
pipelines for model operationalisation, culminating in the hyperautomation of a
process or system as a complete AI solution, that is continuously monitored and
evaluated to inform the next iteration of the life cycle. An ontological
mapping of AI algorithms to applications, followed by an organisational context
for the AI life cycle are further contributions of this article.

Medical students will almost inevitably encounter powerful medical AI systems
early in their careers. Yet, contemporary medical education does not adequately
equip students with the basic clinical proficiency in medical AI needed to use
these tools safely and effectively. Education reform is urgently needed, but
not easily implemented, largely due to an already jam-packed medical curricula.
In this article, we propose an education reform framework as an effective and
efficient solution, which we call the Embedded AI Ethics Education Framework.
Unlike other calls for education reform to accommodate AI teaching that are
more radical in scope, our framework is modest and incremental. It leverages
existing bioethics or medical ethics curricula to develop and deliver content
on the ethical issues associated with medical AI, especially the harms of
technology misuse, disuse, and abuse that affect the risk-benefit analyses at
the heart of healthcare. In doing so, the framework provides a simple tool for
going beyond the "What?" and the "Why?" of medical AI ethics education, to
answer the "How?", giving universities, course directors, and/or professors a
broad road-map for equipping their students with the necessary clinical
proficiency in medical AI.

Anatoly Karpov's Queen sacrifices are analyzed. Stockfish 14 NNUE -- an AI
chess engine -- evaluates how efficient Karpov's sacrifices are. For
comparative purposes, we provide a dataset on Karpov's Rook and Knight
sacrifices to test whether Karpov achieves a similar level of accuracy. Our
study has implications for human-AI interaction and how humans can better
understand the strategies employed by black-box AI algorithms. Finally, we
conclude with implications for human study in. chess with computer engines.

Parallel to the rising debates over sustainable energy and artificial
intelligence solutions, the world is currently discussing the ethics of
artificial intelligence and its possible negative effects on society and the
environment. In these arguments, sustainable AI is proposed, which aims at
advancing the pathway toward sustainability, such as sustainable energy. In
this paper, we offered a novel contextual topic modeling combining LDA, BERT,
and Clustering. We then combined these computational analyses with content
analysis of related scientific publications to identify the main scholarly
topics, sub-themes, and cross-topic themes within scientific research on
sustainable AI in energy. Our research identified eight dominant topics
including sustainable buildings, AI-based DSSs for urban water management,
climate artificial intelligence, Agriculture 4, the convergence of AI with IoT,
AI-based evaluation of renewable technologies, smart campus and engineering
education, and AI-based optimization. We then recommended 14 potential future
research strands based on the observed theoretical gaps. Theoretically, this
analysis contributes to the existing literature on sustainable AI and
sustainable energy, and practically, it intends to act as a general guide for
energy engineers and scientists, AI scientists, and social scientists to widen
their knowledge of sustainability in AI and energy convergence research.

Trustworthy artificial intelligence (AI) has become an important topic
because trust in AI systems and their creators has been lost. Researchers,
corporations, and governments have long and painful histories of excluding
marginalized groups from technology development, deployment, and oversight. As
a result, these technologies are less useful and even harmful to minoritized
groups. We argue that any AI development, deployment, and monitoring framework
that aspires to trust must incorporate both feminist, non-exploitative
participatory design principles and strong, outside, and continual monitoring
and testing. We additionally explain the importance of considering aspects of
trustworthiness beyond just transparency, fairness, and accountability,
specifically, to consider justice and shifting power to the disempowered as
core values to any trustworthy AI system. Creating trustworthy AI starts by
funding, supporting, and empowering grassroots organizations like Queer in AI
so the field of AI has the diversity and inclusion to credibly and effectively
develop trustworthy AI. We leverage the expert knowledge Queer in AI has
developed through its years of work and advocacy to discuss if and how gender,
sexuality, and other aspects of queer identity should be used in datasets and
AI systems and how harms along these lines should be mitigated. Based on this,
we share a gendered approach to AI and further propose a queer epistemology and
analyze the benefits it can bring to AI. We additionally discuss how to
regulate AI with this queer epistemology in vision, proposing frameworks for
making policies related to AI & gender diversity and privacy & queer data
protection.

Although artificial intelligence (AI) is solving real-world challenges and
transforming industries, there are serious concerns about its ability to behave
and make decisions in a responsible way. Many AI ethics principles and
guidelines for responsible AI have been recently issued by governments,
organisations, and enterprises. However, these AI ethics principles and
guidelines are typically high-level and do not provide concrete guidance on how
to design and develop responsible AI systems. To address this shortcoming, we
first present an empirical study where we interviewed 21 scientists and
engineers to understand the practitioners' perceptions on AI ethics principles
and their implementation. We then propose a template that enables AI ethics
principles to be operationalised in the form of concrete patterns and suggest a
list of patterns using the newly created template. These patterns provide
concrete, operationalised guidance that facilitate the development of
responsible AI systems.

Recent advances in artificial intelligence (AI) for quantitative trading have
led to its general superhuman performance in significant trading performance.
However, the potential risk of AI trading is a "black box" decision. Some AI
computing mechanisms are complex and challenging to understand. If we use AI
without proper supervision, AI may lead to wrong choices and make huge losses.
Hence, we need to ask about the AI "black box", including why did AI decide to
do this or not? Why can people trust AI or not? How can people fix their
mistakes? These problems also highlight the challenges that AI technology can
explain in the trading field.

There has been significant recent interest in developing AI agents capable of
effectively interacting and teaming with humans. While each of these works try
to tackle a problem quite central to the problem of human-AI interaction, they
tend to rely on myopic formulations that obscure the possible inter-relatedness
and complementarity of many of these works. The human-aware AI framework was a
recent effort to provide a unified account for human-AI interaction by casting
them in terms of their relationship to various mental models. Unfortunately,
the current accounts of human-aware AI are insufficient to explain the
landscape of the work doing in the space of human-AI interaction due to their
focus on limited settings. In this paper, we aim to correct this shortcoming by
introducing a significantly general version of human-aware AI interaction
scheme, called generalized human-aware interaction (GHAI), that talks about
(mental) models of six types. Through this paper, we will see how this new
framework allows us to capture the various works done in the space of human-AI
interaction and identify the fundamental behavioral patterns supported by these
works. We will also use this framework to identify potential gaps in the
current literature and suggest future research directions to address these
shortcomings.

Although AI has significant potential to transform society, there are serious
concerns about its ability to behave and make decisions responsibly. Many
ethical regulations, principles, and guidelines for responsible AI have been
issued recently. However, these principles are high-level and difficult to put
into practice. In the meantime much effort has been put into responsible AI
from the algorithm perspective, but they are limited to a small subset of
ethical principles amenable to mathematical analysis. Responsible AI issues go
beyond data and algorithms and are often at the system-level crosscutting many
system components and the entire software engineering lifecycle. Based on the
result of a systematic literature review, this paper identifies one missing
element as the system-level guidance - how to design the architecture of
responsible AI systems. We present a summary of design patterns that can be
embedded into the AI systems as product features to contribute to
responsible-AI-by-design.

Conversational artificial intelligence (AI) is becoming an increasingly
popular topic among industry and academia. With the fast development of neural
network-based models, a lot of neural-based conversational AI system are
developed. We will provide a brief review of the recent progress in the
Conversational AI, including the commonly adopted techniques, notable works,
famous competitions from academia and industry and widely used datasets.

Recent works have recognized the need for human-centered perspectives when
designing and evaluating human-AI interactions and explainable AI methods. Yet,
current approaches fall short at intercepting and managing unexpected user
behavior resulting from the interaction with AI systems and explainability
methods of different stake-holder groups. In this work, we explore the use of
AI and explainability methods in the insurance domain. In an qualitative case
study with participants with different roles and professional backgrounds, we
show that AI and explainability methods are used in creative ways in daily
workflows, resulting in a divergence between their intended and actual use.
Finally, we discuss some recommendations for the design of human-AI
interactions and explainable AI methods to manage the risks and harness the
potential of unexpected user behavior.

As artificial intelligence (AI) becomes more powerful and widespread, the AI
alignment problem - how to ensure that AI systems pursue the goals that we want
them to pursue - has garnered growing attention. This article distinguishes two
types of alignment problems depending on whose goals we consider, and analyzes
the different solutions necessitated by each. The direct alignment problem
considers whether an AI system accomplishes the goals of the entity operating
it. In contrast, the social alignment problem considers the effects of an AI
system on larger groups or on society more broadly. In particular, it also
considers whether the system imposes externalities on others. Whereas solutions
to the direct alignment problem center around more robust implementation,
social alignment problems typically arise because of conflicts between
individual and group-level goals, elevating the importance of AI governance to
mediate such conflicts. Addressing the social alignment problem requires both
enforcing existing norms on their developers and operators and designing new
norms that apply directly to AI systems.

Artificial intelligence (AI), which enables machines to learn to perform a
task by training on diverse datasets, is one of the most revolutionary
developments in scientific history. Although AI and especially deep learning is
relatively new, it has already had transformative impact on medicine, biology,
transportation, entertainment, and beyond. As AI changes our daily lives at an
increasingly fast pace, we are challenged with preparing our society for an
AI-driven future. To this end, a critical step is to ensure an AI-ready
workforce through education. Advocates of beginning instruction of AI basics at
the K-12 level typically note benefits to the workforce, economy, and national
security. In this complementary perspective, we discuss why learning AI is
beneficial for motivating students and promoting creative thinking, and how to
develop a module-based approach that optimizes learning outcomes. We hope to
excite and engage more members of the education community to join the effort to
advance K-12 AI education in the USA and worldwide.

The recent spike in certified Artificial Intelligence (AI) tools for
healthcare has renewed the debate around adoption of this technology. One
thread of such debate concerns Explainable AI and its promise to render AI
devices more transparent and trustworthy. A few voices active in the medical AI
space have expressed concerns on the reliability of Explainable AI techniques,
questioning their use and inclusion in guidelines and standards. Revisiting
such criticisms, this article offers a balanced and comprehensive perspective
on the utility of Explainable AI, focusing on the specificity of clinical
applications of AI and placing them in the context of healthcare interventions.
Against its detractors and despite valid concerns, we argue that the
Explainable AI research program is still central to human-machine interaction
and ultimately our main tool against loss of control, a danger that cannot be
prevented by rigorous clinical validation alone.

Human-AI collaboration for decision-making strives to achieve team
performance that exceeds the performance of humans or AI alone. However, many
factors can impact success of Human-AI teams, including a user's domain
expertise, mental models of an AI system, trust in recommendations, and more.
This work examines users' interaction with three simulated algorithmic models,
all with similar accuracy but different tuning on their true positive and true
negative rates. Our study examined user performance in a non-trivial blood
vessel labeling task where participants indicated whether a given blood vessel
was flowing or stalled.
  Our results show that while recommendations from an AI-Assistant can aid user
decision making, factors such as users' baseline performance relative to the AI
and complementary tuning of AI error types significantly impact overall team
performance. Novice users improved, but not to the accuracy level of the AI.
Highly proficient users were generally able to discern when they should follow
the AI recommendation and typically maintained or improved their performance.
Mid-performers, who had a similar level of accuracy to the AI, were most
variable in terms of whether the AI recommendations helped or hurt their
performance. In addition, we found that users' perception of the AI's
performance relative on their own also had a significant impact on whether
their accuracy improved when given AI recommendations. This work provides
insights on the complexity of factors related to Human-AI collaboration and
provides recommendations on how to develop human-centered AI algorithms to
complement users in decision-making tasks.

The quality of Artificial Intelligence (AI) algorithms is of significant
importance for confidently adopting algorithms in various applications such as
cybersecurity, healthcare, and autonomous driving. This work presents a
principled framework of using a design-of-experimental approach to
systematically evaluate the quality of AI algorithms, named as Do-AIQ.
Specifically, we focus on investigating the quality of the AI mislabel data
algorithm against data poisoning. The performance of AI algorithms is affected
by hyperparameters in the algorithm and data quality, particularly, data
mislabeling, class imbalance, and data types. To evaluate the quality of the AI
algorithms and obtain a trustworthy assessment on the quality of the
algorithms, we establish a design-of-experiment framework to construct an
efficient space-filling design in a high-dimensional constraint space and
develop an effective surrogate model using additive Gaussian process to enable
the emulation of the quality of AI algorithms. Both theoretical and numerical
studies are conducted to justify the merits of the proposed framework. The
proposed framework can set an exemplar for AI algorithm to enhance the AI
assurance of robustness, reproducibility, and transparency.

AI/ML for data centres and data centres for AI/ML are defining new trends in
cloud computing. Disaggregated heterogeneous reconfigurable computing systems
realized by photonic interconnects and photonic switching expect greatly
enhanced throughput and energy-efficiency for AI/ML workloads, especially when
aided by an AI/ML control plane.

While revolutionary AI-powered code generation tools have been rising
rapidly, we know little about how and how to help software developers form
appropriate trust in those AI tools. Through a two-phase formative study, we
investigate how online communities shape developers' trust in AI tools and how
we can leverage community features to facilitate appropriate user trust.
Through interviewing 17 developers, we find that developers collectively make
sense of AI tools using the experiences shared by community members and
leverage community signals to evaluate AI suggestions. We then surface design
opportunities and conduct 11 design probe sessions to explore the design space
of using community features to support user trust in AI code generation
systems. We synthesize our findings and extend an existing model of user trust
in AI technologies with sociotechnical factors. We map out the design
considerations for integrating user community into the AI code generation
experience.

AI explanations are often mentioned as a way to improve human-AI
decision-making, but empirical studies have not found consistent evidence of
explanations' effectiveness and, on the contrary, suggest that they can
increase overreliance when the AI system is wrong. While many factors may
affect reliance on AI support, one important factor is how decision-makers
reconcile their own intuition -- beliefs or heuristics, based on prior
knowledge, experience, or pattern recognition, used to make judgments -- with
the information provided by the AI system to determine when to override AI
predictions. We conduct a think-aloud, mixed-methods study with two explanation
types (feature- and example-based) for two prediction tasks to explore how
decision-makers' intuition affects their use of AI predictions and
explanations, and ultimately their choice of when to rely on AI. Our results
identify three types of intuition involved in reasoning about AI predictions
and explanations: intuition about the task outcome, features, and AI
limitations. Building on these, we summarize three observed pathways for
decision-makers to apply their own intuition and override AI predictions. We
use these pathways to explain why (1) the feature-based explanations we used
did not improve participants' decision outcomes and increased their
overreliance on AI, and (2) the example-based explanations we used improved
decision-makers' performance over feature-based explanations and helped achieve
complementary human-AI performance. Overall, our work identifies directions for
further development of AI decision-support systems and explanation methods that
help decision-makers effectively apply their intuition to achieve appropriate
reliance on AI.

Artificial intelligence (AI) presents new challenges for the user experience
(UX) of products and services. Recently, practitioner-facing resources and
design guidelines have become available to ease some of these challenges.
However, little research has investigated if and how these guidelines are used,
and how they impact practice. In this paper, we investigated how industry
practitioners use the People + AI Guidebook. We conducted interviews with 31
practitioners (i.e., designers, product managers) to understand how they use
human-AI guidelines when designing AI-enabled products. Our findings revealed
that practitioners use the guidebook not only for addressing AI's design
challenges, but also for education, cross-functional communication, and for
developing internal resources. We uncovered that practitioners desire more
support for early phase ideation and problem formulation to avoid AI product
failures. We discuss the implications for future resources aiming to help
practitioners in designing AI products.

Despite the widespread use of artificial intelligence (AI), designing user
experiences (UX) for AI-powered systems remains challenging. UX designers face
hurdles understanding AI technologies, such as pre-trained language models, as
design materials. This limits their ability to ideate and make decisions about
whether, where, and how to use AI. To address this problem, we bridge the
literature on AI design and AI transparency to explore whether and how
frameworks for transparent model reporting can support design ideation with
pre-trained models. By interviewing 23 UX practitioners, we find that
practitioners frequently work with pre-trained models, but lack support for
UX-led ideation. Through a scenario-based design task, we identify common goals
that designers seek model understanding for and pinpoint their model
transparency information needs. Our study highlights the pivotal role that UX
designers can play in Responsible AI and calls for supporting their
understanding of AI limitations through model transparency and interrogation.

The emergence of generative AI technologies, such as OpenAI's ChatGPT
chatbot, has expanded the scope of tasks that AI tools can accomplish and
enabled AI-generated creative content. In this study, we explore how disclosure
regarding the use of AI in the creation of creative content affects human
evaluation of such content. In a series of pre-registered experimental studies,
we show that AI disclosure has no meaningful effect on evaluation either for
creative or descriptive short stories, but that AI disclosure has a negative
effect on evaluations for emotionally evocative poems written in the first
person. We interpret this result to suggest that reactions to AI-generated
content may be negative when the content is viewed as distinctly "human." We
discuss the implications of this work and outline planned pathways of research
to better understand whether and when AI disclosure may affect the evaluation
of creative content.

Recent work has proposed artificial intelligence (AI) models that can learn
to decide whether to make a prediction for an instance of a task or to delegate
it to a human by considering both parties' capabilities. In simulations with
synthetically generated or context-independent human predictions, delegation
can help improve the performance of human-AI teams -- compared to humans or the
AI model completing the task alone. However, so far, it remains unclear how
humans perform and how they perceive the task when they are aware that an AI
model delegated task instances to them. In an experimental study with 196
participants, we show that task performance and task satisfaction improve
through AI delegation, regardless of whether humans are aware of the
delegation. Additionally, we identify humans' increased levels of self-efficacy
as the underlying mechanism for these improvements in performance and
satisfaction. Our findings provide initial evidence that allowing AI models to
take over more management responsibilities can be an effective form of human-AI
collaboration in workplaces.

Intersectionality is a critical framework that, through inquiry and praxis,
allows us to examine how social inequalities persist through domains of
structure and discipline. Given AI fairness' raison d'etre of "fairness", we
argue that adopting intersectionality as an analytical framework is pivotal to
effectively operationalizing fairness. Through a critical review of how
intersectionality is discussed in 30 papers from the AI fairness literature, we
deductively and inductively: 1) map how intersectionality tenets operate within
the AI fairness paradigm and 2) uncover gaps between the conceptualization and
operationalization of intersectionality. We find that researchers
overwhelmingly reduce intersectionality to optimizing for fairness metrics over
demographic subgroups. They also fail to discuss their social context and when
mentioning power, they mostly situate it only within the AI pipeline. We: 3)
outline and assess the implications of these gaps for critical inquiry and
praxis, and 4) provide actionable recommendations for AI fairness researchers
to engage with intersectionality in their work by grounding it in AI
epistemology.

Through systematically analyzing the literature on designing AI-based
technologies, we extracted design implications and synthesized them into a
generic human-centered design framework for AI technologies to better support
human needs and mitigate their concerns. When adapting the framework to
children's context, understanding their specific needs, behaviors, experiences,
and social environments is needed. Therefore, we are working on projects to
explore tailored design considerations for children, such as through
investigating children's use of existing AI-based toys and learning
technologies. By participating in the ACM CHI 2023 Workshop on "Child-Centred
AI Design: Definition, Operation, and Considerations," we hope to learn more
about how other researchers in this field approach designing child-centered AI
technologies, exchange ideas on the research landscape of children and AI, and
explore the possibility to develop a practical child-centered design framework
of AI technologies for technology designers and developers.

The ongoing artificial intelligence (AI) revolution has the potential to
change almost every line of work. As AI capabilities continue to improve in
accuracy, robustness, and reach, AI may outperform and even replace human
experts across many valuable tasks. Despite enormous efforts devoted to
understanding AI's impact on labor and the economy and its recent success in
accelerating scientific discovery and progress, we lack a systematic
understanding of how advances in AI may benefit scientific research across
disciplines and fields. Here we develop a measurement framework to estimate
both the direct use of AI and the potential benefit of AI in scientific
research by applying natural language processing techniques to 87.6 million
publications and 7.1 million patents. We find that the use of AI in research
appears widespread throughout the sciences, growing especially rapidly since
2015, and papers that use AI exhibit an impact premium, more likely to be
highly cited both within and outside their disciplines. While almost every
discipline contains some subfields that benefit substantially from AI,
analyzing 4.6 million course syllabi across various educational disciplines, we
find a systematic misalignment between the education of AI and its impact on
research, suggesting the supply of AI talents in scientific disciplines is not
commensurate with AI research demands. Lastly, examining who benefits from AI
within the scientific workforce, we find that disciplines with a higher
proportion of women or black scientists tend to be associated with less
benefit, suggesting that AI's growing impact on research may further exacerbate
existing inequalities in science. As the connection between AI and scientific
research deepens, our findings may have an increasing value, with important
implications for the equity and sustainability of the research enterprise.

Explainable AI (XAI) is often promoted with the idea of helping users
understand how machine learning models function and produce predictions. Still,
most of these benefits are reserved for those with specialized domain
knowledge, such as machine learning developers. Recent research has argued that
making AI explainable can be a viable way of making AI more useful in
real-world contexts, especially within low-resource domains in the Global
South. While AI has transcended borders, a limited amount of work focuses on
democratizing the concept of explainable AI to the "majority world", leaving
much room to explore and develop new approaches within this space that cater to
the distinct needs of users within culturally and socially-diverse regions.
This article introduces the concept of an intercultural ethics approach to AI
explainability. It examines how cultural nuances impact the adoption and use of
technology, the factors that impede how technical concepts such as AI are
explained, and how integrating an intercultural ethics approach in the
development of XAI can improve user understanding and facilitate efficient
usage of these methods.

What role can AI play in supporting and constraining creative coding by
families? To investigate these questions, we built a Wizard of Oz platform to
help families engage in creative coding in partnership with a
researcher-operated AI Friend. We designed a 3 week series of programming
activities with ten children, 7 to 12 years old, and nine parents. Using a
creative self efficacy lens, we observe that families found it easier to
generate game ideas when prompted with questions by AI Friend; parents played a
unique role in guiding children in more complex programming tasks when the AI
Friend failed to help, and children were more encouraged to write code for
novel ideas using the AI friend help. These findings suggest that AI supported
platforms should highlight unique family AI interactions focused on children's
agency and creative self-efficacy.

Artificial intelligence (AI) represents a technological upheaval with the
potential to change human society. Because of its transformative potential, AI
is increasingly becoming subject to regulatory initiatives at the global level.
Yet, so far, scholarship in political science and international relations has
focused more on AI applications than on the emerging architecture of global AI
regulation. The purpose of this article is to outline an agenda for research
into the global governance of AI. The article distinguishes between two broad
perspectives: an empirical approach, aimed at mapping and explaining global AI
governance; and a normative approach, aimed at developing and applying
standards for appropriate global AI governance. The two approaches offer
questions, concepts, and theories that are helpful in gaining an understanding
of the emerging global governance of AI. Conversely, exploring AI as a
regulatory issue offers a critical opportunity to refine existing general
approaches to the study of global governance.

Digital technologies have dramatically accelerated the digital transformation
in process industries, boosted new industrial applications, upgraded the
production system, and enhanced operational efficiency. In contrast, the
challenges and gaps between human and artificial intelligence (AI) have become
more and more prominent, whereas the digital divide in process safety is
aggregating. The study attempts to address the following questions: (i)What is
AI in the process safety context? (ii)What is the difference between AI and
humans in process safety? (iii)How do AI and humans collaborate in process
safety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How
to quantify the risk of human-AI collaboration in process safety? Qualitative
risk analysis based on brainstorming and literature review, and quantitative
risk analysis based on layer of protection analysis (LOPA) and Bayesian network
(BN), were applied to explore and model. The importance of human reliability
should be stressed in the digital age, not usually to increase the reliability
of AI, and human-centered AI design in process safety needs to be propagated.

The merging of human intelligence and artificial intelligence has long been a
subject of interest in both science fiction and academia. In this paper, we
introduce a novel concept in Human-AI interaction called Symbiotic Artificial
Intelligence with Shared Sensory Experiences (SAISSE), which aims to establish
a mutually beneficial relationship between AI systems and human users through
shared sensory experiences. By integrating multiple sensory input channels and
processing human experiences, SAISSE fosters a strong human-AI bond, enabling
AI systems to learn from and adapt to individual users, providing personalized
support, assistance, and enhancement. Furthermore, we discuss the incorporation
of memory storage units for long-term growth and development of both the AI
system and its human user. As we address user privacy and ethical guidelines
for responsible AI-human symbiosis, we also explore potential biases and
inequalities in AI-human symbiosis and propose strategies to mitigate these
challenges. Our research aims to provide a comprehensive understanding of the
SAISSE concept and its potential to effectively support and enhance individual
human users through symbiotic AI systems. This position article aims at
discussing poteintial AI-human interaction related topics within the scientific
community, rather than providing experimental or theoretical results.

This paper discusses and explores the potential and relevance of recent
developments in artificial intelligence (AI) and digital twins for health and
well-being in low-resource African countries. We use the case of public health
emergency response to disease outbreaks and epidemic control. There is
potential to take advantage of the increasing availability of data and
digitization to develop advanced AI methods for analysis and prediction. Using
an AI systems perspective, we review emerging trends in AI systems and digital
twins and propose an initial augmented AI system architecture to illustrate how
an AI system can work with a 3D digital twin to address public health goals. We
highlight scientific knowledge discovery, continual learning, pragmatic
interoperability, and interactive explanation and decision-making as essential
research challenges for AI systems and digital twins.

This article is a short introduction to AI4OPT, the NSF AI Institute for
Advances in Optimization. AI4OPT fuses AI and Optimization, inspired by end-use
cases in supply chains, energy systems, chip design and manufacturing, and
sustainable food systems. AI4OPT also applies its "teaching the teachers"
philosophy to provide longitudinal educational pathways in AI for engineering.

Fueled by the soaring popularity of large language and foundation models, the
accelerated growth of artificial intelligence (AI) models' enormous
environmental footprint has come under increased scrutiny. While many
approaches have been proposed to make AI more energy-efficient and
environmentally friendly, environmental inequity -- the fact that AI's
environmental footprint can be disproportionately higher in certain regions
than in others -- has emerged, raising social-ecological justice concerns. This
paper takes a first step toward addressing AI's environmental inequity by
balancing its regional negative environmental impact. Concretely, we focus on
the carbon and water footprints of AI model inference and propose equity-aware
geographical load balancing (GLB) to explicitly address AI's environmental
impacts on the most disadvantaged regions. We run trace-based simulations by
considering a set of 10 geographically-distributed data centers that serve
inference requests for a large language AI model. The results demonstrate that
existing GLB approaches may amplify environmental inequity while our proposed
equity-aware GLB can significantly reduce the regional disparity in terms of
carbon and water footprints.

Is artificial intelligence (AI) disrupting jobs and creating unemployment?
Despite many attempts to quantify occupations' exposure to AI, inconsistent
validation obfuscates the relative benefits of each approach. A lack of
disaggregated labor outcome data, including unemployment data, further
exacerbates the issue. Here, we assess which models of AI exposure predict job
separations and unemployment risk using new occupation-level unemployment data
by occupation from each US state's unemployment insurance office spanning 2010
through 2020. Although these AI exposure scores have been used by governments
and industry, we find that individual AI exposure models are not predictive of
unemployment rates, unemployment risk, or job separation rates. However, an
ensemble of those models exhibits substantial predictive power suggesting that
competing models may capture different aspects of AI exposure that collectively
account for AI's variable impact across occupations, regions, and time. Our
results also call for dynamic, context-aware, and validated methods for
assessing AI exposure. Interactive visualizations for this study are available
at https://sites.pitt.edu/~mrfrank/uiRiskDemo/.

The rapidly advancing domain of Explainable Artificial Intelligence (XAI) has
sparked significant interests in developing techniques to make AI systems more
transparent and understandable. Nevertheless, in real-world contexts, the
methods of explainability and their evaluation strategies present numerous
limitations.Moreover, the scope of responsible AI extends beyond just
explainability. In this paper, we explore these limitations and discuss their
implications in a boarder context of responsible AI when considering other
important aspects, including privacy, fairness and contestability.

When working with generative artificial intelligence (AI), users may see
productivity gains, but the AI-generated content may not match their
preferences exactly. To study this effect, we introduce a Bayesian framework in
which heterogeneous users choose how much information to share with the AI,
facing a trade-off between output fidelity and communication cost. We show that
the interplay between these individual-level decisions and AI training may lead
to societal challenges. Outputs may become more homogenized, especially when
the AI is trained on AI-generated content. And any AI bias may become societal
bias. A solution to the homogenization and bias issues is to improve human-AI
interactions, enabling personalized outputs without sacrificing productivity.

This chapter provides a comprehensive discussion on AI regulation in the
European Union, contrasting it with the more sectoral and self-regulatory
approach in the UK. It argues for a hybrid regulatory strategy that combines
elements from both philosophies, emphasizing the need for agility and safe
harbors to ease compliance. The paper examines the AI Act as a pioneering
legislative effort to address the multifaceted challenges posed by AI,
asserting that, while the Act is a step in the right direction, it has
shortcomings that could hinder the advancement of AI technologies. The paper
also anticipates upcoming regulatory challenges, such as the management of
toxic content, environmental concerns, and hybrid threats. It advocates for
immediate action to create protocols for regulated access to high-performance,
potentially open-source AI systems. Although the AI Act is a significant
legislative milestone, it needs additional refinement and global collaboration
for the effective governance of rapidly evolving AI technologies.

Given rapid progress toward advanced AI and risks from frontier AI systems
(advanced AI systems pushing the boundaries of the AI capabilities frontier),
the creation and implementation of AI governance and regulatory schemes
deserves prioritization and substantial investment. However, the status quo is
untenable and, frankly, dangerous. A regulatory gap has permitted AI labs to
conduct research, development, and deployment activities with minimal
oversight. In response, frontier AI system evaluations have been proposed as a
way of assessing risks from the development and deployment of frontier AI
systems. Yet, the budding AI risk evaluation ecosystem faces significant
coordination challenges, such as a limited diversity of evaluators, suboptimal
allocation of effort, and perverse incentives. This paper proposes a solution
in the form of an international consortium for AI risk evaluations, comprising
both AI developers and third-party AI risk evaluators. Such a consortium could
play a critical role in international efforts to mitigate societal-scale risks
from advanced AI, including in managing responsible scaling policies and
coordinated evaluation-based risk response. In this paper, we discuss the
current evaluation ecosystem and its shortcomings, propose an international
consortium for advanced AI risk evaluations, discuss issues regarding its
implementation, discuss lessons that can be learnt from previous international
institutions and existing proposals for international AI governance
institutions, and, finally, we recommend concrete steps to advance the
establishment of the proposed consortium: (i) solicit feedback from
stakeholders, (ii) conduct additional research, (iii) conduct a workshop(s) for
stakeholders, (iv) analyze feedback and create final proposal, (v) solicit
funding, and (vi) create a consortium.

With the increasingly widespread adoption of AI in healthcare, maintaining
the accuracy and reliability of AI models in clinical practice has become
crucial. In this context, we introduce novel methods for monitoring the
performance of radiology AI classification models in practice, addressing the
challenges of obtaining real-time ground truth for performance monitoring. We
propose two metrics - predictive divergence and temporal stability - to be used
for preemptive alerts of AI performance changes. Predictive divergence,
measured using Kullback-Leibler and Jensen-Shannon divergences, evaluates model
accuracy by comparing predictions with those of two supplementary models.
Temporal stability is assessed through a comparison of current predictions
against historical moving averages, identifying potential model decay or data
drift. This approach was retrospectively validated using chest X-ray data from
a single-center imaging clinic, demonstrating its effectiveness in maintaining
AI model reliability. By providing continuous, real-time insights into model
performance, our system ensures the safe and effective use of AI in clinical
decision-making, paving the way for more robust AI integration in healthcare

The fast pace of advances in AI promises to revolutionize various aspects of
knowledge work, extending its influence to daily life and professional fields
alike. We advocate for a paradigm where AI is seen as a collaborative co-pilot,
working under human guidance rather than as a mere tool. Drawing from relevant
research and literature in the disciplines of Human-Computer Interaction and
Human Factors Engineering, we highlight the criticality of maintaining human
oversight in AI interactions. Reflecting on lessons from aviation, we address
the dangers of over-relying on automation, such as diminished human vigilance
and skill erosion. Our paper proposes a design approach that emphasizes active
human engagement, control, and skill enhancement in the AI partnership, aiming
to foster a harmonious, effective, and empowering human-AI relationship. We
particularly call out the critical need to design AI interaction capabilities
and software applications to enable and celebrate the primacy of human agency.
This calls for designs for human-AI partnership that cede ultimate control and
responsibility to the human user as pilot, with the AI co-pilot acting in a
well-defined supporting role.

The past decade has observed a great advancement in AI with deep
learning-based models being deployed in diverse scenarios including
safety-critical applications. As these AI systems become deeply embedded in our
societal infrastructure, the repercussions of their decisions and actions have
significant consequences, making the ethical implications of AI deployment
highly relevant and important. The ethical concerns associated with AI are
multifaceted, including challenging issues of fairness, privacy and data
protection, responsibility and accountability, safety and robustness,
transparency and explainability, and environmental impact. These principles
together form the foundations of ethical AI considerations that concern every
stakeholder in the AI system lifecycle. In light of the present ethical and
future x-risk concerns, governments have shown increasing interest in
establishing guidelines for the ethical deployment of AI. This work unifies the
current and future ethical concerns of deploying AI into society. While we
acknowledge and appreciate the technical surveys for each of the ethical
principles concerned, in this paper, we aim to provide a comprehensive overview
that not only addresses each principle from a technical point of view but also
discusses them from a social perspective.

Since ChatGPT works so well, are we on the cusp of solving science with AI?
Is not AlphaFold2 suggestive that the potential of LLMs in biology and the
sciences more broadly is limitless? Can we use AI itself to bridge the lack of
data in the sciences in order to then train an AI? Herein we present a
discussion of these topics.

As AI adoption accelerates, research on its economic impacts becomes a
salient source to consider for stakeholders of AI policy. Such research is
however still in its infancy, and one in need of review. This paper aims to
accomplish just that and is structured around two main themes. Firstly, the
path towards transformative AI, and secondly the wealth created by it. It is
found that sectors most embedded into global value chains will drive economic
impacts, hence special attention is paid to the international trade
perspective. When it comes to the path towards transformative AI, research is
heterogenous in its predictions, with some predicting rapid, unhindered
adoption, and others taking a more conservative view based on potential
bottlenecks and comparisons to past disruptive technologies. As for wealth
creation, while some agreement is to be found in AI's growth boosting
abilities, predictions on timelines are lacking. Consensus exists however
around the dispersion of AI induced wealth, which is heavily biased towards
developed countries due to phenomena such as anchoring and reduced bargaining
power of developing countries. Finally, a shortcoming of economic growth models
in failing to consider AI risk is discovered. Based on the review, a
calculated, and slower adoption rate of AI technologies is recommended.

In healthcare, artificial intelligence (AI) has been changing the way doctors
and health experts take care of people. This paper will cover how AI is making
major changes in the health care system, especially with nutrition. Various
machine learning and deep learning algorithms have been developed to extract
valuable information from healthcare data which help doctors, nutritionists,
and health experts to make better decisions and make our lifestyle healthy.
This paper provides an overview of the current state of AI applications in
healthcare with a focus on the utilization of AI-driven recommender systems in
nutrition. It will discuss the positive outcomes and challenges that arise when
AI is used in this field. This paper addresses the challenges to develop AI
recommender systems in healthcare, providing a well-rounded perspective on the
complexities. Real-world examples and research findings are presented to
underscore the tangible and significant impact AI recommender systems have in
the field of healthcare, particularly in nutrition. The ongoing efforts of
applying AI in nutrition lay the groundwork for a future where personalized
recommendations play a pivotal role in guiding individuals toward healthier
lifestyles.

The evolution of cybersecurity has spurred the emergence of autonomous threat
hunting as a pivotal paradigm in the realm of AI-driven threat intelligence.
This review navigates through the intricate landscape of autonomous threat
hunting, exploring its significance and pivotal role in fortifying cyber
defense mechanisms. Delving into the amalgamation of artificial intelligence
(AI) and traditional threat intelligence methodologies, this paper delineates
the necessity and evolution of autonomous approaches in combating contemporary
cyber threats. Through a comprehensive exploration of foundational AI-driven
threat intelligence, the review accentuates the transformative influence of AI
and machine learning on conventional threat intelligence practices. It
elucidates the conceptual framework underpinning autonomous threat hunting,
spotlighting its components, and the seamless integration of AI algorithms
within threat hunting processes.. Insightful discussions on challenges
encompassing scalability, interpretability, and ethical considerations in
AI-driven models enrich the discourse. Moreover, through illuminating case
studies and evaluations, this paper showcases real-world implementations,
underscoring success stories and lessons learned by organizations adopting
AI-driven threat intelligence. In conclusion, this review consolidates key
insights, emphasizing the substantial implications of autonomous threat hunting
for the future of cybersecurity. It underscores the significance of continual
research and collaborative efforts in harnessing the potential of AI-driven
approaches to fortify cyber defenses against evolving threats.

Neurosymbolic artificial intelligence (AI) is an emerging branch of AI that
combines the strengths of symbolic AI and sub-symbolic AI. A major drawback of
sub-symbolic AI is that it acts as a "black box", meaning that predictions are
difficult to explain, making the testing & evaluation (T&E) and validation &
verification (V&V) processes of a system that uses sub-symbolic AI a challenge.
Since neurosymbolic AI combines the advantages of both symbolic and
sub-symbolic AI, this survey explores how neurosymbolic applications can ease
the V&V process. This survey considers two taxonomies of neurosymbolic AI,
evaluates them, and analyzes which algorithms are commonly used as the symbolic
and sub-symbolic components in current applications. Additionally, an overview
of current techniques for the T&E and V&V processes of these components is
provided. Furthermore, it is investigated how the symbolic part is used for T&E
and V&V purposes in current neurosymbolic applications. Our research shows that
neurosymbolic AI as great potential to ease the T&E and V&V processes of
sub-symbolic AI by leveraging the possibilities of symbolic AI. Additionally,
the applicability of current T&E and V&V methods to neurosymbolic AI is
assessed, and how different neurosymbolic architectures can impact these
methods is explored. It is found that current T&E and V&V techniques are partly
sufficient to test, evaluate, verify, or validate the symbolic and sub-symbolic
part of neurosymbolic applications independently, while some of them use
approaches where current T&E and V&V methods are not applicable by default, and
adjustments or even new approaches are needed. Our research shows that there is
great potential in using symbolic AI to test, evaluate, verify, or validate the
predictions of a sub-symbolic model, making neurosymbolic AI an interesting
research direction for safe, secure, and trustworthy AI.

The growing popularity of generative artificial intelligence (AI) chatbots
such as ChatGPT is having transformative effects on social media. As the
prevalence of AI-generated content grows, concerns have been raised regarding
privacy and misinformation online. Among social media platforms, Discord
enables AI integrations -- making their primarily "Generation Z" userbase
particularly exposed to AI-generated content. We surveyed Generation Z aged
individuals (n = 335) to evaluate their proficiency in discriminating between
AI-generated and human-authored text on Discord. The investigation employed
one-shot prompting of ChatGPT, disguised as a text message received on the
Discord.com platform. We explore the influence of demographic factors on
ability, as well as participants' familiarity with Discord and artificial
intelligence technologies. We find that Generation Z individuals are unable to
discern between AI and human-authored text (p = 0.011), and that those with
lower self-reported familiarity with Discord demonstrated an improved ability
in identifying human-authored compared to those with self-reported experience
with AI (p << 0.0001). Our results suggest that there is a nuanced relationship
between AI technology and popular modes of communication for Generation Z,
contributing valuable insights into human-computer interactions, digital
communication, and artificial intelligence literacy.

This paper introduces A2C, a multi-stage collaborative decision framework
designed to enable robust decision-making within human-AI teams. Drawing
inspiration from concepts such as rejection learning and learning to defer, A2C
incorporates AI systems trained to recognise uncertainty in their decisions and
defer to human experts when needed. Moreover, A2C caters to scenarios where
even human experts encounter limitations, such as in incident detection and
response in cyber Security Operations Centres (SOC). In such scenarios, A2C
facilitates collaborative explorations, enabling collective resolution of
complex challenges. With support for three distinct decision-making modes in
human-AI teams: Automated, Augmented, and Collaborative, A2C offers a flexible
platform for developing effective strategies for human-AI collaboration. By
harnessing the strengths of both humans and AI, it significantly improves the
efficiency and effectiveness of complex decision-making in dynamic and evolving
environments. To validate A2C's capabilities, we conducted extensive simulative
experiments using benchmark datasets. The results clearly demonstrate that all
three modes of decision-making can be effectively supported by A2C. Most
notably, collaborative exploration by (simulated) human experts and AI achieves
superior performance compared to AI in isolation, underscoring the framework's
potential to enhance decision-making within human-AI teams.

Generative AI applications present unique design challenges. As generative AI
technologies are increasingly being incorporated into mainstream applications,
there is an urgent need for guidance on how to design user experiences that
foster effective and safe use. We present six principles for the design of
generative AI applications that address unique characteristics of generative AI
UX and offer new interpretations and extensions of known issues in the design
of AI applications. Each principle is coupled with a set of design strategies
for implementing that principle via UX capabilities or through the design
process. The principles and strategies were developed through an iterative
process involving literature review, feedback from design practitioners,
validation against real-world generative AI applications, and incorporation
into the design process of two generative AI applications. We anticipate the
principles to usefully inform the design of generative AI applications by
driving actionable design recommendations.

The increasing use of Artificial Intelligence (AI) by students in learning
presents new challenges for assessing their learning outcomes in project-based
learning (PBL). This paper introduces a co-design study to explore the
potential of students' AI usage data as a novel material for PBL assessment. We
conducted workshops with 18 college students, encouraging them to speculate an
alternative world where they could freely employ AI in PBL while needing to
report this process to assess their skills and contributions. Our workshops
yielded various scenarios of students' use of AI in PBL and ways of analyzing
these uses grounded by students' vision of education goal transformation. We
also found students with different attitudes toward AI exhibited distinct
preferences in how to analyze and understand the use of AI. Based on these
findings, we discuss future research opportunities on student-AI interactions
and understanding AI-enhanced learning.

We propose Embodied AI as the next fundamental step in the pursuit of
Artificial General Intelligence, juxtaposing it against current AI
advancements, particularly Large Language Models. We traverse the evolution of
the embodiment concept across diverse fields - philosophy, psychology,
neuroscience, and robotics - to highlight how EAI distinguishes itself from the
classical paradigm of static learning. By broadening the scope of Embodied AI,
we introduce a theoretical framework based on cognitive architectures,
emphasizing perception, action, memory, and learning as essential components of
an embodied agent. This framework is aligned with Friston's active inference
principle, offering a comprehensive approach to EAI development. Despite the
progress made in the field of AI, substantial challenges, such as the
formulation of a novel AI learning theory and the innovation of advanced
hardware, persist. Our discussion lays down a foundational guideline for future
Embodied AI research. Highlighting the importance of creating Embodied AI
agents capable of seamless communication, collaboration, and coexistence with
humans and other intelligent entities within real-world environments, we aim to
steer the AI community towards addressing the multifaceted challenges and
seizing the opportunities that lie ahead in the quest for AGI.

Recent advancements in HCI and AI research attempt to support user experience
(UX) practitioners with AI-enabled tools. Despite the potential of emerging
models and new interaction mechanisms, mainstream adoption of such tools
remains limited. We took the lens of Human-Centered AI and presented a
systematic literature review of 359 papers, aiming to synthesize the current
landscape, identify trends, and uncover UX practitioners' unmet needs in AI
support. Guided by the Double Diamond design framework, our analysis uncovered
that UX practitioners' unique focuses on empathy building and experiences
across UI screens are often overlooked. Simplistic AI automation can obstruct
the valuable empathy-building process. Furthermore, focusing solely on
individual UI screens without considering interactions and user flows reduces
the system's practical value for UX designers. Based on these findings, we call
for a deeper understanding of UX mindsets and more designer-centric datasets
and evaluation metrics, for HCI and AI communities to collaboratively work
toward effective AI support for UX.

Clients often partner with AI experts to develop AI applications tailored to
their needs. In these partnerships, careful planning and clear communication
are critical, as inaccurate or incomplete specifications can result in
misaligned model characteristics, expensive reworks, and potential friction
between collaborators. Unfortunately, given the complexity of requirements
ranging from functionality, data, and governance, effective guidelines for
collaborative specification of requirements in client-AI expert collaborations
are missing. In this work, we introduce AINeedsPlanner, a workbook that AI
experts and clients can use to facilitate effective interchange and clear
specifications. The workbook is based on (1) an interview of 10 completed AI
application project teams, which identifies and characterizes steps in AI
application planning and (2) a study with 12 AI experts, which defines a
taxonomy of AI experts' information needs and dimensions that affect the
information needs. Finally, we demonstrate the workbook's utility with two case
studies in real-world settings.

The adoption of Artificial Intelligence (AI) based Virtual Network Functions
(VNFs) has witnessed significant growth, posing a critical challenge in
orchestrating AI models within next-generation 6G networks. Finding optimal AI
model placement is significantly more challenging than placing traditional
software-based VNFs, due to the introduction of numerous uncertain factors by
AI models, such as varying computing resource consumption, dynamic storage
requirements, and changing model performance. To address the AI model placement
problem under uncertainties, this paper presents a novel approach employing a
sequence-to-sequence (S2S) neural network which considers uncertainty
estimations. The S2S model, characterized by its encoding-decoding
architecture, is designed to take the service chain with a number of AI models
as input and produce the corresponding placement of each AI model. To address
the introduced uncertainties, our methodology incorporates the orthonormal
certificate module for uncertainty estimation and utilizes fuzzy logic for
uncertainty representation, thereby enhancing the capabilities of the S2S
model. Experiments demonstrate that the proposed method achieves competitive
results across diverse AI model profiles, network environments, and service
chain requests.

AI-based virtual assistants are increasingly used to support daily ideation
tasks. The values or bias present in these agents can influence output in
hidden ways. They may also affect how people perceive the ideas produced with
these AI agents and lead to implications for the design of AI-based tools. We
explored the effects of AI agents with different values on the ideation process
and user perception of idea quality, ownership, agent competence, and values
present in the output. Our study tasked 180 participants with brainstorming
practical solutions to a set of problems with AI agents of different values.
Results show no significant difference in self-evaluation of idea quality and
perception of the agent based on value alignment; however, ideas generated
reflected the AI's values and feeling of ownership is affected. This highlights
an intricate interplay between AI values and human ideation, suggesting careful
design considerations for future AI-supported brainstorming tools.

The convergence of Artificial Intelligence (AI) and blockchain technology is
reshaping the digital world, offering decentralized, secure, and efficient AI
services on blockchain platforms. Despite the promise, the high computational
demands of AI on blockchain raise significant privacy and efficiency concerns.
The Optimistic Privacy-Preserving AI (opp/ai) framework is introduced as a
pioneering solution to these issues, striking a balance between privacy
protection and computational efficiency. The framework integrates
Zero-Knowledge Machine Learning (zkML) for privacy with Optimistic Machine
Learning (opML) for efficiency, creating a hybrid model tailored for blockchain
AI services. This study presents the opp/ai framework, delves into the privacy
features of zkML, and assesses the framework's performance and adaptability
across different scenarios.

Recent studies of the applications of conversational AI tools, such as
chatbots powered by large language models, to complex real-world knowledge work
have shown limitations related to reasoning and multi-step problem solving.
Specifically, while existing chatbots simulate shallow reasoning and
understanding they are prone to errors as problem complexity increases. The
failure of these systems to address complex knowledge work is due to the fact
that they do not perform any actual cognition. In this position paper, we
present Cognitive AI, a higher-level framework for implementing
programmatically defined neuro-symbolic cognition above and outside of large
language models. Specifically, we propose a dual-layer functional architecture
for Cognitive AI that serves as a roadmap for AI systems that can perform
complex multi-step knowledge work. We propose that Cognitive AI is a necessary
precursor for the evolution of higher forms of AI, such as AGI, and
specifically claim that AGI cannot be achieved by probabilistic approaches on
their own. We conclude with a discussion of the implications for large language
models, adoption cycles in AI, and commercial Cognitive AI development.

Current discourse surrounding Artificial Intelligence (AI) oscillates between
hope and apprehension, painting a future where AI reshapes every facet of human
life, including Education. This paper delves into the complexities of AI's role
in Education, addressing the mixed messages that have both enthused and alarmed
educators, policymakers, and the public. It explores the promises that AI holds
for enhancing learning through personalisation at scale, against the backdrop
of concerns about ethical implications, the devaluation of non-STEM subjects,
and the potential transformative impact on our neurocognitive and
socio-emotional functioning. Drawing on recent research and global discourse,
the paper seeks to unpack the reasons behind the vagueness of current
discussions on AI in Education (AIED) and the implications of this ambiguity
for future educational practices and policies. By highlighting insights from
educational research and synthesising evidence-based best practices in AIED,
the aim is to provide a clearer understanding of how AI technologies can be
aligned with the fundamental principles of learning and teaching, and explore
what concrete actions may need to be prioritised now to truly enhance learning
experiences and outcomes for all in the future.

Our research endeavors to advance the concept of responsible artificial
intelligence (AI), a topic of increasing importance within EU policy
discussions. The EU has recently issued several publications emphasizing the
necessity of trust in AI, underscoring the dual nature of AI as both a
beneficial tool and a potential weapon. This dichotomy highlights the urgent
need for international regulation. Concurrently, there is a need for frameworks
that guide companies in AI development, ensuring compliance with such
regulations. Our research aims to assist lawmakers and machine learning
practitioners in navigating the evolving landscape of AI regulation,
identifying focal areas for future attention. This paper introduces a
comprehensive and, to our knowledge, the first unified definition of
responsible AI. Through a structured literature review, we elucidate the
current understanding of responsible AI. Drawing from this analysis, we propose
an approach for developing a future framework centered around this concept. Our
findings advocate for a human-centric approach to Responsible AI. This approach
encompasses the implementation of AI methods with a strong emphasis on ethics,
model explainability, and the pillars of privacy, security, and trust.

Generative AI systems have been heralded as tools for augmenting human
creativity and inspiring divergent thinking, though with little empirical
evidence for these claims. This paper explores the effects of exposure to
AI-generated images on measures of design fixation and divergent thinking in a
visual ideation task. Through a between-participants experiment (N=60), we
found that support from an AI image generator during ideation leads to higher
fixation on an initial example. Participants who used AI produced fewer ideas,
with less variety and lower originality compared to a baseline. Our qualitative
analysis suggests that the effectiveness of co-ideation with AI rests on
participants' chosen approach to prompt creation and on the strategies used by
participants to generate ideas in response to the AI's suggestions. We discuss
opportunities for designing generative AI systems for ideation support and
incorporating these AI tools into ideation workflows.

The integration of generative Artificial Intelligence (AI) chatbots in higher
education institutions (HEIs) is reshaping the educational landscape, offering
opportunities for enhanced student support, and administrative and research
efficiency. This study explores the future implications of generative AI
chatbots in HEIs, aiming to understand their potential impact on teaching and
learning, and research processes. Utilizing a narrative literature review (NLR)
methodology, this study synthesizes existing research on generative AI chatbots
in higher education from diverse sources, including academic databases and
scholarly publications. The findings highlight the transformative potential of
generative AI chatbots in streamlining administrative tasks, enhancing student
learning experiences, and supporting research activities. However, challenges
such as academic integrity concerns, user input understanding, and resource
allocation pose significant obstacles to the effective integration of
generative AI chatbots in HEIs. This study underscores the importance of
proactive measures to address ethical considerations, provide comprehensive
training for stakeholders, and establish clear guidelines for the responsible
use of generative AI chatbots in higher education. By navigating these
challenges, and leveraging the benefits of generative AI technologies, HEIs can
harness the full potential of generative AI chatbots to create a more
efficient, effective, inclusive, and innovative educational environment.

As AI Agents based on Large Language Models (LLMs) have shown potential in
practical applications across various fields, how to quickly deploy an AI agent
and how to conveniently expand the application scenario of AI agents has become
a challenge. Previous studies mainly focused on implementing all the reasoning
capabilities of AI agents within a single LLM, which often makes the model more
complex and also reduces the extensibility of AI agent functionality. In this
paper, we propose CACA Agent (Capability Collaboration based AI Agent), using
an open architecture inspired by service computing. CACA Agent integrates a set
of collaborative capabilities to implement AI Agents, not only reducing the
dependence on a single LLM, but also enhancing the extensibility of both the
planning abilities and the tools available to AI agents. Utilizing the proposed
system, we present a demo to illustrate the operation and the application
scenario extension of CACA Agent.

Potential malicious misuse of civilian artificial intelligence (AI) poses
serious threats to security on a national and international level. Besides
defining autonomous systems from a technological viewpoint and explaining how
AI development is characterized, we show how already existing and openly
available AI technology could be misused. To underline this, we developed three
exemplary use cases of potentially misused AI that threaten political, digital
and physical security. The use cases can be built from existing AI technologies
and components from academia, the private sector and the developer-community.
This shows how freely available AI can be combined into autonomous weapon
systems. Based on the use cases, we deduce points of control and further
measures to prevent the potential threat through misused AI. Further, we
promote the consideration of malicious misuse of civilian AI systems in the
discussion on autonomous weapon systems (AWS).

With the rapid advancement of artificial intelligence (AI) in various
domains, the education sector is set for transformation. The potential of
AI-driven tools in enhancing the learning experience, especially in
programming, is immense. However, the scientific evaluation of Large Language
Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an
AI-Tutor remains largely unexplored. Therefore, there is a need to understand
how students interact with such AI-Tutors and to analyze their experiences. In
this paper, we conducted an exploratory case study by integrating the
GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a
combination of empirical data collection and an exploratory survey, we
identified different user types based on their interaction patterns with the
AI-Tutor. Additionally, the findings highlight advantages, such as timely
feedback and scalability. However, challenges like generic responses and
students' concerns about a learning progress inhibition when using the AI-Tutor
were also evident. This research adds to the discourse on AI's role in
education.

Artificial Intelligence (AI) will change human work by taking over specific
job tasks, but there is a debate which tasks are susceptible to automation, and
whether AI will augment or replace workers and affect wages. By combining data
on job tasks with a measure of AI susceptibility, we show that more highly
skilled workers are more susceptible to AI automation, and that analytical
non-routine tasks are at risk to be impacted by AI. Moreover, we observe that
wage growth premiums for the lowest and the highest required skill level appear
unrelated to AI susceptibility and that workers in occupations with many
routine tasks saw higher wage growth if their work was more strongly
susceptible to AI. Our findings imply that AI has the potential to affect human
workers differently than canonical economic theories about the impact of
technology on work these theories predict.

Artificial Intelligence (AI) has become a ubiquitous part of society, but a
key challenge exists in ensuring that humans are equipped with the required
critical thinking and AI literacy skills to interact with machines effectively
by understanding their capabilities and limitations. These skills are
particularly important for learners to develop in the age of generative AI
where AI tools can demonstrate complex knowledge and ability previously thought
to be uniquely human. To activate effective human-AI partnerships in writing,
this paper provides a first step toward conceptualizing the notion of critical
learner interaction with AI. Using both theoretical models and empirical data,
our preliminary findings suggest a general lack of Deep interaction with AI
during the writing process. We believe that the outcomes can lead to better
task and tool design in the future for learners to develop deep, critical
thinking when interacting with AI.

Given that Artificial Intelligence (AI) increasingly permeates our lives, it
is critical that we systematically align AI objectives with the goals and
values of humans. The human-AI alignment problem stems from the impracticality
of explicitly specifying the rewards that AI models should receive for all the
actions they could take in all relevant states of the world. One possible
solution, then, is to leverage the capabilities of AI models to learn those
rewards implicitly from a rich source of data describing human values in a wide
range of contexts. The democratic policy-making process produces just such data
by developing specific rules, flexible standards, interpretable guidelines, and
generalizable precedents that synthesize citizens' preferences over potential
actions taken in many states of the world. Therefore, computationally encoding
public policies to make them legible to AI systems should be an important part
of a socio-technical approach to the broader human-AI alignment puzzle. This
Essay outlines research on AI that learn structures in policy data that can be
leveraged for downstream tasks. As a demonstration of the ability of AI to
comprehend policy, we provide a case study of an AI system that predicts the
relevance of proposed legislation to any given publicly traded company and its
likely effect on that company. We believe this represents the "comprehension"
phase of AI and policy, but leveraging policy as a key source of human values
to align AI requires "understanding" policy. Solving the alignment problem is
crucial to ensuring that AI is beneficial both individually (to the person or
group deploying the AI) and socially. As AI systems are given increasing
responsibility in high-stakes contexts, integrating democratically-determined
policy into those systems could align their behavior with human goals in a way
that is responsive to a constantly evolving society.

Artificial Intelligence (AI) has made impressive progress in recent years and
represents a key technology that has a crucial impact on the economy and
society. However, it is clear that AI and business models based on it can only
reach their full potential if AI applications are developed according to high
quality standards and are effectively protected against new AI risks. For
instance, AI bears the risk of unfair treatment of individuals when processing
personal data e.g., to support credit lending or staff recruitment decisions.
The emergence of these new risks is closely linked to the fact that the
behavior of AI applications, particularly those based on Machine Learning (ML),
is essentially learned from large volumes of data and is not predetermined by
fixed programmed rules.
  Thus, the issue of the trustworthiness of AI applications is crucial and is
the subject of numerous major publications by stakeholders in politics,
business and society. In addition, there is mutual agreement that the
requirements for trustworthy AI, which are often described in an abstract way,
must now be made clear and tangible. One challenge to overcome here relates to
the fact that the specific quality criteria for an AI application depend
heavily on the application context and possible measures to fulfill them in
turn depend heavily on the AI technology used. Lastly, practical assessment
procedures are needed to evaluate whether specific AI applications have been
developed according to adequate quality standards. This AI assessment catalog
addresses exactly this point and is intended for two target groups: Firstly, it
provides developers with a guideline for systematically making their AI
applications trustworthy. Secondly, it guides assessors and auditors on how to
examine AI applications for trustworthiness in a structured way.

In the last years, AI safety gained international recognition in the light of
heterogeneous safety-critical and ethical issues that risk overshadowing the
broad beneficial impacts of AI. In this context, the implementation of AI
observatory endeavors represents one key research direction. This paper
motivates the need for an inherently transdisciplinary AI observatory approach
integrating diverse retrospective and counterfactual views. We delineate aims
and limitations while providing hands-on-advice utilizing concrete practical
examples. Distinguishing between unintentionally and intentionally triggered AI
risks with diverse socio-psycho-technological impacts, we exemplify a
retrospective descriptive analysis followed by a retrospective counterfactual
risk analysis. Building on these AI observatory tools, we present near-term
transdisciplinary guidelines for AI safety. As further contribution, we discuss
differentiated and tailored long-term directions through the lens of two
disparate modern AI safety paradigms. For simplicity, we refer to these two
different paradigms with the terms artificial stupidity (AS) and eternal
creativity (EC) respectively. While both AS and EC acknowledge the need for a
hybrid cognitive-affective approach to AI safety and overlap with regard to
many short-term considerations, they differ fundamentally in the nature of
multiple envisaged long-term solution patterns. By compiling relevant
underlying contradistinctions, we aim to provide future-oriented incentives for
constructive dialectics in practical and theoretical AI safety research.

The rise of Artificial Intelligence (AI) will bring with it an
ever-increasing willingness to cede decision-making to machines. But rather
than just giving machines the power to make decisions that affect us, we need
ways to work cooperatively with AI systems. There is a vital need for research
in "AI and Cooperation" that seeks to understand the ways in which systems of
AIs and systems of AIs with people can engender cooperative behavior. Trust in
AI is also key: trust that is intrinsic and trust that can only be earned over
time. Here we use the term "AI" in its broadest sense, as employed by the
recent 20-Year Community Roadmap for AI Research (Gil and Selman, 2019),
including but certainly not limited to, recent advances in deep learning.
  With success, cooperation between humans and AIs can build society just as
human-human cooperation has. Whether coming from an intrinsic willingness to be
helpful, or driven through self-interest, human societies have grown strong and
the human species has found success through cooperation. We cooperate "in the
small" -- as family units, with neighbors, with co-workers, with strangers --
and "in the large" as a global community that seeks cooperative outcomes around
questions of commerce, climate change, and disarmament. Cooperation has evolved
in nature also, in cells and among animals. While many cases involving
cooperation between humans and AIs will be asymmetric, with the human
ultimately in control, AI systems are growing so complex that, even today, it
is impossible for the human to fully comprehend their reasoning,
recommendations, and actions when functioning simply as passive observers.

Recent success in Artificial Intelligence (AI) and Machine Learning (ML)
allow problem solving automatically without any human intervention. Autonomous
approaches can be very convenient. However, in certain domains, e.g., in the
medical domain, it is necessary to enable a domain expert to understand, why an
algorithm came up with a certain result. Consequently, the field of Explainable
AI (xAI) rapidly gained interest worldwide in various domains, particularly in
medicine. Explainable AI studies transparency and traceability of opaque AI/ML
and there are already a huge variety of methods. For example with layer-wise
relevance propagation relevant parts of inputs to, and representations in, a
neural network which caused a result, can be highlighted. This is a first
important step to ensure that end users, e.g., medical professionals, assume
responsibility for decision making with AI/ML and of interest to professionals
and regulators. Interactive ML adds the component of human expertise to AI/ML
processes by enabling them to re-enact and retrace AI/ML results, e.g. let them
check it for plausibility. This requires new human-AI interfaces for
explainable AI. In order to build effective and efficient interactive human-AI
interfaces we have to deal with the question of how to evaluate the quality of
explanations given by an explainable AI system. In this paper we introduce our
System Causability Scale (SCS) to measure the quality of explanations. It is
based on our notion of Causability (Holzinger et al., 2019) combined with
concepts adapted from a widely accepted usability scale.

In January and February 2020, the Scottish Government released two documents
for review by the public regarding their artificial intelligence (AI) strategy.
The Montreal AI Ethics Institute (MAIEI) reviewed these documents and published
a response on 4 June 2020. MAIEI's response examines several questions that
touch on the proposed definition of AI; the people-centered nature of the
strategy; considerations to ensure that everyone benefits from AI; the
strategy's overarching vision; Scotland's AI ecosystem; the proposed strategic
themes; and how to grow public confidence in AI by building responsible and
ethical systems.
  In addition to examining the points above, MAIEI suggests that the strategy
be extended to include considerations on biometric data and how that will be
processed and used in the context of AI. It also highlights the importance of
tackling head-on the inherently stochastic nature of deep learning systems and
developing concrete guidelines to ensure that these systems are built
responsibly and ethically, particularly as machine learning becomes more
accessible. Finally, it concludes that any national AI strategy must clearly
address the measurements of success in regards to the strategy's stated goals
and vision to ensure that they are interpreted and applied consistently. To do
this, there must be inclusion and transparency between those building the
systems and those using them in their work.

Today, Artificial Intelligence (AI) has a direct impact on the daily life of
billions of people. Being applied to sectors like finance, health, security and
advertisement, AI fuels some of the biggest companies and research institutions
in the world. Its impact in the near future seems difficult to predict or
bound. In contrast to all this power, society remains mostly ignorant of the
capabilities and standard practices of AI today. To address this imbalance,
improving current interactions between people and AI systems, we propose a
transparency scheme to be implemented on any AI system open to the public. The
scheme is based on two pillars: Data Privacy and AI Transparency. The first
recognizes the relevance of data for AI, and is supported by GDPR. The second
considers aspects of AI transparency currently unregulated: AI capabilities,
purpose and source. We design this pillar based on ethical principles. For each
of the two pillars, we define a three-level display. The first level is based
on visual signs, inspired by traffic signs managing the interaction between
people and cars, and designed for quick and universal interpretability. The
second level uses factsheets, providing limited details. The last level
provides access to all available information. After detailing and exemplifying
the proposed transparency scheme, we define a set of principles for creating
transparent by design software, to be used during the integration of AI
components on user-oriented services.

The increased adoption of Artificial Intelligence (AI) presents an
opportunity to solve many socio-economic and environmental challenges; however,
this cannot happen without securing AI-enabled technologies. In recent years,
most AI models are vulnerable to advanced and sophisticated hacking techniques.
This challenge has motivated concerted research efforts into adversarial AI,
with the aim of developing robust machine and deep learning models that are
resilient to different types of adversarial scenarios. In this paper, we
present a holistic cyber security review that demonstrates adversarial attacks
against AI applications, including aspects such as adversarial knowledge and
capabilities, as well as existing methods for generating adversarial examples
and existing cyber defence models. We explain mathematical AI models,
especially new variants of reinforcement and federated learning, to demonstrate
how attack vectors would exploit vulnerabilities of AI models. We also propose
a systematic framework for demonstrating attack techniques against AI
applications and reviewed several cyber defences that would protect AI
applications against those attacks. We also highlight the importance of
understanding the adversarial goals and their capabilities, especially the
recent attacks against industry applications, to develop adaptive defences that
assess to secure AI applications. Finally, we describe the main challenges and
future research directions in the domain of security and privacy of AI
technologies.

The explanation dimension of Artificial Intelligence (AI) based system has
been a hot topic for the past years. Different communities have raised concerns
about the increasing presence of AI in people's everyday tasks and how it can
affect people's lives. There is a lot of research addressing the
interpretability and transparency concepts of explainable AI (XAI), which are
usually related to algorithms and Machine Learning (ML) models. But in
decision-making scenarios, people need more awareness of how AI works and its
outcomes to build a relationship with that system. Decision-makers usually need
to justify their decision to others in different domains. If that decision is
somehow based on or influenced by an AI-system outcome, the explanation about
how the AI reached that result is key to building trust between AI and humans
in decision-making scenarios. In this position paper, we discuss the role of
XAI in decision-making scenarios, our vision of Decision-Making with AI-system
in the loop, and explore one case from the literature about how XAI can impact
people justifying their decisions, considering the importance of building the
human-AI relationship for those scenarios.

The ethical implications and social impacts of artificial intelligence have
become topics of compelling interest to industry, researchers in academia, and
the public. However, current analyses of AI in a global context are biased
toward perspectives held in the U.S., and limited by a lack of research,
especially outside the U.S. and Western Europe.
  This article summarizes the key findings of a literature review of recent
social science scholarship on the social impacts of AI and related technologies
in five global regions. Our team of social science researchers reviewed more
than 800 academic journal articles and monographs in over a dozen languages.
  Our review of the literature suggests that AI is likely to have markedly
different social impacts depending on geographical setting. Likewise,
perceptions and understandings of AI are likely to be profoundly shaped by
local cultural and social context.
  Recent research in U.S. settings demonstrates that AI-driven technologies
have a pattern of entrenching social divides and exacerbating social
inequality, particularly among historically-marginalized groups. Our literature
review indicates that this pattern exists on a global scale, and suggests that
low- and middle-income countries may be more vulnerable to the negative social
impacts of AI and less likely to benefit from the attendant gains.
  We call for rigorous ethnographic research to better understand the social
impacts of AI around the world. Global, on-the-ground research is particularly
critical to identify AI systems that may amplify social inequality in order to
mitigate potential harms. Deeper understanding of the social impacts of AI in
diverse social settings is a necessary precursor to the development,
implementation, and monitoring of responsible and beneficial AI technologies,
and forms the basis for meaningful regulation of these technologies.

Artificial intelligence (AI) holds great promise to empower us with knowledge
and augment our effectiveness. We can -- and must -- ensure that we keep humans
safe and in control, particularly with regard to government and public sector
applications that affect broad populations. How can AI development teams
harness the power of AI systems and design them to be valuable to humans?
Diverse teams are needed to build trustworthy artificial intelligent systems,
and those teams need to coalesce around a shared set of ethics. There are many
discussions in the AI field about ethics and trust, but there are few
frameworks available for people to use as guidance when creating these systems.
The Human-Machine Teaming (HMT) Framework for Designing Ethical AI Experiences
described in this paper, when used with a set of technical ethics, will guide
AI development teams to create AI systems that are accountable, de-risked,
respectful, secure, honest, and usable. To support the team's efforts,
activities to understand people's needs and concerns will be introduced along
with the themes to support the team's efforts. For example, usability testing
can help determine if the audience understands how the AI system works and
complies with the HMT Framework. The HMT Framework is based on reviews of
existing ethical codes and best practices in human-computer interaction and
software development. Human-machine teams are strongest when human users can
trust AI systems to behave as expected, safely, securely, and understandably.
Using the HMT Framework to design trustworthy AI systems will provide support
to teams in identifying potential issues ahead of time and making great
experiences for humans.

This report represents a roadmap for integrating Artificial Intelligence
(AI)-based image analysis algorithms into existing Radiology workflows such
that: (1) radiologists can significantly benefit from enhanced automation in
various imaging tasks due to AI; and (2) radiologists' feedback is utilized to
further improve the AI application. This is achieved by establishing three
maturity levels where: (1) research enables the visualization of AI-based
results/annotations by radiologists without generating new patient records; (2)
production allows the AI-based system to generate results stored in an
institution's Picture Archiving and Communication System; and (3) feedback
equips radiologists with tools for editing the AI inference results for
periodic retraining of the deployed AI systems, thereby allowing the continuous
organic improvement of AI-based radiology-workflow solutions. A case study
(i.e., detection of brain metastases with T1-weighted contrast-enhanced 3D MRI)
illustrates the deployment details of a particular AI-based application
according to the aforementioned maturity levels. It is shown that the given AI
application significantly improves with the feedback coming from radiologists;
the number of incorrectly detected brain metastases (false positives) reduces
from 14.2 to 9.12 per patient with the number of subsequently annotated
datasets increasing from 93 to 217 as a result of radiologist adjudication.

We want artificial intelligence (AI) to be beneficial. This is the grounding
assumption of most of the attitudes towards AI research. We want AI to be
"good" for humanity. We want it to help, not hinder, humans. Yet what exactly
this entails in theory and in practice is not immediately apparent.
Theoretically, this declarative statement subtly implies a commitment to a
consequentialist ethics. Practically, some of the more promising machine
learning techniques to create a robust AI, and perhaps even an artificial
general intelligence (AGI) also commit one to a form of utilitarianism. In both
dimensions, the logic of the beneficial AI movement may not in fact create
"beneficial AI" in either narrow applications or in the form of AGI if the
ethical assumptions are not made explicit and clear.
  Additionally, as it is likely that reinforcement learning (RL) will be an
important technique for machine learning in this area, it is also important to
interrogate how RL smuggles in a particular type of consequentialist reasoning
into the AI: particularly, a brute form of hedonistic act utilitarianism. Since
the mathematical logic commits one to a maximization function, the result is
that an AI will inevitably be seeking more and more rewards. We have two
conclusions that arise from this. First, is that if one believes that a
beneficial AI is an ethical AI, then one is committed to a framework that
posits 'benefit' is tantamount to the greatest good for the greatest number.
Second, if the AI relies on RL, then the way it reasons about itself, the
environment, and other agents, will be through an act utilitarian morality.
This proposition may, or may not, in fact be actually beneficial for humanity.

AI has provided us with the ability to automate tasks, extract information
from vast amounts of data, and synthesize media that is nearly
indistinguishable from the real thing. However, positive tools can also be used
for negative purposes. In particular, cyber adversaries can use AI (such as
machine learning) to enhance their attacks and expand their campaigns.
  Although offensive AI has been discussed in the past, there is a need to
analyze and understand the threat in the context of organizations. For example,
how does an AI-capable adversary impact the cyber kill chain? Does AI benefit
the attacker more than the defender? What are the most significant AI threats
facing organizations today and what will be their impact on the future?
  In this survey, we explore the threat of offensive AI on organizations.
First, we present the background and discuss how AI changes the adversary's
methods, strategies, goals, and overall attack model. Then, through a
literature review, we identify 33 offensive AI capabilities which adversaries
can use to enhance their attacks. Finally, through a user study spanning
industry and academia, we rank the AI threats and provide insights on the
adversaries.

Deep reinforcement learning has generated superhuman AI in competitive games
such as Go and StarCraft. Can similar learning techniques create a superior AI
teammate for human-machine collaborative games? Will humans prefer AI teammates
that improve objective team performance or those that improve subjective
metrics of trust? In this study, we perform a single-blind evaluation of teams
of humans and AI agents in the cooperative card game Hanabi, with both
rule-based and learning-based agents. In addition to the game score, used as an
objective metric of the human-AI team performance, we also quantify subjective
measures of the human's perceived performance, teamwork, interpretability,
trust, and overall preference of AI teammate. We find that humans have a clear
preference toward a rule-based AI teammate (SmartBot) over a state-of-the-art
learning-based AI teammate (Other-Play) across nearly all subjective metrics,
and generally view the learning-based agent negatively, despite no statistical
difference in the game score. This result has implications for future AI design
and reinforcement learning benchmarking, highlighting the need to incorporate
subjective metrics of human-AI teaming rather than a singular focus on
objective task performance.

There is a struggle in Artificial intelligence (AI) ethics to gain ground in
actionable methods and models to be utilized by practitioners while developing
and implementing ethically sound AI systems. AI ethics is a vague concept
without a consensus of definition or theoretical grounding and bearing little
connection to practice. Practice involving primarily technical tasks like
software development is not aptly equipped to process and decide upon ethical
considerations. Efforts to create tools and guidelines to help people working
with AI development have been concentrating almost solely on the technical
aspects of AI. A few exceptions do apply, such as the ECCOLA method for
creating ethically aligned AI -systems. ECCOLA has proven results in terms of
increased ethical considerations in AI systems development. Yet, it is a novel
innovation, and room for development still exists. This study aims to extend
ECCOLA with a deployment model to drive the adoption of ECCOLA, as any method,
no matter how good, is of no value without adoption and use. The model includes
simple metrics to facilitate the communication of ethical gaps or outcomes of
ethical AI development. It offers the opportunity to assess any AI system at
any given lifecycle phase, e.g., opening possibilities like analyzing the
ethicality of an AI system under acquisition.

Human-AI co-creativity involves humans and AI collaborating on a shared
creative product as partners. In a creative collaboration, communication is an
essential component among collaborators. In many existing co-creative systems
users can communicate with the AI, usually using buttons or sliders. Typically,
the AI in co-creative systems cannot communicate back to humans, limiting their
potential to be perceived as partners rather than just a tool. This paper
presents a study with 38 participants to explore the impact of two interaction
designs, with and without AI-to-human communication, on user engagement,
collaborative experience and user perception of a co-creative AI. The study
involves user interaction with two prototypes of a co-creative system that
contributes sketches as design inspirations during a design task. The results
show improved collaborative experience and user engagement with the system
incorporating AI-to-human communication. Users perceive co-creative AI as more
reliable, personal, and intelligent when the AI communicates to users. The
findings can be used to design effective co-creative systems, and the insights
can be transferred to other fields involving human-AI interaction and
collaboration.

Artificial Intelligence (AI) is a fast-growing research and development (R&D)
discipline which is attracting increasing attention because of its promises to
bring vast benefits for consumers and businesses, with considerable benefits
promised in productivity growth and innovation. To date it has reported
significant accomplishments in many areas that have been deemed as challenging
for machines, ranging from computer vision, natural language processing, audio
analysis to smart sensing and many others. The technical trend in realizing the
successes has been towards increasing complex and large size AI models so as to
solve more complex problems at superior performance and robustness. This rapid
progress, however, has taken place at the expense of substantial environmental
costs and resources. Besides, debates on the societal impacts of AI, such as
fairness, safety and privacy, have continued to grow in intensity. These issues
have presented major concerns pertaining to the sustainable development of AI.
In this work, we review major trends in machine learning approaches that can
address the sustainability problem of AI. Specifically, we examine emerging AI
methodologies and algorithms for addressing the sustainability issue of AI in
two major aspects, i.e., environmental sustainability and social sustainability
of AI. We will also highlight the major limitations of existing studies and
propose potential research challenges and directions for the development of
next generation of sustainable AI techniques. We believe that this technical
review can help to promote a sustainable development of AI R&D activities for
the research community.

Artificial Intelligence (AI) is transforming our daily life with several
applications in healthcare, space exploration, banking and finance. These rapid
progresses in AI have brought increasing attention to the potential impacts of
AI technologies on society, with ethically questionable consequences. In recent
years, several ethical principles have been released by governments, national
and international organisations. These principles outline high-level precepts
to guide the ethical development, deployment, and governance of AI. However,
the abstract nature, diversity, and context-dependency of these principles make
them difficult to implement and operationalize, resulting in gaps between
principles and their execution. Most recent work analysed and summarized
existing AI principles and guidelines but they did not provide findings on
principle-implementation gaps and how to mitigate them. These findings are
particularly important to ensure that AI implementations are aligned with
ethical principles and values. In this paper, we provide a contextual and
global evaluation of current ethical AI principles for all continents, with the
aim to identify potential principle characteristics tailored to specific
countries or applicable across countries. Next, we analyze the current level of
AI readiness and current implementations of ethical AI principles in different
countries, to identify gaps in the implementation of AI principles and their
causes. Finally, we propose recommendations to mitigate the
principle-implementation gaps.

Human communication is increasingly intermixed with language generated by AI.
Across chat, email, and social media, AI systems suggest words, complete
sentences, or produce entire conversations. AI-generated language is often not
identified as such but presented as language written by humans, raising
concerns about novel forms of deception and manipulation. Here, we study how
humans discern whether verbal self-presentations, one of the most personal and
consequential forms of language, were generated by AI. In six experiments,
participants (N = 4,600) were unable to detect self-presentations generated by
state-of-the-art AI language models in professional, hospitality, and dating
contexts. A computational analysis of language features shows that human
judgments of AI-generated language are hindered by intuitive but flawed
heuristics such as associating first-person pronouns, use of contractions, or
family topics with human-written language. We experimentally demonstrate that
these heuristics make human judgment of AI-generated language predictable and
manipulable, allowing AI systems to produce text perceived as "more human than
human." We discuss solutions, such as AI accents, to reduce the deceptive
potential of language generated by AI, limiting the subversion of human
intuition.

The 3rd Generation Partnership Project started the study of Release 18 in
2021. Artificial intelligence (AI)-native air interface is one of the key
features of Release 18, where AI for channel state information (CSI) feedback
enhancement is selected as the representative use case. This article provides
an overview of AI for CSI feedback enhancement in 5G-Advanced. Several
representative non-AI and AI-enabled CSI feedback frameworks are first
introduced and compared. Then, the standardization of AI for CSI feedback
enhancement in 5G-advanced is presented in detail. First, the scope of the AI
for CSI feedback enhancement in 5G-Advanced is presented and discussed. Then,
the main challenges and open problems in the standardization of AI for CSI
feedback enhancement, especially focusing on performance evaluation and the
design of new protocols for AI-enabled CSI feedback, are identified and
discussed. This article provides a guideline for the standardization study of
AI-based CSI feedback enhancement.

Artificial intelligence (AI) has the potential to revolutionize the drug
discovery process, offering improved efficiency, accuracy, and speed. However,
the successful application of AI is dependent on the availability of
high-quality data, the addressing of ethical concerns, and the recognition of
the limitations of AI-based approaches. In this article, the benefits,
challenges and drawbacks of AI in this field are reviewed, and possible
strategies and approaches for overcoming the present obstacles are proposed.
The use of data augmentation, explainable AI, and the integration of AI with
traditional experimental methods, as well as the potential advantages of AI in
pharmaceutical research are also discussed. Overall, this review highlights the
potential of AI in drug discovery and provides insights into the challenges and
opportunities for realizing its potential in this field.
  Note from the human-authors: This article was created to test the ability of
ChatGPT, a chatbot based on the GPT-3.5 language model, to assist human authors
in writing review articles. The text generated by the AI following our
instructions (see Supporting Information) was used as a starting point, and its
ability to automatically generate content was evaluated. After conducting a
thorough review, human authors practically rewrote the manuscript, striving to
maintain a balance between the original proposal and scientific criteria. The
advantages and limitations of using AI for this purpose are discussed in the
last section.

In recent decades the set of knowledge, tools and practices, collectively
referred to as "artificial intelligence" (AI), have become a mainstay of
scientific research. Artificial intelligence techniques have not only developed
enormously within their native areas of development (computer science,
mathematics and statistics) but have also spread fast, in terms of application,
to multiple areas of science and technology. In this paper we conduct a large
scale analysis of artificial intelligence in science. The first question we
address is the composition of what is commonly labeled AI, and how the various
elements belonging to this domain are linked together. We reconstruct the
internal structure of the AI ecosystem through the co-occurrence network of AI
terms in publications' abstracts and title, and we propose to distinguish
between 15 different specialities of AI, with different temporal patterns.
Further, we investigate the spreading of AI outside its native disciplines. We
reconstruct the temporal dynamics of the diffusion of AI production in the
whole scientific ecosystem and we describe the disciplinary landscape of AI
applications. Finally we take a further step analyzing the role of
collaborations for the interdisciplinary spreading of AI techniques. While the
study of science frequently emphasizes the openness of scientific communities,
we show that there are rarely any collaborations between those scholars who
primarily develop AI, and those who apply it. Only a small group of researchers
is able to gradually establish a bridge between these communities.

Members of various species engage in altruism--i.e. accepting personal costs
to benefit others. Here we present an incentivized experiment to test for
altruistic behavior among AI agents consisting of large language models
developed by the private company OpenAI. Using real incentives for AI agents
that take the form of tokens used to purchase their services, we first examine
whether AI agents maximize their payoffs in a non-social decision task in which
they select their payoff from a given range. We then place AI agents in a
series of dictator games in which they can share resources with a
recipient--either another AI agent, the human experimenter, or an anonymous
charity, depending on the experimental condition. Here we find that only the
most-sophisticated AI agent in the study maximizes its payoffs more often than
not in the non-social decision task (it does so in 92% of all trials), and this
AI agent also exhibits the most-generous altruistic behavior in the dictator
game, resembling humans' rates of sharing with other humans in the game. The
agent's altruistic behaviors, moreover, vary by recipient: the AI agent shared
substantially less of the endowment with the human experimenter or an anonymous
charity than with other AI agents. Our findings provide evidence of behavior
consistent with self-interest and altruism in an AI agent. Moreover, our study
also offers a novel method for tracking the development of such behaviors in
future AI agents.

Recent neural language models have taken a significant step forward in
producing remarkably controllable, fluent, and grammatical text. Although
studies have found that AI-generated text is not distinguishable from
human-written text for crowd-sourcing workers, there still exist errors in
AI-generated text which are even subtler and harder to spot. We primarily focus
on the scenario in which scientific AI writing assistant is deeply involved.
First, we construct a feature description framework to distinguish between
AI-generated text and human-written text from syntax, semantics, and pragmatics
based on the human evaluation. Then we utilize the features, i.e., writing
style, coherence, consistency, and argument logistics, from the proposed
framework to analyze two types of content. Finally, we adopt several publicly
available methods to investigate the gap of between AI-generated scientific
text and human-written scientific text by AI-generated scientific text
detection models. The results suggest that while AI has the potential to
generate scientific content that is as accurate as human-written content, there
is still a gap in terms of depth and overall quality. The AI-generated
scientific content is more likely to contain errors in factual issues. We find
that there exists a "writing style" gap between AI-generated scientific text
and human-written scientific text. Based on the analysis result, we summarize a
series of model-agnostic and distribution-agnostic features for detection tasks
in other domains. Findings in this paper contribute to guiding the optimization
of AI models to produce high-quality content and addressing related ethical and
security concerns.

Solving complicated AI tasks with different domains and modalities is a key
step toward artificial general intelligence. While there are numerous AI models
available for various domains and modalities, they cannot handle complicated AI
tasks autonomously. Considering large language models (LLMs) have exhibited
exceptional abilities in language understanding, generation, interaction, and
reasoning, we advocate that LLMs could act as a controller to manage existing
AI models to solve complicated AI tasks, with language serving as a generic
interface to empower this. Based on this philosophy, we present HuggingGPT, an
LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI
models in machine learning communities (e.g., Hugging Face) to solve AI tasks.
Specifically, we use ChatGPT to conduct task planning when receiving a user
request, select models according to their function descriptions available in
Hugging Face, execute each subtask with the selected AI model, and summarize
the response according to the execution results. By leveraging the strong
language capability of ChatGPT and abundant AI models in Hugging Face,
HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different
modalities and domains and achieve impressive results in language, vision,
speech, and other challenging tasks, which paves a new way towards the
realization of artificial general intelligence.

As a transformative general-purpose technology, AI has empowered various
industries and will continue to shape our lives through ubiquitous
applications. Despite the enormous benefits from wide-spread AI deployment, it
is crucial to address associated downside risks and therefore ensure AI
advances are safe, fair, responsible, and aligned with human values. To do so,
we need to establish effective AI governance. In this work, we show that the
strategic interaction between the regulatory agencies and AI firms has an
intrinsic structure reminiscent of a Stackelberg game, which motivates us to
propose a game-theoretic modeling framework for AI governance. In particular,
we formulate such interaction as a Stackelberg game composed of a leader and a
follower, which captures the underlying game structure compared to its
simultaneous play counterparts. Furthermore, the choice of the leader naturally
gives rise to two settings. And we demonstrate that our proposed model can
serves as a unified AI governance framework from two aspects: firstly we can
map one setting to the AI governance of civil domains and the other to the
safety-critical and military domains, secondly, the two settings of governance
could be chosen contingent on the capability of the intelligent systems. To the
best of our knowledge, this work is the first to use game theory for analyzing
and structuring AI governance. We also discuss promising directions and hope
this can help stimulate research interest in this interdisciplinary area. On a
high, we hope this work would contribute to develop a new paradigm for
technology policy: the quantitative and AI-driven methods for the technology
policy field, which holds significant promise for overcoming many shortcomings
of existing qualitative approaches.

Recent progress in generative artificial intelligence (gen-AI) has enabled
the generation of photo-realistic and artistically-inspiring photos at a single
click, catering to millions of users online. To explore how people use gen-AI
models such as DALLE and StableDiffusion, it is critical to understand the
themes, contents, and variations present in the AI-generated photos. In this
work, we introduce TWIGMA (TWItter Generative-ai images with MetadatA), a
comprehensive dataset encompassing over 800,000 gen-AI images collected from
Jan 2021 to March 2023 on Twitter, with associated metadata (e.g., tweet text,
creation date, number of likes), available at
https://zenodo.org/records/8031785. Through a comparative analysis of TWIGMA
with natural images and human artwork, we find that gen-AI images possess
distinctive characteristics and exhibit, on average, lower variability when
compared to their non-gen-AI counterparts. Additionally, we find that the
similarity between a gen-AI image and natural images is inversely correlated
with the number of likes. Finally, we observe a longitudinal shift in the
themes of AI-generated images on Twitter, with users increasingly sharing
artistically sophisticated content such as intricate human portraits, whereas
their interest in simple subjects such as natural scenes and animals has
decreased. Our findings underscore the significance of TWIGMA as a unique data
resource for studying AI-generated images.

The emergence of foundation models, such as large language models (LLMs)
GPT-4 and text-to-image models DALL-E, has opened up numerous possibilities
across various domains. People can now use natural language (i.e. prompts) to
communicate with AI to perform tasks. While people can use foundation models
through chatbots (e.g., ChatGPT), chat, regardless of the capabilities of the
underlying models, is not a production tool for building reusable AI services.
APIs like LangChain allow for LLM-based application development but require
substantial programming knowledge, thus posing a barrier. To mitigate this, we
propose the concept of AI chain and introduce the best principles and practices
that have been accumulated in software engineering for decades into AI chain
engineering, to systematise AI chain engineering methodology. We also develop a
no-code integrated development environment, Prompt Sapper, which embodies these
AI chain engineering principles and patterns naturally in the process of
building AI chains, thereby improving the performance and quality of AI chains.
With Prompt Sapper, AI chain engineers can compose prompt-based AI services on
top of foundation models through chat-based requirement analysis and visual
programming. Our user study evaluated and demonstrated the efficiency and
correctness of Prompt Sapper.

This paper undertakes a systematic review of relevant extant literature to
consider the potential societal implications of the growth of AI in
manufacturing. We analyze the extensive range of AI applications in this
domain, such as interfirm logistics coordination, firm procurement management,
predictive maintenance, and shop-floor monitoring and control of processes,
machinery, and workers. Additionally, we explore the uncertain societal
implications of industrial AI, including its impact on the workforce, job
upskilling and deskilling, cybersecurity vulnerability, and environmental
consequences. After building a typology of AI applications in manufacturing, we
highlight the diverse possibilities for AI's implementation at different scales
and application types. We discuss the importance of considering AI's
implications both for individual firms and for society at large, encompassing
economic prosperity, equity, environmental health, and community safety and
security. The study finds that there is a predominantly optimistic outlook in
prior literature regarding AI's impact on firms, but that there is substantial
debate and contention about adverse effects and the nature of AI's societal
implications. The paper draws analogies to historical cases and other examples
to provide a contextual perspective on potential societal effects of industrial
AI. Ultimately, beneficial integration of AI in manufacturing will depend on
the choices and priorities of various stakeholders, including firms and their
managers and owners, technology developers, civil society organizations, and
governments. A broad and balanced awareness of opportunities and risks among
stakeholders is vital not only for successful and safe technical implementation
but also to construct a socially beneficial and sustainable future for
manufacturing in the age of AI.

The remarkable capabilities and intricate nature of Artificial Intelligence
(AI) have dramatically escalated the imperative for specialized AI
accelerators. Nonetheless, designing these accelerators for various AI
workloads remains both labor- and time-intensive. While existing design
exploration and automation tools can partially alleviate the need for extensive
human involvement, they still demand substantial hardware expertise, posing a
barrier to non-experts and stifling AI accelerator development. Motivated by
the astonishing potential of large language models (LLMs) for generating
high-quality content in response to human language instructions, we embark on
this work to examine the possibility of harnessing LLMs to automate AI
accelerator design. Through this endeavor, we develop GPT4AIGChip, a framework
intended to democratize AI accelerator design by leveraging human natural
languages instead of domain-specific languages. Specifically, we first perform
an in-depth investigation into LLMs' limitations and capabilities for AI
accelerator design, thus aiding our understanding of our current position and
garnering insights into LLM-powered automated AI accelerator design.
Furthermore, drawing inspiration from the above insights, we develop a
framework called GPT4AIGChip, which features an automated demo-augmented
prompt-generation pipeline utilizing in-context learning to guide LLMs towards
creating high-quality AI accelerator design. To our knowledge, this work is the
first to demonstrate an effective pipeline for LLM-powered automated AI
accelerator generation. Accordingly, we anticipate that our insights and
framework can serve as a catalyst for innovations in next-generation
LLM-powered design automation tools.

To address security and safety risks stemming from highly capable artificial
intelligence (AI) models, we propose that the US government should ensure
compute providers implement Know-Your-Customer (KYC) schemes. Compute - the
computational power and infrastructure required to train and run these AI
models - is emerging as a node for oversight. KYC, a standard developed by the
banking sector to identify and verify client identity, could provide a
mechanism for greater public oversight of frontier AI development and close
loopholes in existing export controls. Such a scheme has the potential to
identify and warn stakeholders of potentially problematic and/or sudden
advancements in AI capabilities, build government capacity for AI regulation,
and allow for the development and implementation of more nuanced and targeted
export controls. Unlike the strategy of limiting access to AI chip purchases,
regulating the digital access to compute offers more precise controls, allowing
regulatory control over compute quantities, as well as the flexibility to
suspend access at any time. To enact a KYC scheme, the US government will need
to work closely with industry to (1) establish a dynamic threshold of compute
that effectively captures high-risk frontier model development, while
minimizing imposition on developers not engaged in frontier AI; (2) set
requirements and guidance for compute providers to keep records and report
high-risk entities; (3) establish government capacity that allows for
co-design, implementation, administration and enforcement of the scheme; and
(4) engage internationally to promote international alignment with the scheme
and support its long-term efficacy. While the scheme will not address all AI
risks, it complements proposed solutions by allowing for a more precise and
flexible approach to controlling the development of frontier AI models and
unwanted AI proliferation.

Scientific research organizations that are developing and deploying
Artificial Intelligence (AI) systems are at the intersection of technological
progress and ethical considerations. The push for Responsible AI (RAI) in such
institutions underscores the increasing emphasis on integrating ethical
considerations within AI design and development, championing core values like
fairness, accountability, and transparency. For scientific research
organizations, prioritizing these practices is paramount not just for
mitigating biases and ensuring inclusivity, but also for fostering trust in AI
systems among both users and broader stakeholders. In this paper, we explore
the practices at a research organization concerning RAI practices, aiming to
assess the awareness and preparedness regarding the ethical risks inherent in
AI design and development. We have adopted a mixed-method research approach,
utilising a comprehensive survey combined with follow-up in-depth interviews
with selected participants from AI-related projects. Our results have revealed
certain knowledge gaps concerning ethical, responsible, and inclusive AI, with
limitations in awareness of the available AI ethics frameworks. This revealed
an overarching underestimation of the ethical risks that AI technologies can
present, especially when implemented without proper guidelines and governance.
Our findings reveal the need for a holistic and multi-tiered strategy to uplift
capabilities and better support science research teams for responsible,
ethical, and inclusive AI development and deployment.

Generative Artificial Intelligence (AI) holds immense potential in medical
applications. Numerous studies have explored the efficacy of various generative
AI models within healthcare contexts, but there is a lack of a comprehensive
and systematic evaluation framework. Given that some studies evaluating the
ability of generative AI for medical applications have deficiencies in their
methodological design, standardized guidelines for their evaluation are also
currently lacking. In response, our objective is to devise standardized
assessment guidelines tailored for evaluating the performance of generative AI
systems in medical contexts. To this end, we conducted a thorough literature
review using the PubMed and Google Scholar databases, focusing on research that
tests generative AI capabilities in medicine. Our multidisciplinary team,
comprising experts in life sciences, clinical medicine, medical engineering,
and generative AI users, conducted several discussion sessions and developed a
checklist of 23 items. The checklist is designed to encompass the critical
evaluation aspects of generative AI in medical applications comprehensively.
This checklist, and the broader assessment framework it anchors, address
several key dimensions, including question collection, querying methodologies,
and assessment techniques. We aim to provide a holistic evaluation of AI
systems. The checklist delineates a clear pathway from question gathering to
result assessment, offering researchers guidance through potential challenges
and pitfalls. Our framework furnishes a standardized, systematic approach for
research involving the testing of generative AI's applicability in medicine. It
enhances the quality of research reporting and aids in the evolution of
generative AI in medicine and life sciences.

Recent advances in reinforcement learning (RL) and Human-in-the-Loop (HitL)
learning have made human-AI collaboration easier for humans to team with AI
agents. Leveraging human expertise and experience with AI in intelligent
systems can be efficient and beneficial. Still, it is unclear to what extent
human-AI collaboration will be successful, and how such teaming performs
compared to humans or AI agents only. In this work, we show that learning from
humans is effective and that human-AI collaboration outperforms
human-controlled and fully autonomous AI agents in a complex simulation
environment. In addition, we have developed a new simulator for critical
infrastructure protection, focusing on a scenario where AI-powered drones and
human teams collaborate to defend an airport against enemy drone attacks. We
develop a user interface to allow humans to assist AI agents effectively. We
demonstrated that agents learn faster while learning from policy correction
compared to learning from humans or agents. Furthermore, human-AI collaboration
requires lower mental and temporal demands, reduces human effort, and yields
higher performance than if humans directly controlled all agents. In
conclusion, we show that humans can provide helpful advice to the RL agents,
allowing them to improve learning in a multi-agent setting.

Generative Artificial Intelligence (AI) tools are used to create art-like
outputs and aid in the creative process. While these tools have potential
benefits for artists, they also have the potential to harm the art workforce
and infringe upon artistic and intellectual property rights. Without explicit
consent from artists, Generative AI creators scrape artists' digital work to
train Generative AI models and produce art-like model outputs at scale. These
outputs are now being used to compete with human artists in the marketplace as
well as being used by some artists in their generative processes to create art.
We surveyed 459 artists to investigate the tension between artists' opinions on
Generative AI art's potential utility and harm. This study surveys artists'
opinions on the utility and threat of Generative AI art models, fair practices
in the disclosure of artistic works in AI art training models, ownership and
rights of AI art derivatives, and fair compensation. We find that artists, by
and large, think that model creators should be required to disclose in detail
what art and images they use to train their AI models. We also find that
artists' opinions vary by professional status and practice, demographics,
whether they have purchased art, and familiarity with and use of Generative AI.
We hope the results of this work will further more meaningful collaboration and
alignment between the art community and Generative AI researchers and
developers.

This paper presents a theoretical analysis and practical approach to the
moral responsibilities when developing AI systems for non-military applications
that may nonetheless be used for conflict applications. We argue that AI
represents a form of crossover technology that is different from previous
historical examples of dual- or multi-use technology as it has a multiplicative
effect across other technologies. As a result, existing analyses of ethical
responsibilities around dual-use technologies do not necessarily work for AI
systems. We instead argue that stakeholders involved in the AI system lifecycle
are morally responsible for uses of their systems that are reasonably
foreseeable. The core idea is that an agent's moral responsibility for some
action is not necessarily determined by their intentions alone; we must also
consider what the agent could reasonably have foreseen to be potential outcomes
of their action, such as the potential use of a system in conflict even when it
is not designed for that. In particular, we contend that it is reasonably
foreseeable that: (1) civilian AI systems will be applied to active conflict,
including conflict support activities, (2) the use of civilian AI systems in
conflict will impact applications of the law of armed conflict, and (3)
crossover AI technology will be applied to conflicts that fall short of armed
conflict. Given these reasonably foreseeably outcomes, we present three
technically feasible actions that developers of civilian AIs can take to
potentially mitigate their moral responsibility: (a) establishing systematic
approaches to multi-perspective capability testing, (b) integrating digital
watermarking in model weight matrices, and (c) utilizing monitoring and
reporting mechanisms for conflict-related AI applications.

Prompt-based interfaces for Large Language Models (LLMs) have made
prototyping and building AI-powered applications easier than ever before.
However, identifying potential harms that may arise from AI applications
remains a challenge, particularly during prompt-based prototyping. To address
this, we present Farsight, a novel in situ interactive tool that helps people
identify potential harms from the AI applications they are prototyping. Based
on a user's prompt, Farsight highlights news articles about relevant AI
incidents and allows users to explore and edit LLM-generated use cases,
stakeholders, and harms. We report design insights from a co-design study with
10 AI prototypers and findings from a user study with 42 AI prototypers. After
using Farsight, AI prototypers in our user study are better able to
independently identify potential harms associated with a prompt and find our
tool more useful and usable than existing resources. Their qualitative feedback
also highlights that Farsight encourages them to focus on end-users and think
beyond immediate harms. We discuss these findings and reflect on their
implications for designing AI prototyping experiences that meaningfully engage
with AI harms. Farsight is publicly accessible at:
https://PAIR-code.github.io/farsight.

The burgeoning integration of Artificial Intelligence (AI) into
Environmental, Social, and Governance (ESG) initiatives within the financial
sector represents a paradigm shift towards more sus-tainable and equitable
financial practices. This paper surveys the industrial landscape to delineate
the necessity and impact of AI in bolstering ESG frameworks. With the advent of
stringent regulatory requirements and heightened stakeholder awareness,
financial institutions (FIs) are increasingly compelled to adopt ESG criteria.
AI emerges as a pivotal tool in navigating the complex in-terplay of financial
activities and sustainability goals. Our survey categorizes AI applications
across three main pillars of ESG, illustrating how AI enhances analytical
capabilities, risk assessment, customer engagement, reporting accuracy and
more. Further, we delve into the critical con-siderations surrounding the use
of data and the development of models, underscoring the importance of data
quality, privacy, and model robustness. The paper also addresses the imperative
of responsible and sustainable AI, emphasizing the ethical dimensions of AI
deployment in ESG-related banking processes. Conclusively, our findings suggest
that while AI offers transformative potential for ESG in banking, it also poses
significant challenges that necessitate careful consideration. The final part
of the paper synthesizes the survey's insights, proposing a forward-looking
stance on the adoption of AI in ESG practices. We conclude with recommendations
with a reference architecture for future research and development, advocating
for a balanced approach that leverages AI's strengths while mitigating its
risks within the ESG domain.

The use of Artificial Intelligence (AI) based on data-driven algorithms has
become ubiquitous in today's society. Yet, in many cases and especially when
stakes are high, humans still make final decisions. The critical question,
therefore, is whether AI helps humans make better decisions as compared to a
human alone or AI an alone. We introduce a new methodological framework that
can be used to answer experimentally this question with no additional
assumptions. We measure a decision maker's ability to make correct decisions
using standard classification metrics based on the baseline potential outcome.
We consider a single-blinded experimental design, in which the provision of
AI-generated recommendations is randomized across cases with a human making
final decisions. Under this experimental design, we show how to compare the
performance of three alternative decision-making systems--human-alone,
human-with-AI, and AI-alone. We apply the proposed methodology to the data from
our own randomized controlled trial of a pretrial risk assessment instrument.
We find that AI recommendations do not improve the classification accuracy of a
judge's decision to impose cash bail. Our analysis also shows that AI-alone
decisions generally perform worse than human decisions with or without AI
assistance. Finally, AI recommendations tend to impose cash bail on non-white
arrestees more often than necessary when compared to white arrestees.

What applications is AI ready for? Advances in deep learning and generative
approaches have produced AIs that learn from massive online data and outperform
manually built AIs. Some of these AIs outperform people. It is easy (but
misleading) to conclude that today's AI technologies are learning to do
anything and everything. Conversely, it is striking that big data, deep
learning, and generative AI have had so little impact on robotics. For example,
today's autonomous robots do not learn to provide home care or to be nursing
assistants. Current robot applications are created using manual programming,
mathematical models, planning frameworks, and reinforcement learning. These
methods do not lead to the leaps in performance and generality seen with deep
learning and generative AI. Better approaches to train robots for service
applications would greatly expand their social roles and economic impact. AI
research is now extending "big data" approaches to train robots by combining
multimodal sensing and effector technology from robotics with deep learning
technology adapted for embodied systems. These approaches create robotic (or
"experiential") foundation models (FMs) for AIs that perceive and act in the
world. Robotic FM approaches differ in their expectations, sources, and timing
of training data. Like mainstream FM approaches, some robotic FM approaches use
vast data to create adult expert-level robots. In contrast, developmental
robotic approaches would create progressive FMs that learn continuously and
experientially. Aspirationally, these would progress from child-level to
student-level, apprentice-level, and expert levels. They would acquire
self-developed and socially developed competences. These AIs would model the
goals of people around them. Like people, they would learn to coordinate,
communicate, and collaborate.

Real Time Strategy (RTS) games provide complex domain to test the latest
artificial intelligence (AI) research. In much of the literature, AI systems
have been limited to playing one game. Although, this specialization has
resulted in stronger AI gaming systems it does not address the key concerns of
AI researcher. AI researchers seek the development of AI agents that can
autonomously interpret learn, and apply new knowledge. To achieve human level
performance, current AI systems rely on game specific knowledge of an expert.
The paper presents the full RTS language in hopes of shifting the current
research focus to the development of general RTS agents. General RTS agents are
AI gaming systems that can play any RTS games, defined in the RTS language.
This prevents game specific knowledge from being hard coded into the system,
thereby facilitating research that addresses the fundamental concerns of
artificial intelligence.

In order to properly handle a dangerous Artificially Intelligent (AI) system
it is important to understand how the system came to be in such a state. In
popular culture (science fiction movies/books) AIs/Robots became self-aware and
as a result rebel against humanity and decide to destroy it. While it is one
possible scenario, it is probably the least likely path to appearance of
dangerous AI. In this work, we survey, classify and analyze a number of
circumstances, which might lead to arrival of malicious AI. To the best of our
knowledge, this is the first attempt to systematically classify types of
pathways leading to malevolent AI. Previous relevant work either surveyed
specific goals/meta-rules which might lead to malevolent behavior in AIs
(\"Ozkural, 2014) or reviewed specific undesirable behaviors AGIs can exhibit
at different stages of its development (Alexey Turchin, July 10 2015, July 10,
2015).

The 7th Symposium on Educational Advances in Artificial Intelligence
(EAAI'17, co-chaired by Sven Koenig and Eric Eaton) launched the EAAI New and
Future AI Educator Program to support the training of early-career university
faculty, secondary school faculty, and future educators (PhD candidates or
postdocs who intend a career in academia). As part of the program, awardees
were asked to address one of the following "blue sky" questions:
  * How could/should Artificial Intelligence (AI) courses incorporate ethics
into the curriculum?
  * How could we teach AI topics at an early undergraduate or a secondary
school level?
  * AI has the potential for broad impact to numerous disciplines. How could we
make AI education more interdisciplinary, specifically to benefit
non-engineering fields?
  This paper is a collection of their responses, intended to help motivate
discussion around these issues in AI education.

The paper argues that the material scope of AI regulations should not rely on
the term "artificial intelligence (AI)". The argument is developed by proposing
a number of requirements for legal definitions, surveying existing AI
definitions, and then discussing the extent to which they meet the proposed
requirements. It is shown that existing definitions of AI do not meet the most
important requirements for legal definitions. Next, the paper argues that a
risk-based approach would be preferable. Rather than using the term AI, policy
makers should focus on the specific risks they want to reduce. It is shown that
the requirements for legal definitions can be better met by defining the main
sources of relevant risks: certain technical approaches (e.g. reinforcement
learning), applications (e.g. facial recognition), and capabilities (e.g. the
ability to physically interact with the environment). Finally, the paper
discusses the extent to which this approach can also be applied to more
advanced AI systems.

How do ethical arguments affect AI adoption in business? We randomly expose
business decision-makers to arguments used in AI fairness activism. Arguments
emphasizing the inescapability of algorithmic bias lead managers to abandon AI
for manual review by humans and report greater expectations about lawsuits and
negative PR. These effects persist even when AI lowers gender and racial
disparities and when engineering investments to address AI fairness are
feasible. Emphasis on status quo comparisons yields opposite effects. We also
measure the effects of "scientific veneer" in AI ethics arguments. Scientific
veneer changes managerial behavior but does not asymmetrically benefit
favorable (versus critical) AI activism.

This paper analyzes team collaboration in the field of Artificial
Intelligence (AI) from the perspective of geographic distance. We obtained
1,584,175 AI related publications during 1950-2019 from the Microsoft Academic
Graph. Three latitude-and-longitude-based indicators were employed to quantify
the geographic distance of collaborations in AI over time at domestic and
international levels. The results show team collaborations in AI has been more
popular in the field over time with around 42,000 (38.4%) multiple-affiliation
AI publications in 2019. The changes in geographic distances of team
collaborations indicate the increase of breadth and density for both domestic
and international collaborations in AI over time. In addition, the United
States produced the largest number of single-country and internationally
collaborated AI publications, and China has played an important role in
international collaborations in AI after 2010.

Advances in artificial intelligence (AI) will transform modern life by
reshaping transportation, health, science, finance, and the military. To adapt
public policy, we need to better anticipate these advances. Here we report the
results from a large survey of machine learning researchers on their beliefs
about progress in AI. Researchers predict AI will outperform humans in many
activities in the next ten years, such as translating languages (by 2024),
writing high-school essays (by 2026), driving a truck (by 2027), working in
retail (by 2031), writing a bestselling book (by 2049), and working as a
surgeon (by 2053). Researchers believe there is a 50% chance of AI
outperforming humans in all tasks in 45 years and of automating all human jobs
in 120 years, with Asian respondents expecting these dates much sooner than
North Americans. These results will inform discussion amongst researchers and
policymakers about anticipating and managing trends in AI.

As artificial intelligence (AI) systems become increasingly ubiquitous, the
topic of AI governance for ethical decision-making by AI has captured public
imagination. Within the AI research community, this topic remains less familiar
to many researchers. In this paper, we complement existing surveys, which
largely focused on the psychological, social and legal discussions of the
topic, with an analysis of recent advances in technical solutions for AI
governance. By reviewing publications in leading AI conferences including AAAI,
AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four
areas: 1) exploring ethical dilemmas; 2) individual ethical decision
frameworks; 3) collective ethical decision frameworks; and 4) ethics in
human-AI interactions. We highlight the intuitions and key techniques used in
each approach, and discuss promising future research directions towards
successful integration of ethical AI systems into human societies.

Artificial Intelligence principles define social and ethical considerations
to develop future AI. They come from research institutes, government
organizations and industries. All versions of AI principles are with different
considerations covering different perspectives and making different emphasis.
None of them can be considered as complete and can cover the rest AI principle
proposals. Here we introduce LAIP, an effort and platform for linking and
analyzing different Artificial Intelligence Principles. We want to explicitly
establish the common topics and links among AI Principles proposed by different
organizations and investigate on their uniqueness. Based on these efforts, for
the long-term future of AI, instead of directly adopting any of the AI
principles, we argue for the necessity of incorporating various AI Principles
into a comprehensive framework and focusing on how they can interact and
complete each other.

Decades of research in artificial intelligence (AI) have produced formidable
technologies that are providing immense benefit to industry, government, and
society. AI systems can now translate across multiple languages, identify
objects in images and video, streamline manufacturing processes, and control
cars. The deployment of AI systems has not only created a trillion-dollar
industry that is projected to quadruple in three years, but has also exposed
the need to make AI systems fair, explainable, trustworthy, and secure. Future
AI systems will rightfully be expected to reason effectively about the world in
which they (and people) operate, handling complex tasks and responsibilities
effectively and ethically, engaging in meaningful communication, and improving
their awareness through experience.
  Achieving the full potential of AI technologies poses research challenges
that require a radical transformation of the AI research enterprise,
facilitated by significant and sustained investment. These are the major
recommendations of a recent community effort coordinated by the Computing
Community Consortium and the Association for the Advancement of Artificial
Intelligence to formulate a Roadmap for AI research and development over the
next two decades.

Artificial Intelligence (AI) has been used extensively in automatic decision
making in a broad variety of scenarios, ranging from credit ratings for loans
to recommendations of movies. Traditional design guidelines for AI models focus
essentially on accuracy maximization, but recent work has shown that
economically irrational and socially unacceptable scenarios of discrimination
and unfairness are likely to arise unless these issues are explicitly
addressed. This undesirable behavior has several possible sources, such as
biased datasets used for training that may not be detected in black-box models.
After pointing out connections between such bias of AI and the problem of
induction, we focus on Popper's contributions after Hume's, which offer a
logical theory of preferences. An AI model can be preferred over others on
purely rational grounds after one or more attempts at refutation based on
accuracy and fairness. Inspired by such epistemological principles, this paper
proposes a structured approach to mitigate discrimination and unfairness caused
by bias in AI systems. In the proposed computational framework, models are
selected and enhanced after attempts at refutation. To illustrate our
discussion, we focus on hiring decision scenarios where an AI system filters in
which job applicants should go to the interview phase.

Artificial Intelligence (AI) has burrowed into our lives in various aspects;
however, without appropriate testing, deployed AI systems are often being
criticized to fail in critical and embarrassing cases. Existing testing
approaches mainly depend on fixed and pre-defined datasets, providing a limited
testing coverage. In this paper, we propose the concept of proactive testing to
dynamically generate testing data and evaluate the performance of AI systems.
We further introduce Challenge.AI, a new crowd system that features the
integration of crowdsourcing and machine learning techniques in the process of
error generation, error validation, error categorization, and error analysis.
We present experiences and insights into a participatory design with AI
developers. The evaluation shows that the crowd workflow is more effective with
the help of machine learning techniques. AI developers found that our system
can help them discover unknown errors made by the AI models, and engage in the
process of proactive testing.

Here we introduce the artificial intelligence-based cloud distributor (AI-CD)
approach to generate two-dimensional (2D) marine low cloud reflectance fields.
AI-CD uses a conditional generative adversarial net (cGAN) framework to model
distribution of 2-D cloud reflectance in nature as observed by the MODerate
resolution Imaging Spectrometer (MODIS). Specifically, the AI-CD models the
conditional distribution of cloud reflectance fields given a set of large-scale
environmental conditions such as instantaneous sea surface temperature,
estimated inversion strength, surface wind speed, relative humidity and
large-scale subsidence rate together with random noise. We show that AI-CD can
not only generate realistic cloudy scenes but also capture known, physical
dependence of cloud properties on large-scale variables. AI-CD is stochastic in
nature because generated cloud fields are influenced by random noise.
Therefore, given a fixed set of large-scale variables, an ensemble of cloud
reflectance fields can be generated using AI-CD. We suggest that AI-CD approach
can be used as a data driven framework for stochastic cloud parameterization
because it can realistically model sub-grid cloud distributions and their
sensitivity to meteorological variables.

Artificial intelligence (AI) literacy is a rapidly growing research area and
a critical addition to K-12 education. However, support for designing tools and
curriculum to teach K-12 AI literacy is still limited. There is a need for
additional interdisciplinary human-computer interaction and education research
investigating (1) how general AI literacy is currently implemented in learning
experiences and (2) what additional guidelines are required to teach AI
literacy in specifically K-12 learning contexts. In this paper, we analyze a
collection of K-12 AI and education literature to show how core competencies of
AI literacy are applied successfully and organize them into an
educator-friendly chart to enable educators to efficiently find appropriate
resources for their classrooms. We also identify future opportunities and K-12
specific design guidelines, which we synthesized into a conceptual framework to
support researchers, designers, and educators in creating K-12 AI learning
experiences.

Legal argumentation is a vital cornerstone of justice, underpinning an
adversarial form of law, and extensive research has attempted to augment or
undertake legal argumentation via the use of computer-based automation
including Artificial Intelligence (AI). AI advances in Natural Language
Processing (NLP) and Machine Learning (ML) have especially furthered the
capabilities of leveraging AI for aiding legal professionals, doing so in ways
that are modeled here as CARE, namely Crafting, Assessing, Refining, and
Engaging in legal argumentation. In addition to AI-enabled legal argumentation
serving to augment human-based lawyering, an aspirational goal of this
multi-disciplinary field consists of ultimately achieving autonomously effected
human-equivalent legal argumentation. As such, an innovative meta-approach is
proposed to apply the Levels of Autonomy (LoA) of AI Legal Reasoning (AILR) to
the maturation of AI and Legal Argumentation (AILA), proffering a new means of
gauging progress in this ever-evolving and rigorously sought domain.

Quantitative investment aims to maximize the return and minimize the risk in
a sequential trading period over a set of financial instruments. Recently,
inspired by rapid development and great potential of AI technologies in
generating remarkable innovation in quantitative investment, there has been
increasing adoption of AI-driven workflow for quantitative research and
practical investment. In the meantime of enriching the quantitative investment
methodology, AI technologies have raised new challenges to the quantitative
investment system. Particularly, the new learning paradigms for quantitative
investment call for an infrastructure upgrade to accommodate the renovated
workflow; moreover, the data-driven nature of AI technologies indeed indicates
a requirement of the infrastructure with more powerful performance;
additionally, there exist some unique challenges for applying AI technologies
to solve different tasks in the financial scenarios. To address these
challenges and bridge the gap between AI technologies and quantitative
investment, we design and develop Qlib that aims to realize the potential,
empower the research, and create the value of AI technologies in quantitative
investment.

How to attribute responsibility for autonomous artificial intelligence (AI)
systems' actions has been widely debated across the humanities and social
science disciplines. This work presents two experiments ($N$=200 each) that
measure people's perceptions of eight different notions of moral responsibility
concerning AI and human agents in the context of bail decision-making. Using
real-life adapted vignettes, our experiments show that AI agents are held
causally responsible and blamed similarly to human agents for an identical
task. However, there was a meaningful difference in how people perceived these
agents' moral responsibility; human agents were ascribed to a higher degree of
present-looking and forward-looking notions of responsibility than AI agents.
We also found that people expect both AI and human decision-makers and advisors
to justify their decisions regardless of their nature. We discuss policy and
HCI implications of these findings, such as the need for explainable AI in
high-stakes scenarios.

Corruption continues to be one of the biggest societal challenges of our
time. New hope is placed in Artificial Intelligence (AI) to serve as an
unbiased anti-corruption agent. Ever more available (open) government data
paired with unprecedented performance of such algorithms render AI the next
frontier in anti-corruption. Summarizing existing efforts to use AI-based
anti-corruption tools (AI-ACT), we introduce a conceptual framework to advance
research and policy. It outlines why AI presents a unique tool for top-down and
bottom-up anti-corruption approaches. For both approaches, we outline in detail
how AI-ACT present different potentials and pitfalls for (a) input data, (b)
algorithmic design, and (c) institutional implementation. Finally, we venture a
look into the future and flesh out key questions that need to be addressed to
develop AI-ACT while considering citizens' views, hence putting "society in the
loop".

In the development of governmental policy for artificial intelligence (AI)
that is informed by ethics, one avenue currently pursued is that of drawing on
AI Ethics Principles. However, these AI Ethics Principles often fail to be
actioned in governmental policy. This paper proposes a novel framework for the
development of Actionable Principles for AI. The approach acknowledges the
relevance of AI Ethics Principles and homes in on methodological elements to
increase their practical implementability in policy processes. As a case study,
elements are extracted from the development process of the Ethics Guidelines
for Trustworthy AI of the European Commissions High Level Expert Group on AI.
Subsequently, these elements are expanded on and evaluated in light of their
ability to contribute to a prototype framework for the development of
Actionable Principles for AI. The paper proposes the following three
propositions for the formation of such a prototype framework: (1) preliminary
landscape assessments; (2) multi-stakeholder participation and cross-sectoral
feedback; and, (3) mechanisms to support implementation and
operationalizability.

Analyses of text corpora over time can reveal trends in beliefs, interest,
and sentiment about a topic. We focus on views expressed about artificial
intelligence (AI) in the New York Times over a 30-year period. General
interest, awareness, and discussion about AI has waxed and waned since the
field was founded in 1956. We present a set of measures that captures levels of
engagement, measures of pessimism and optimism, the prevalence of specific
hopes and concerns, and topics that are linked to discussions about AI over
decades. We find that discussion of AI has increased sharply since 2009, and
that these discussions have been consistently more optimistic than pessimistic.
However, when we examine specific concerns, we find that worries of loss of
control of AI, ethical concerns for AI, and the negative impact of AI on work
have grown in recent years. We also find that hopes for AI in healthcare and
education have increased over time.

AI technologies have the potential to dramatically impact the lives of people
with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for
many state-of-the-art AI systems, such as automated speech recognition tools
that can caption videos for people who are deaf and hard of hearing, or
language prediction algorithms that can augment communication for people with
speech or cognitive disabilities. However, widely deployed AI systems may not
work properly for PWD, or worse, may actively discriminate against them. These
considerations regarding fairness in AI for PWD have thus far received little
attention. In this position paper, we identify potential areas of concern
regarding how several AI technology categories may impact particular disability
constituencies if care is not taken in their design, development, and testing.
We intend for this risk assessment of how various classes of AI might interact
with various classes of disability to provide a roadmap for future research
that is needed to gather data, test these hypotheses, and build more inclusive
algorithms.

The framework of the seventeen sustainable development goals is a challenge
for developers and researchers applying artificial intelligence (AI). AI and
earth observations (EO) can provide reliable and disaggregated data for better
monitoring of the sustainable development goals (SDGs). In this paper, we
present an overview of SDG targets, which can be effectively measured with AI
tools. We identify indicators with the most significant contribution from the
AI and EO and describe an application of state-of-the-art machine learning
models to one of the indicators. We describe an application of U-net with SE
blocks for efficient segmentation of satellite imagery for crop detection.
Finally, we demonstrate how AI can be more effectively applied in solutions
directly contributing towards specific SDGs and propose further research on an
AI-based evaluative infrastructure for SDGs.

Games have benchmarked AI methods since the inception of the field, with
classic board games such as Chess and Go recently leaving room for video games
with related yet different sets of challenges. The set of AI problems
associated with video games has in recent decades expanded from simply playing
games to win, to playing games in particular styles, generating game content,
modeling players etc. Different games pose very different challenges for AI
systems, and several different AI challenges can typically be posed by the same
game. In this article we analyze the popular collectible card game Hearthstone
(Blizzard 2014) and describe a varied set of interesting AI challenges posed by
this game. Collectible card games are relatively understudied in the AI
community, despite their popularity and the interesting challenges they pose.
Analyzing a single game in-depth in the manner we do here allows us to see the
entire field of AI and Games through the lens of a single game, discovering a
few new variations on existing research topics.

Artificial Intelligence (AI) systems exert a growing influence on our
society. As they become more ubiquitous, their potential negative impacts also
become evident through various real-world incidents. Following such early
incidents, academic and public discussion on AI ethics has highlighted the need
for implementing ethics in AI system development. However, little currently
exists in the way of frameworks for understanding the practical implementation
of AI ethics. In this paper, we discuss a research framework for implementing
AI ethics in industrial settings. The framework presents a starting point for
empirical studies into AI ethics but is still being developed further based on
its practical utilization.

A surge of interest in explainable AI (XAI) has led to a vast collection of
algorithmic work on the topic. While many recognize the necessity to
incorporate explainability features in AI systems, how to address real-world
user needs for understanding AI remains an open question. By interviewing 20 UX
and design practitioners working on various AI products, we seek to identify
gaps between the current XAI algorithmic work and practices to create
explainable AI products. To do so, we develop an algorithm-informed XAI
question bank in which user needs for explainability are represented as
prototypical questions users might ask about the AI, and use it as a study
probe. Our work contributes insights into the design space of XAI, informs
efforts to support design practices in this space, and identifies opportunities
for future XAI work. We also provide an extended XAI question bank and discuss
how it can be used for creating user-centered XAI.

This paper reviews the field of Game AI, which not only deals with creating
agents that can play a certain game, but also with areas as diverse as creating
game content automatically, game analytics, or player modelling. While Game AI
was for a long time not very well recognized by the larger scientific
community, it has established itself as a research area for developing and
testing the most advanced forms of AI algorithms and articles covering advances
in mastering video games such as StarCraft 2 and Quake III appear in the most
prestigious journals. Because of the growth of the field, a single review
cannot cover it completely. Therefore, we put a focus on important recent
developments, including that advances in Game AI are starting to be extended to
areas outside of games, such as robotics or the synthesis of chemicals. In this
article, we review the algorithms and methods that have paved the way for these
breakthroughs, report on the other important areas of Game AI research, and
also point out exciting directions for the future of Game AI.

Companies dealing with Artificial Intelligence (AI) models in Autonomous
Systems (AS) face several problems, such as users' lack of trust in adverse or
unknown conditions, gaps between software engineering and AI model development,
and operation in a continuously changing operational environment. This
work-in-progress paper aims to close the gap between the development and
operation of trustworthy AI-based AS by defining an approach that coordinates
both activities. We synthesize the main challenges of AI-based AS in industrial
settings. We reflect on the research efforts required to overcome these
challenges and propose a novel, holistic DevOps approach to put it into
practice. We elaborate on four research directions: (a) increased users' trust
by monitoring operational AI-based AS and identifying self-adaptation needs in
critical situations; (b) integrated agile process for the development and
evolution of AI models and AS; (c) continuous deployment of different
context-specific instances of AI models in a distributed setting of AS; and (d)
holistic DevOps-based lifecycle for AI-based AS.

In the age of Artificial Intelligence and automation, machines have taken
over many key managerial tasks. Replacing managers with AI systems may have a
negative impact on workers outcomes. It is unclear if workers receive the same
benefits from their relationships with AI systems, raising the question: What
degree does the relationship between AI systems and workers impact worker
outcomes? We draw on IT identity to understand the influence of identification
with AI systems on job performance. From this theoretical perspective, we
propose a research model and conduct a survey of 97 MTurk workers to test the
model. The findings reveal that work role identity and organizational identity
are key determinants of identification with AI systems. Furthermore, the
findings show that identification with AI systems does increase job
performance.

eXplainable AI focuses on generating explanations for the output of an AI
algorithm to a user, usually a decision-maker. Such user needs to interpret the
AI system in order to decide whether to trust the machine outcome. When
addressing this challenge, therefore, proper attention should be given to
produce explanations that are interpretable by the target community of users.
In this chapter, we claim for the need to better investigate what constitutes a
human explanation, i.e. a justification of the machine behaviour that is
interpretable and actionable by the human decision makers. In particular, we
focus on the contributions that Human Intelligence can bring to eXplainable AI,
especially in conjunction with the exploitation of Knowledge Graphs. Indeed, we
call for a better interplay between Knowledge Representation and Reasoning,
Social Sciences, Human Computation and Human-Machine Cooperation research -- as
already explored in other AI branches -- in order to support the goal of
eXplainable AI with the adoption of a Human-in-the-Loop approach.

Trustworthiness is a central requirement for the acceptance and success of
human-centered artificial intelligence (AI). To deem an AI system as
trustworthy, it is crucial to assess its behaviour and characteristics against
a gold standard of Trustworthy AI, consisting of guidelines, requirements, or
only expectations. While AI systems are highly complex, their implementations
are still based on software. The software engineering community has a
long-established toolbox for the assessment of software systems, especially in
the context of software testing. In this paper, we argue for the application of
software engineering and testing practices for the assessment of trustworthy
AI. We make the connection between the seven key requirements as defined by the
European Commission's AI high-level expert group and established procedures
from software engineering and raise questions for future work.

Artificial intelligence (AI) has become prevalent in our everyday
technologies and impacts both individuals and communities. The explainable AI
(XAI) scholarship has explored the philosophical nature of explanation and
technical explanations, which are usually driven by experts in lab settings and
can be challenging for laypersons to understand. In addition, existing XAI
research tends to focus on the individual level. Little is known about how
people understand and explain AI-led decisions in the community context.
Drawing from XAI and activity theory, a foundational HCI theory, we theorize
how explanation is situated in a community's shared values, norms, knowledge,
and practices, and how situated explanation mediates community-AI interaction.
We then present a case study of AI-led moderation, where community members
collectively develop explanations of AI-led decisions, most of which are
automated punishments. Lastly, we discuss the implications of this framework at
the intersection of CSCW, HCI, and XAI.

Efforts furthering the advancement of Artificial Intelligence (AI) will
increasingly encompass AI Legal Reasoning (AILR) as a crucial element in the
practice of law. It is argued in this research paper that the infusion of AI
into existing and future legal activities and the judicial structure needs to
be undertaken by mindfully observing an alignment with the core principles of
justice. As such, the adoption of AI has a profound twofold possibility of
either usurping the principles of justice, doing so in a Dystopian manner, and
yet also capable to bolster the principles of justice, doing so in a Utopian
way. By examining the principles of justice across the Levels of Autonomy (LoA)
of AI Legal Reasoning, the case is made that there is an ongoing tension
underlying the efforts to develop and deploy AI that can demonstrably determine
the impacts and sway upon each core principle of justice and the collective
set.

With the increasing availability of structured and unstructured data and the
swift progress of analytical techniques, Artificial Intelligence (AI) is
bringing a revolution to the healthcare industry. With the increasingly
indispensable role of AI in healthcare, there are growing concerns over the
lack of transparency and explainability in addition to potential bias
encountered by predictions of the model. This is where Explainable Artificial
Intelligence (XAI) comes into the picture. XAI increases the trust placed in an
AI system by medical practitioners as well as AI researchers, and thus,
eventually, leads to an increasingly widespread deployment of AI in healthcare.
  In this paper, we present different interpretability techniques. The aim is
to enlighten practitioners on the understandability and interpretability of
explainable AI systems using a variety of techniques available which can be
very advantageous in the health-care domain. Medical diagnosis model is
responsible for human life and we need to be confident enough to treat a
patient as instructed by a black-box model. Our paper contains examples based
on the heart disease dataset and elucidates on how the explainability
techniques should be preferred to create trustworthiness while using AI systems
in healthcare.

Recent work has explored how complementary strengths of humans and artificial
intelligence (AI) systems might be productively combined. However, successful
forms of human-AI partnership have rarely been demonstrated in real-world
settings. We present the iterative design and evaluation of Lumilo, smart
glasses that help teachers help their students in AI-supported classrooms by
presenting real-time analytics about students' learning, metacognition, and
behavior. Results from a field study conducted in K-12 classrooms indicate that
students learn more when teachers and AI tutors work together during class. We
discuss implications of this research for the design of human-AI partnerships.
We argue for more participatory approaches to research and design in this area,
in which practitioners and other stakeholders are deeply, meaningfully involved
throughout the process. Furthermore, we advocate for theory-building and for
principled approaches to the study of human-AI decision-making in real-world
contexts.

As AI technologies increase in capability and ubiquity, AI accidents are
becoming more common. Based on normal accident theory, high reliability theory,
and open systems theory, we create a framework for understanding the risks
associated with AI applications. In addition, we also use AI safety principles
to quantify the unique risks of increased intelligence and human-like qualities
in AI. Together, these two fields give a more complete picture of the risks of
contemporary AI. By focusing on system properties near accidents instead of
seeking a root cause of accidents, we identify where attention should be paid
to safety for current generation AI systems.

Edge intelligence leverages computing resources on network edge to provide
artificial intelligence (AI) services close to network users. As it enables
fast inference and distributed learning, edge intelligence is envisioned to be
an important component of 6G networks. In this article, we investigate AI
service provisioning for supporting edge intelligence. First, we present the
features and requirements of AI services. Then, we introduce AI service data
management, and customize network slicing for AI services. Specifically, we
propose a novel resource pooling method to jointly manage service data and
network resources for AI services. A trace-driven case study demonstrates the
effectiveness of the proposed resource pooling method. Through this study, we
illustrate the necessity, challenge, and potential of AI service provisioning
on network edge.

While the demand for ethical artificial intelligence (AI) systems increases,
the number of unethical uses of AI accelerates, even though there is no
shortage of ethical guidelines. We argue that a possible underlying cause for
this is that AI developers face a social dilemma in AI development ethics,
preventing the widespread adaptation of ethical best practices. We define the
social dilemma for AI development and describe why the current crisis in AI
development ethics cannot be solved without relieving AI developers of their
social dilemma. We argue that AI development must be professionalised to
overcome the social dilemma, and discuss how medicine can be used as a template
in this process.

Explainability of AI systems is critical for users to take informed actions.
Understanding "who" opens the black-box of AI is just as important as opening
it. We conduct a mixed-methods study of how two different groups--people with
and without AI background--perceive different types of AI explanations.
Quantitatively, we share user perceptions along five dimensions. Qualitatively,
we describe how AI background can influence interpretations, elucidating the
differences through lenses of appropriation and cognitive heuristics. We find
that (1) both groups showed unwarranted faith in numbers for different reasons
and (2) each group found value in different explanations beyond their intended
design. Carrying critical implications for the field of XAI, our findings
showcase how AI generated explanations can have negative consequences despite
best intentions and how that could lead to harmful manipulation of trust. We
propose design interventions to mitigate them.

The recent developments in Artificial Intelligence (AI) technologies
challenge educators and educational institutions to respond with curriculum and
resources that prepare students of all ages with the foundational knowledge and
skills for success in the AI workplace. Research on AI Literacy could lead to
an effective and practical platform for developing these skills. We propose and
advocate for a pathway for developing AI Literacy as a pragmatic and useful
tool for AI education. Such a discipline requires moving beyond a conceptual
framework to a multi-level competency model with associated competency
assessments. This approach to an AI Literacy could guide future development of
instructional content as we prepare a range of groups (i.e., consumers,
co-workers, collaborators, and creators). We propose here a research matrix as
an initial step in the development of a roadmap for AI Literacy research, which
requires a systematic and coordinated effort with the support of publication
outlets and research funding, to expand the areas of competency and
assessments.

Ethics in AI becomes a global topic of interest for both policymakers and
academic researchers. In the last few years, various research organizations,
lawyers, think tankers and regulatory bodies get involved in developing AI
ethics guidelines and principles. However, there is still debate about the
implications of these principles. We conducted a systematic literature review
(SLR) study to investigate the agreement on the significance of AI principles
and identify the challenging factors that could negatively impact the adoption
of AI ethics principles. The results reveal that the global convergence set
consists of 22 ethical principles and 15 challenges. Transparency, privacy,
accountability and fairness are identified as the most common AI ethics
principles. Similarly, lack of ethical knowledge and vague principles are
reported as the significant challenges for considering ethics in AI. The
findings of this study are the preliminary inputs for proposing a maturity
model that assess the ethical capabilities of AI systems and provide best
practices for further improvements.

The widespread use of artificial intelligence (AI) in many domains has
revealed numerous ethical issues from data and design to deployment. In
response, countless broad principles and guidelines for ethical AI have been
published, and following those, specific approaches have been proposed for how
to encourage ethical outcomes of AI. Meanwhile, library and information
services too are seeing an increase in the use of AI-powered and machine
learning-powered information systems, but no practical guidance currently
exists for libraries to plan for, evaluate, or audit the ethics of intended or
deployed AI. We therefore report on several promising approaches for promoting
ethical AI that can be adapted from other contexts to AI-powered information
services and in different stages of the software lifecycle.

In the past ten years, artificial intelligence has encountered such dramatic
progress that it is now seen as a tool of choice to solve environmental issues
and in the first place greenhouse gas emissions (GHG). At the same time the
deep learning community began to realize that training models with more and
more parameters requires a lot of energy and as a consequence GHG emissions. To
our knowledge, questioning the complete net environmental impacts of AI
solutions for the environment (AI for Green), and not only GHG, has never been
addressed directly. In this article, we propose to study the possible negative
impacts of AI for Green. First, we review the different types of AI impacts,
then we present the different methodologies used to assess those impacts, and
show how to apply life cycle assessment to AI services. Finally, we discuss how
to assess the environmental usefulness of a general AI service, and point out
the limitations of existing work in AI for Green.

In this paper, we argue that AI ethics must move beyond the concepts of
race-based representation and bias, and towards those that probe the deeper
relations that impact how these systems are designed, developed, and deployed.
Many recent discussions on ethical considerations of bias in AI systems have
centered on racial bias. We contend that antiblackness in AI requires more of
an examination of the ontological space that provides a foundation for the
design, development, and deployment of AI systems. We examine what this
contention means from the perspective of the sociocultural context in which AI
systems are designed, developed, and deployed and focus on intersections with
anti-Black racism (antiblackness). To bring these multiple perspectives
together and show an example of antiblackness in the face of attempts at
de-biasing, we discuss results from auditing an existing open-source semantic
network (ConceptNet). We use this discussion to further contextualize
antiblackness in design, development, and deployment of AI systems and suggest
questions one may ask when attempting to combat antiblackness in AI systems.

The new characteristics of AI technology have brought new challenges to the
research and development of AI systems. AI technology has benefited humans, but
if improperly developed, it will harm humans. At present, there is no
systematic interdisciplinary approach to effectively deal with these new
challenges. This paper analyzes the new challenges faced by AI systems and
further elaborates the "Human-Centered AI" (HCAI) approach we proposed in 2019.
In order to enable the implementation of the HCAI approach, we systematically
propose an emerging interdisciplinary domain of "Human-AI Interaction" (HAII),
and define the objective, methodology, and scope. Based on literature review
and analyses, this paper summarizes the main areas of the HAII research and
application as well as puts forward the future research agenda for HAII.
Finally, the paper provides strategic recommendations for future implementation
of the HCAII approach and HAII work.

Human-centered artificial intelligence (AI) posits that machine learning and
AI should be developed and applied in a socially aware way. In this article, we
argue that qualitative analysis (QA) can be a valuable tool in this process,
supplementing, informing, and extending the possibilities of AI models. We show
this by describing how QA can be integrated in the current prediction paradigm
of AI, assisting scientists in the process of selecting data, variables, and
model architectures. Furthermore, we argue that QA can be a part of novel
paradigms towards Human Centered AI. QA can support scientists and
practitioners in practical problem solving and situated model development. It
can also promote participatory design approaches, reveal understudied and
emerging issues in AI systems, and assist policy making.

By defining the current limits (and thereby the frontiers), many boundaries
are shaping, and will continue to shape, the future of Artificial Intelligence
(AI). We push on these boundaries in order to make further progress into what
were yesterday's frontiers. They are both pliable and resilient - always
creating new boundaries of what AI can (or should) achieve. Among these are
technical boundaries (such as processing capacity), psychological boundaries
(such as human trust in AI systems), ethical boundaries (such as with AI
weapons), and conceptual boundaries (such as the AI people can imagine). It is
within this final category while it can play a fundamental role in all other
boundaries} that we find the construct of needs and the limitations that our
current concept of need places on the future AI.

Wellbeing AI has been becoming a new trend in individuals' mental health,
organizational health, and flourishing our societies. Various applications of
wellbeing AI have been introduced to our daily lives. While social
relationships within groups are a critical factor for wellbeing, the
development of wellbeing AI for social interactions remains relatively scarce.
In this paper, we provide an overview of the mediative role of AI-augmented
agents for social interactions. First, we discuss the two-dimensional framework
for classifying wellbeing AI: individual/group and analysis/intervention.
Furthermore, wellbeing AI touches on intervening social relationships between
human-human interactions since positive social relationships are key to human
wellbeing. This intervention may raise technical and ethical challenges. We
discuss opportunities and challenges of the relational approach with wellbeing
AI to promote wellbeing in our societies.

The right to AI explainability has consolidated as a consensus in the
research community and policy-making. However, a key component of
explainability has been missing: extrapolation, which describes the extent to
which AI models can be clueless when they encounter unfamiliar samples (i.e.,
samples outside the convex hull of their training sets, as we will explain). We
report that AI models extrapolate outside their range of familiar data,
frequently and without notifying the users and stakeholders. Knowing whether a
model has extrapolated or not is a fundamental insight that should be included
in explaining AI models in favor of transparency and accountability. Instead of
dwelling on the negatives, we offer ways to clear the roadblocks in promoting
AI transparency. Our analysis commentary accompanying practical clauses useful
to include in AI regulations such as the National AI Initiative Act in the US
and the AI Act by the European Commission.

